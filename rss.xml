<?xml version="1.0" encoding="UTF-8" ?>
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#">
  <channel>
    <title><![CDATA[Blog]]></title>
    <atom:link href="http://localhost:3333/rss.xml" rel="self" type="application/rss+xml"/>
    <link>http://localhost:3333</link>
    <description><![CDATA[Our blog-style wiki about technology!]]></description>
    <image>
      <url>http://localhost:3333/logo.jpg</url>
      <title>Blog</title>
      <link>http://localhost:3333</link>
    </image>
    <pubDate>Wed, 10 May 2017 00:46:32 GMT</pubDate>
    <lastBuildDate>Wed, 10 May 2017 00:46:32 GMT</lastBuildDate>
    <language>en-US</language>
    <generator>Metalsmith custom plugin</generator>
    <ttl>60</ttl>
    <item>
      <title><![CDATA[DataTables Keep Conditions Plugin – Link to the exact settings within the current table]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/datatables-keep-conditions-plugin-link-to-the-exact-settings-within-the-current-table/</link>
      <guid isPermaLink="true">http://localhost:3333/datatables-keep-conditions-plugin-link-to-the-exact-settings-within-the-current-table/</guid>
      <category><![CDATA[DataTables]]></category>
      <category><![CDATA[javascript]]></category>
      <category><![CDATA[jQuery]]></category>
      <category><![CDATA[plugins]]></category>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Thu, 17 Nov 2016 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>I recently posted a plugin I created for DataTables, called <a href="/articles/datatables-live-ajax-plugin-keep-your-ajax-sourced-tables-up-to-date.html">Live Ajax</a>, which basically keeps an AJAX sourced DataTable up to date, updating only the required rows.</p>
<p>But thats actually only one of several plugins I&#39;ve recently created. I happen to use <a href="http://datatables.net">DataTables</a> quite a bit, probably for every project that needs a table with anything done to it (pagination, searching, ordering, length change, etc). DataTables handles all of that right out of the box!</p>
<p>Before I used DataTables, I was doing most of the above actions via the backend, via libraries like the <a href="http://www.codeigniter.com/user_guide/libraries/pagination.html">Pagination Class</a> that comes with <a href="http://www.codeigniter.com/">CodeIgniter</a>. There were pros and cons to both versions of processing. While processing everything on the server side made it easier for viewers to copy/paste the URL, or bookmark it, and return to the table exactly as it was, it was a new HTTP request for every change... Even if you use jQuery and AJAX, thats still more connections than I would like, just to render a table. Then on the other hand, if you use a jQuery plugin, such as DataTables, the rendering is much faster, since it&#39;s all done right there on the client side, but since it alters elements right in the DOM, you can&#39;t copy/paste the URL, or bookmark it, and return to it exactly as it was.</p>
<p>I got tired of sending someone to a page within one of my projects that was using DataTables, and telling them to &quot;Search for this&quot; or &quot;Order this column Ascending&quot; or &quot;Go to page #&quot;.. I wanted to just copy and paste the URL to them, and have the table draw itself up just as I was seeing it on my side.</p>
<p>And DataTables made this easy, with it&#39;s robust and powerful <a href="http://datatables.net/reference/api/">API</a>, I was able to <a href="http://datatables.net/manual/plug-ins/">create a plugin</a> to do exactly what I wanted.</p>
<p>The <em>KeepConditions</em> plugin has the ability to keep the following conditions:</p>
<ul>
<li>Table Search String</li>
<li>Column Ordering</li>
<li>Pagination</li>
<li>Table Length</li>
<li><a href="http://datatables.net/reference/button/colvis">Column Visibility</a> (A <a href="http://datatables.net/extensions/buttons/">buttons</a> extension)</li>
<li><a href="https://datatables.net/extensions/scroller/">Scroll Position</a></li>
<li><a href="http://datatables.net/extensions/colreorder/">Column Reordering</a></li>
</ul>
<h3 id="-git-repo-https-github-com-jhyland87-datatables-keep-conditions-"><a href="https://github.com/jhyland87/DataTables-Keep-Conditions">GIT Repo</a></h3>
<hr>
<h2 id="configuration">Configuration</h2>
<p>I made the plugin about as easy to use as I possibly could. All you need to do is initiate a DataTable instance as you normally would, but add a new setting, <em>keepConditions</em>. Setting the <em>keepConditions</em> simply to true will automatically enable processing of every column on said table. But there are a few other options you can use.</p>
<p>Heres a list of the parameters/options you can use to configure <em>keepConditions</em></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Type</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>keepConditions</td>
<td>boolean/object</td>
<td>true</td>
<td>Enable/Disable keepConditions plugin</td>
</tr>
<tr>
<td>keepConditions.page</td>
<td>boolean</td>
<td>true</td>
<td>Enable keepConditions for pagination</td>
</tr>
<tr>
<td>keepConditions.length</td>
<td>boolean</td>
<td>true</td>
<td>Enable keepConditions for page length</td>
</tr>
<tr>
<td>keepConditions.search</td>
<td>boolean</td>
<td>true</td>
<td>Enable keepConditions for table search/filter</td>
</tr>
<tr>
<td>keepConditions.order</td>
<td>boolean</td>
<td>true</td>
<td>Enable keepConditions for column ordering</td>
</tr>
<tr>
<td>keepConditions.colvis</td>
<td>boolean</td>
<td>true</td>
<td>Enable keepConditions for <a href="http://datatables.net/reference/button/colvis">column visibility</a></td>
</tr>
<tr>
<td>keepConditions.scroller</td>
<td>boolean</td>
<td>true</td>
<td>Enable keepConditions for the <a href="https://datatables.net/extensions/scroller/">Scroller</a> extension (Only enabled by default if Scroller extension is included)</td>
</tr>
<tr>
<td>keepConditions.colorder</td>
<td>boolean</td>
<td>true</td>
<td>Enable keepConditions for <a href="http://datatables.net/extensions/colreorder/">column ordering</a> (If ColReorder is enabled)</td>
</tr>
</tbody>
</table>
<p>Also... The <em>KeepConditions</em> plugin comes with a button! If you properly initiate a table with the <a href="http://datatables.net/extensions/buttons/">buttons extensio</a>, then all you need to do is add the button <em>copyConditions</em>. This will add a button labeled <em>Copy Conditions</em>. If the viewers browser supports the ability to copy text to the clipboard, then the URL will automatically be sent to the clipboard, if not, then the viewer will get a DataTables prompt with the URL, informing them to copy it&#39;s already selected contents.</p>
<hr>
<h2 id="examples">Examples</h2>
<p><strong>Basic Initialization</strong> - Enabling for Paging, Length, Search and Order</p>
<pre><code class="lang-javascript">$(&#39;#example&#39;).DataTable({
    keepConditions: true
});
</code></pre>
<p><strong>Advanced Initialization</strong> - Select what conditions to keep</p>
<pre><code class="lang-javascript">$(&#39;#example&#39;).DataTable({
    dom: &#39;lfrtip&#39;,
    keepConditions: {
        page:   true,
        length: true,
        search: true,
        order:  true
    }
});
</code></pre>
<p><strong>Basic w/ Button</strong></p>
<pre><code class="lang-javascript">$(&#39;#example-1&#39;).DataTable({
    keepConditions: true,
    dom: &#39;Blfrtip&#39;,
    buttons: [
        &#39;copyConditions&#39;
    ]
});
</code></pre>
<p><strong>Multiple Tables</strong> - With different configurations, as well as tables targeting a table via class name</p>
<pre><code class="lang-javascript">$(&#39;#example-1&#39;).DataTable({
    dom: &#39;Blftipr&#39;,
    keepConditions: true,
    buttons: [
        &#39;copyConditions&#39;
    ]
});

$(&#39;.example-2&#39;).DataTable({ // Using Class
    dom: &#39;lftipr&#39;,
    pageLength: 25,
    keepConditions: {
        search: true,
        order: true,
        page: true,
        length: true
    }
});
</code></pre>
<p><strong>Extension Compatibility</strong> - Initiation with the <a href="http://datatables.net/reference/button/colvis">ColVis</a> button and <a href="https://datatables.net/extensions/scroller/">Scroller</a> and <a href="http://datatables.net/extensions/colreorder/">ColReorder</a> extensions, on an AJAX sourced table, as well as disabling un-necessary conditions.</p>
<pre><code class="lang-javascript">$(&#39;#example&#39;).DataTable({
    ajax:           &quot;;dataSrc.txt&quot;;,
    deferRender:    true,
    scrollY:        200,
    scrollCollapse: true,
    scroller:       true,
    colReorder:     true,
    dom: &#39;Bfrtip&#39;,
    buttons: [
        &#39;colvis&#39;
    ],
    keepConditions: {
        page:     false,
        length:   false,
        search:   true,
        order:    true,
        colvis:   true,
        scroller: true
    }
});
</code></pre>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[CentOS 7 Icebreaker - Everything you need to know to get started]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/centos-7-icebreaker-everything-you-need-to-know-to-get-started/</link>
      <guid isPermaLink="true">http://localhost:3333/centos-7-icebreaker-everything-you-need-to-know-to-get-started/</guid>
      <category><![CDATA[centos6]]></category>
      <category><![CDATA[centos7]]></category>
      <category><![CDATA[ext4]]></category>
      <category><![CDATA[firewalld]]></category>
      <category><![CDATA[network-manager]]></category>
      <category><![CDATA[rhel6]]></category>
      <category><![CDATA[rhel7]]></category>
      <category><![CDATA[xfs]]></category>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Tue, 12 Apr 2016 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>There&#39;s actually quite a bit of changes from CentOS 6 to CentOS 7, some new commands, packages and some new services. Some older packages have been discontinued and replaced with newer packages, as well as some packages introduced that make it easier to automate some tasks that were previously done via just editing flat text files. In here, I&#39;ll cover the ones that I have used thus far in my job and at home. Hopefully this post will make your transition from CentOS 6 to 7 fast and painless, and maybe even fun!</p>
<h2 id="centos-7-package-command-replacements">CentOS 7 Package/Command Replacements</h2>
<h3 id="_ifconfig-ip_"><em>ifconfig -&gt; ip</em></h3>
<p>In the previous CentOS/RHEL releases, the command to view the interface configuration was just ifconfig, but in r7, theres a few different ways, the primary one would be via the new command: <em><strong>ip</strong></em>; Which is installed by default (even on the minimal install).</p>
<p>To get the same information you would get via <em>ifconfig</em>, you can type <em><strong>ip addr show</strong></em>, or simply just <em><strong>ip addr</strong></em>. Heres an example of the output:</p>
<pre><code class="lang-bash">$ ip addr
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: enp0s3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether 08:00:27:cd:98:f7 brd ff:ff:ff:ff:ff:ff
    inet 192.168.0.49/24 brd 192.168.0.255 scope global enp0s3
       valid_lft forever preferred_lft forever
    inet6 fe80::a00:27ff:fecd:98f7/64 scope link
       valid_lft forever preferred_lft forever
3: enp0s8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether 08:00:27:02:5c:4e brd ff:ff:ff:ff:ff:ff
    inet 192.168.0.50/24 brd 192.168.0.255 scope global enp0s8
       valid_lft forever preferred_lft forever
    inet6 fe80::a00:27ff:fe02:5c4e/64 scope link
       valid_lft forever preferred_lft forever
4: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN
    link/ether 02:42:9f:f9:3c:56 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 scope global docker0
       valid_lft forever preferred_lft forever
</code></pre>
<hr>
<h3 id="_service-systemctl_"><em>service -&gt; systemctl</em></h3>
<p>That&#39;s right, the service <service> <stop|start|status|restart> isn&#39;t the recommended method for managing services, there&#39;s now the <strong><em>systemctl</em></strong> command (which is also used to manage <em>runlevels</em>, effectively replacing <em>chkconfig</em> as well, well cover that separately though). The systemctl does come with a lot more functionality though, the commands that are used to interact with services are:</p>
<ul>
<li><strong>list-units</strong> - List known units (Default command).</li>
<li><strong>list-sockets</strong> - List socket units ordered by listening address.</li>
<li><strong>list-timers</strong> - List timer units ordered by the time they elapse next.</li>
<li><strong>start</strong> - Start (activate) one or more units specified on the command line.</li>
<li><strong>stop</strong> - Stop (deactivate) one or more units specified on the command line.</li>
<li><strong>reload</strong> - Asks all units listed on the command line to reload their configuration. Note that this will reload the service-specific configuration, not the unit configuration file of systemd. If you want systemd to reload the configuration file
of a unit, use the daemon-reload command. In other words: for the example case of Apache, this will reload Apache&#39;s httpd.conf in the web server, not the apache.service systemd unit file.</li>
<li><strong>restart</strong> -  Restart one or more units specified on the command line. If the units are not running yet, they will be started</li>
<li><strong>try-restart</strong> - Restart one or more units specified on the command line if the units are running. This does nothing if units are not running.</li>
<li><strong>reload-or-restart</strong> - Reload one or more units if they support it. If not, restart them instead. If the units are not running yet, they will be started.</li>
<li><strong>reload-or-try-restart</strong> - Reload one or more units if they support it. If not, restart them instead. This does nothing if the units are not running. Note that, for compatibility with SysV init scripts, force-reload is equivalent to this command.</li>
<li><strong>isolate</strong> - Start the unit specified on the command line and its dependencies and stop all others.</li>
<li><strong>kill</strong> - Send a signal to one or more processes of the unit.</li>
<li><strong>is-active</strong> - heck whether any of the specified units are active (i.e. running).</li>
<li><strong>is-failed</strong> - Check whether any of the specified units are in a &quot;failed&quot; state.</li>
<li><strong>status</strong> - Show terse runtime status information about one or more units, followed by most recent log data from the journal.</li>
<li><strong>show</strong> - Show properties of one or more units, jobs, or the manager itself.</li>
<li><strong>cat</strong> - Show backing files of one or more units.</li>
<li><strong>set-property</strong> - Set the specified unit properties at runtime where this is supported.</li>
<li><strong>reset-failed</strong> - Reset the &quot;failed&quot; state of the specified units, or if no unit name is passed, reset the state of all units.</li>
<li><strong>list-dependencies</strong> - Shows units required and wanted by the specified unit.</li>
</ul>
<p>The above descriptions were taken right from the man page, but some are shortened to make this post a quicker read, but you can view the man page itself, <a href="https://www.freedesktop.org/software/systemd/man/systemctl.html">here</a></p>
<p>The syntax is:</p>
<pre><code class="lang-bash">$ /usr/bin/systemctl &lt;em&gt;command&lt;/em&gt; &lt;em&gt;service&lt;/em&gt;.service
</code></pre>
<p>Where <em><command></em> is one of the commands listed above, and <em><service></em> is the name of the service.</p>
<p>So instead of using:</p>
<pre><code class="lang-bash">$ service httpd restart
</code></pre>
<p>You would type:</p>
<pre><code class="lang-bash">$ systemctl restart httpd.service
</code></pre>
<p>Do keep in mind that when you&#39;re simply starting/stoping/restarting a service, it doesn&#39;t show the <em>Stopping... [ OK ]</em> and <em>Starting... [ OK ]</em> anymore, but the status command definitely is much more verbose and helpful than it was with the service package:</p>
<pre><code class="lang-bash">$ systemctl status httpd.service
httpd.service - The Apache HTTP Server Loaded: loaded (/usr/lib/systemd/system/httpd.service; disabled; vendor preset: disabled) Active: active (running) since Sat 2016-04-09 06:25:40 MST; 41s ago Docs: man:httpd(8) man:apachectl(8) Process: 12920 ExecStop=/bin/kill -WINCH ${MAINPID} (code=exited, status=0/SUCCESS) Main PID: 12925 (httpd) Status: &quot;;Total requests: 0; Current requests/sec: 0; Current traffic: 0 B/sec&quot;; CGroup: /system.slice/httpd.service ├─12925 /usr/sbin/httpd -DFOREGROUND ├─12927 /usr/sbin/httpd -DFOREGROUND ├─12928 /usr/sbin/httpd -DFOREGROUND ├─12929 /usr/sbin/httpd -DFOREGROUND ├─12930 /usr/sbin/httpd -DFOREGROUND └─12931 /usr/sbin/httpd -DFOREGROUND Apr 09 06:25:40 localhost.localdomain systemd[1]: Starting The Apache HTTP Server... Apr 09 06:25:40 localhost.localdomain httpd[12925]: AH00558: httpd: Could not reliably determine the server\&#39;s fully qualified domain name, using localhost.localdomain. Set the &#39;ServerName&#39; directive globally to suppress this message Apr 09 06:25:40 localhost.localdomain systemd[1]: Started The Apache HTTP Server.
</code></pre>
<p><strong>Note:</strong> Even though the <em>service</em> command is no longer used for managing the service status, if you attempt to use it, <em>systemctl</em> will intercept the command, translate it to the <em>systemctl</em> equivalent, and tell you the command it&#39;s actually running, so you can see what should be typed:</p>
<pre><code class="lang-bash">$ service httpd restart
Redirecting to /bin/systemctl restart  httpd.service
</code></pre>
<hr>
<h4 id="how-systemd-works">How SystemD Works</h4>
<p>The <em>systemd</em> method of targeting runlevels for specific applications also works differently than in previous releases. In earlier releases, you would create startup scripts in <em>/etc/init.d</em>, then create symlinks to said scripts in the <em>/etc/rc*.d</em> directories. Systemd stores various files inside the <strong>/lib/systemd/system</strong> directory, each with a specific suffix to specify what the file is meant to do:</p>
<ul>
<li>automount</li>
<li>mount</li>
<li>path</li>
<li>service</li>
<li>slice</li>
<li>socket</li>
<li>target</li>
<li>timer</li>
<li>wants
These files are not simply bash scripts, they&#39;re actual configuration files that specify settings such as the default runlevel, the nice level, the kill signals, the directories, and other service configuration settings and default settings.</li>
</ul>
<p>Now if you look in the <strong>/etc/systemd/system</strong> directory, you will notice some folders that are named after the runlevel targets, with a <strong>.wants</strong> suffix, these are the equivalent of the <em>/etc/rc*.d</em> directories in CentOS 6. Inside these subfolders are where the symlinks are kept, these symlinks have a<strong> .service</strong> suffix, and are pointed at the service configuration file located at <strong>/usr/lib/systemd/system</strong>.</p>
<p>Now that we have an idea how it works, let&#39;s look at the Apaches service. We know that it&#39;s targeted to run at <strong>multi-user</strong>, so lets see whats located at <strong>/etc/systemd/system/multi-user.target.wants/httpd.service</strong>.</p>
<pre><code class="lang-bash">$ ls -alrth /etc/systemd/system/multi-user.target.wants/httpd.service
lrwxrwxrwx. 1 root root 37 Apr 14 12:15 /etc/systemd/system/multi-user.target.wants/httpd.service -&gt; /usr/lib/systemd/system/httpd.service
</code></pre>
<p>Ok, so it&#39;s a symlink, lets take a look at the content (I haven&#39;t changed anything, so it&#39;s all default):</p>
<pre><code class="lang-bash">
$ cat /usr/lib/systemd/system/httpd.service
[Unit]
Description=The Apache HTTP Server
After=network.target remote-fs.target nss-lookup.target
Documentation=man:httpd(8)
Documentation=man:apachectl(8)

[Service]
Type=notify
EnvironmentFile=/etc/sysconfig/httpd
ExecStart=/usr/sbin/httpd $OPTIONS -DFOREGROUND
ExecReload=/usr/sbin/httpd $OPTIONS -k graceful
ExecStop=/bin/kill -WINCH ${MAINPID}
# We want systemd to give httpd some time to finish gracefully, but still want
# it to kill httpd after TimeoutStopSec if something went wrong during the
# graceful stop. Normally, Systemd sends SIGTERM signal right after the
# ExecStop, which would kill httpd. We are sending useless SIGCONT here to give
# httpd time to finish.
KillSignal=SIGCONT
PrivateTmp=true

[Install]
WantedBy=multi-user.target
</code></pre>
<p>And there you have it, that&#39;s how systemd manages the targeted runlevels. I haven&#39;t had time to look into them yet, but you can tell by all of the other suffixes that the files in the <strong>/lib/systemd/system</strong> directory have, that systemd can do a lot more than just managing the service runlevels... Maybe I&#39;ll write a separate post on that when I dive into it.</p>
<h3 id="_chkconfig-systemctl_"><em>chkconfig -&gt; systemctl</em></h3>
<p>The <em>chkconfig</em> command was also replaced by <em>systemctl</em>, which I personally love that managing the service status as well as the runlevels can both be done with the same binary.</p>
<p>One of the first things you need to realize, is that instead of using run-levels, you now use targets, the targets are listed below, and if there is a corresponding runlevel in the previous CentOS/RHEL releases, I put that right next to it:</p>
<ul>
<li>poweroff.target, runlevel0.target - Runlevel 0</li>
<li>poweroff.target, runlevel0.target - Runlevel 0</li>
<li>rescue.target, runlevel1.target - Runlevel 1</li>
<li>multi-user.target, runlevel2.target, runlevel3.target, runlevel4.target - Runlevels 2 3 and 4</li>
<li>graphical.target, runlevel5.target - Runlevel 5</li>
<li>reboot.target, runlevel6.target - Runlevel 6</li>
<li>default.target</li>
<li>emergency.target</li>
<li>halt.target</li>
<li>kexec.target</li>
<li>suspend..target</li>
<li>hibernate.target</li>
<li>hybrid-sleep.target</li>
<li>basic.target</li>
<li>getty.target</li>
<li>sockets.target</li>
<li>sysinit.target</li>
<li>system-update.target</li>
</ul>
<p>As you can see, they tried to make the transition easier. If you don&#39;t know the exact target name, you can simply use <strong>runlevel<em><0-6></em>.target</strong> as the target, but do realize that some of the new targets actually correspond with more than one runlevel, so I suggest you get use to the new targets.</p>
<p>The <em>default.target</em> can be set to any runlevel you want to use as the default. In the previous releases, it would default to whatever runlevel you were currently in, but in CentOS 7, it&#39;s set to the <em>multi-user.target</em>, and you can verify that via the <em>get-default</em> command:</p>
<pre><code class="lang-bash">$ systemctl get-default
multi-user.target
</code></pre>
<p>And the default target can be just as easily set via the <em>set-default</em> command:</p>
<pre><code class="lang-bash">$ systemctl set-default multi-user.target
Removed symlink /etc/systemd/system/default.target.
Created symlink from /etc/systemd/system/default.target to /usr/lib/systemd/system/multi-user.target.
</code></pre>
<p>If you&#39;re looking for more details on managing <em>systemd</em> targets via <em>systemctl</em>, you can look at the <a href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/System_Administrators_Guide/sect-Managing_Services_with_systemd-Targets.html">RedHat documentation</a>, it&#39;s very detailed and useful, as always.</p>
<hr>
<h3 id="_hostname-hostnamectl_"><em>hostname -&gt; hostnamectl</em></h3>
<p>Setting the hostname is now done via the hostnamectl package, which really doesn&#39;t have too much to it, it just manages the systems hostname:</p>
<pre><code class="lang-bash">$ hostnamectl set-hostname c7box.justinhyland.local
$ hostname
c7box.justinhyland.local
</code></pre>
<hr>
<p>Timezone Management</p>
<p>To change the timezone in a <r7 release, you would have to symlink the timezone file located in _/usr/share/zoneinfo/<timezone><em> to </em>/etc/localtime_. CentOS 7 comes with <em>timedatectl</em>, making it much easier:</p>
<pre><code class="lang-bash">$ timedatectl set-timezone America/Phoenix
</code></pre>
<p>Thats all, not much to the timezone management really.</p>
<hr>
<h3 id="_iptables-firewalld_"><em>iptables -&gt; firewalld</em></h3>
<p>iptables is still available, and it&#39;s actually more effective for IPv6 packet filtering than before, but, theres a new firewall management package called <strong>firewalld</strong>, and I think it&#39;s awesome!</p>
<p>The <em>firewalld</em> service is managed via the <em>firewall-cmd</em> command, and it&#39;s actually pretty easy to use. Basically, you manage <strong>zones</strong>, a network zone defines the level of trust for network connections. This is a one to many relation, which means that a connection can only be part of one zone, but a zone can be used for many network connections. So basically connections get assigned to one or more <em>zones</em>, then the filtering is done to the zones themselves, you can think of zones as groups.</p>
<p>There&#39;s no way I can cover everything in the new firewalld package, so I&#39;ll just cover the very basics, and list some of the most useful/common commands.</p>
<p>After you install an application (Apache, for example), you can either whitelist whatever ports it will be using like so:</p>
<pre><code class="lang-bash">$ firewall-cmd --permanent --zone=public --add-port=80/tcp
</code></pre>
<p>Or, if you plan on using multiple ports for Apache, and you don&#39;t want to have to add each one of them to the firewall, you can add the service itself:</p>
<pre><code class="lang-bash">$ firewall-cmd --permanent --zone=public --add-service=http
</code></pre>
<p>After any changes are made, you need to reload firewalld:</p>
<pre><code class="lang-bash">$ firewall-cmd --reload
</code></pre>
<p>Most useful firewall-cmd commands:</p>
<ul>
<li><strong>firewall-cmd --list-all</strong> - List everything added for or enabled in zone.</li>
<li><strong>firewall-cmd --list-all-zones</strong> - List everything added for or enabled in all zones</li>
<li><strong>firewall-cmd --zone=<zone> --list-services</strong> - List all services available to a specific zone</li>
<li><strong>firewall-cmd --permanent --zone=<em><zone></em> --add-service=<em><service></em></strong> - Permanently whitelist all connections for a specific service within a specific zone</li>
<li><strong>firewall-cmd --permanent --zone=<em><zone></em> --add-port=<em><port>/<tcp|udp></em></strong> - Permanentlywhitelist a port (or port range) within a specific zone</li>
<li><strong>firewall-cmd --get-active-zones</strong> - Get the list of active zones</li>
<li><strong>firewall-cmd --reload</strong> - Reload any changes that were made
(Ill add more detail to this one later)</li>
</ul>
<p>The <em>firewalld</em> package wasn&#39;t installed by default for me, but then again, I only installed the minimal iso, to install it, just <em>yum install firewalld</em></p>
<p>The changes made via <em>firewall-cmd</em> will only be part of the runtime configuration, thus, they will be reset on the next reboot, to make them permanent, you need to add <strong>--permanent</strong> to the command.</p>
<p>If you don&#39;t like the new <em>firewalld</em>, you can still use the old <em>iptables</em> package, just follow <a href="https://www.digitalocean.com/community/tutorials/how-to-migrate-from-firewalld-to-iptables-on-centos-7">this</a> guide.</p>
<hr>
<h2 id="new-packages-features">New Packages/Features</h2>
<p>In addition to providing newer and more powerful substitutions to some well-known and highly used packages, there&#39;s quite a few new packages that were included</p>
<h3 id="network-manager-nmcli-">Network Manager (nmcli)</h3>
<p>In the previous releases of CentOS/RHEL, the tool that was used to manage the network interfaces was some crappy GUI tool called _<a href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Migration_Planning_Guide/chap-Migration_Guide-Networking.html">System-Config-Network</a><em>, which I never used, and I don&#39;t know anyone that used it. Typically, the most common approach was just to script it yourself, since all you&#39;re really doing is managing flat files inside </em>/etc/sysconfig/network-scripts/ifcfg-<em>_. In release 7, the new tool of choice is called the Network-Manager, a very easy yet powerful package that lets you manage just about every aspect of the network interfaces. You interact with it via the <em>*nmcli</em></em> command.</p>
<p>A good example scenario would be if you wanted to change a current network interface from using DHCP to static, previously, you would either edit the <em>ifcfg-eth#</em> file directly, then restart the network service, but now it can be done via a series of commands:</p>
<pre><code class="lang-bash">$ nmcli connection modify enp0s3 ipv4.address 192.168.1.48
$ nmcli connection modify enp0s3 ipv4.gateway 192.168.1.1
$ nmcli connection modify enp0s3 ipv4.method manual
$ nmcli connection modify enp0s3 ipv4.dns &quot;;8.8.8.8&quot;;
$ nmcli connection reload enp0s3
</code></pre>
<p>There&#39;s really too much to cover in the new Network Manager, so I recommend that you take a look at the <a href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Networking_Guide/sec-Using_the_NetworkManager_Command_Line_Tool_nmcli.html">documentation</a>, but heres a few of the common commands I&#39;ve been using thus far, or commands I thought were intriguing:</p>
<ul>
<li><strong>nmcli connection edit <interface-name></strong> - Opens the interactive editor to edit said interface</li>
<li><strong>nmcli device show</strong> - Show network interfaces</li>
<li><strong>nmcli connection show</strong> - Show all network connections</li>
<li><strong>nmcli connection show --active</strong> - Show only active connections</li>
<li><strong>nmcli -f all dev show enp0s3</strong> - Show all fields for the device called <em>enp0s3</em></li>
</ul>
<p>Out of all of the packages I&#39;m covering in this post, I would say that the Network Manager is the one of the more advanced ones, and I barely just covered the tip of the iceberg. For more detail, look at RedHats <a href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Networking_Guide/sec-Using_the_NetworkManager_Command_Line_Tool_nmcli.html">Network Manager documentation</a></p>
<hr>
<p>##Other Changes</p>
<h3 id="network-interface-names">Network Interface Names</h3>
<p>Release 7 introduces a new naming convention for the NIC names, it&#39;s called <strong><a href="https://www.freedesktop.org/wiki/Software/systemd/PredictableNetworkInterfaceNames/">Predictable Network Interface Names</a></strong>, and just like the title says, it provides a more &#39;predictable&#39; name for the network interfaces. At first, you&#39;re going to see the names of your interfaces, and think &quot;How in the world is THAT predictable?...&quot;, and you would even think having the old <em>ifcfg-eth#</em> or <em>ifcfg-wlan#</em> interface names would be more predictable.. But once you read a little more into it, you&#39;ll see the sense in it.</p>
<p>The primary problem they were trying to solve was unpredictability of what interfaces would get assigned to what numeric value (as in <em>ifcfg-eth#</em>), have you ever rebooted a server that had multiple interfaces, and noticed that the interface names were switched once you booted it back up? I have. It typically happens when you haven&#39;t statically configured the interface configurations, and haven&#39;t binded them to a Mac address (or <em>HWDADDR</em>). Obviously this problem isn&#39;t too difficult to fix, but it still opens up the possibilities of network issues, security issues, application issues, etc etc.</p>
<p>The new convention is setup so you should actually be able to predict exactly what the interface names will be named, even before the system is booted up! I haven&#39;t quite got the convention down to pat just yet, but from reading the documentation (which is all referenced), I can summarize it for you..</p>
<p>The name comes with a two character prefix, which is based on the interface type:</p>
<ul>
<li><strong>en</strong> -- Ethernet</li>
<li><strong>sl</strong> -- serial line IP (slip)</li>
<li><strong>wl</strong> -- wlan</li>
<li><strong>ww</strong> -- wwan</li>
</ul>
<p>Then the rest of the interface name is based off of:</p>
<ul>
<li>Firmware/bios-provided index numbers for on-board devices</li>
<li>Firmware-provided pci-express hotplug slot index number</li>
<li>Physical/geographical location of the hardware</li>
<li>The interface&#39;s MAC address</li>
</ul>
<p>If you want to see what the schema looks like <em>exactly</em>... heres some <strong><a href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Networking_Guide/sec-Understanding_the_Predictable_Network_Interface_Device_Names.html">documentation</a></strong>, or look directly in the <strong><a href="https://github.com/systemd/systemd/blob/master/src/udev/udev-builtin-net_id.c#L20">source</a></strong>.</p>
<p>Undoubtedly, there&#39;s going to be quite a few people out there who aren&#39;t a fan of the new &quot;predictable&quot; interface names, and if thats the case for you, you can disable it, just read over the documentation found <strong><a href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Networking_Guide/sec-Disabling_Consistent_Network_Device_Naming.html">here</a></strong>.</p>
<hr>
<h3 id="systemd-log-parsing">Systemd Log Parsing</h3>
<p>Instead of just handing you some parsing tools (<em>cat, head, tail, grep,</em> etc), and pointing you in the direction of the <em>systemd</em> log files, you get another tool! It&#39;s called, <em>The Journal</em> (<strong>journalctl</strong>).</p>
<p>Whenever I parse log files while debugging an application or service, there&#39;s usually some pretty tedious little things I need to do, things that aren&#39;t very complicated, but they&#39;re just a pain in the ass.. Such as grabbing logs between specific timestamps, changing the timestamps to the local time, parsing the previously rotated logs with the current one, etc etc. These are the types of things that <em>Journal</em> does for you.</p>
<p>Heres just some of the included features I&#39;ve found so far:</p>
<ul>
<li>The priority of entries is marked visually. Lines of error priority and higher are highlighted with red color and a bold font is used for lines with notice and warning priority</li>
<li>The time stamps are converted for the local time zone of your system</li>
<li>All logged data is shown, including rotated logs</li>
<li>The beginning of a boot is tagged with a special line</li>
<li>Specify different verbosity levels of the log content to be viewed</li>
<li>Specify what fields should be shown</li>
<li>Reverse the output of the logs (placing newest data at top)</li>
</ul>
<p>Obviously theres nothing here you can&#39;t do with some already existing binaries, so it&#39;s more of a convenience factor I suppose.</p>
<hr>
<h3 id="file-system">File System</h3>
<p>Every time RHEL publishes a new OS release, they change the default filesystem - RHEL 5 was on <em>ext3</em>, RHEL 6 was on <em>ext4</em>, and RHEL 7 is on <strong>xfs</strong>. Since the filesystems aren&#39;t completely compatible, you can&#39;t just migrate from <em>ext3/4</em> to <em>xfs</em>, but that doesn&#39;t mean you&#39;ll lose any functionality after migrating to <em>xfs.</em> I looked around for quite a bit for any loss of functionality when moving to xfs, and came up with nothing, other than awesome performance boosts.</p>
<p>Obviously since the underlying filesystem has been changed, the commands used to manage them will be different as well. The chart below should help you, it lists the task, then the commands to accomplish said task in <em>ext3/4</em>, then how to accomplish the same thing on an <em>xfs</em> filesystem:</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>ext3/4</th>
<th>xfs</th>
</tr>
</thead>
<tbody>
<tr>
<td>Create new FS</td>
<td>mkfs.ext4 or mkfs.ext3</td>
<td>mkfs.xfs</td>
</tr>
<tr>
<td>Check filesystem</td>
<td>e2fsck</td>
<td>xfs_repair</td>
</tr>
<tr>
<td>Resize FS</td>
<td>resize2fs</td>
<td>xfs_growfs</td>
</tr>
<tr>
<td>Create image of the FS</td>
<td>e2image</td>
<td>xfs_metadump &amp; xfs_mdrestore</td>
</tr>
<tr>
<td>Tune or label an FS</td>
<td>tune2fs</td>
<td>xfs_admin</td>
</tr>
<tr>
<td>Backup a FS</td>
<td>dump and restore</td>
<td>xfsdump &amp; xfsrestore</td>
</tr>
</tbody>
</table>
<p>The last thing I&#39;ll bring up is the file size limitations, In RHEL6 using ext4, the largest a single filesystem could get would be <strong>16TB</strong> - That limit has been increased to <strong>500TB</strong> in RHEL7 using xfs</p>
<hr>
<p><strong>Sources</strong></p>
<ul>
<li><a href="https://fedoraproject.org/wiki/FirewallD#What_is_a_zone.3F">Fedora Project</a></li>
<li><a href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/System_Administrators_Guide/sect-Managing_Services_with_systemd-Targets.html">RHEL 7 - Systemd Targets</a></li>
<li><a href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Security_Guide/sec-Using_Firewalls.html">RHEL 7 - Firewalls</a></li>
<li><a href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Networking_Guide/sec-Using_the_NetworkManager_Command_Line_Tool_nmcli.html">RHEL 7 - Network Manager</a></li>
<li><a href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Networking_Guide/sec-Understanding_the_Predictable_Network_Interface_Device_Names.html">RHEL 7 - Predictable Network Interface Names</a></li>
<li><a href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Storage_Administration_Guide/ch06s09.html">RHEL 7 - Migrating from ext4 to xfs</a></li>
<li><a href="https://access.redhat.com/sites/default/files/attachments/rhel_5_6_7_cheatsheet_27x36_1014_jcs_web.pdf">RHEL 5/6/7 Cheatsheet</a></li>
<li><a href="https://access.redhat.com/solutions/1532">RedHat - Filesystem Limitations</a></li>
<li><a href="https://www.freedesktop.org/software/systemd/man/systemd.service.html">SystemD Manual</a></li>
</ul>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[Writing your first AWS CloudFormation Template]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/writing-your-first-aws-cloudformation-template/</link>
      <guid isPermaLink="true">http://localhost:3333/writing-your-first-aws-cloudformation-template/</guid>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Mon, 04 Jan 2016 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>When starting out in AWS, and trying to understand everything about CloudFormation, I admit, I struggled a little bit. All of the different types of things that you can do in CloudFormation was both amazing and frightening at the same time. However, after using it for quite awhile now I figured I would guide others through creating their very first template.</p>
<p>For those who don&#39;t know what CloudFormation is, it&#39;s a service that will give you an easy way to create and manage AWS resources and provide a way to update them in a orderly and predictable fashion. Basically, what this means is it&#39;s a way for your to basically write a template that you can run against AWS to provision your entire environment as you need.</p>
<p>The reason I really enjoy using CloudFormation, is because it allows me to create the exact same set of ec2 instances, security groups, load balancers, etc. in the exact same way every single time. That means that when I go to create a Stage or Production version of a dev environment I created, I can do so quickly and easily.</p>
<p>In this example, we&#39;re going to create an EC2 instance with a load balancer in front of it (Yes, you don&#39;t need a load balancer in front of a single node typically, I just wanted to show you an example of how to create a load balancer and put EC2 instances behind it). So to get us started, there are a few terms and information you should know before hand so you understand. The first thing to know, is that all a CloudFormation Template is, is a JSON-formatted text file that describes your AWS infrastructure. There are also a lot of other sections inside a CloudFormation template that you can use. Let&#39;s list them out and give them a brief description of what they are.</p>
<ul>
<li><strong>AWSTemplateFormatVersion</strong> -  (OPTIONAL) Specifies the AWS CloudFormation template version that the template conforms to. The template format version is not the same as the API or WSDL version. The template format version can change independently of the API and WSDL versions.</li>
<li><strong>Description</strong> - (OPTIONAL) A text string that describes the template. This section must always follow the template format version section.</li>
<li><strong>Metadata</strong> - (OPTIONAL) JSON objects that provide additional information about the template.</li>
<li><strong>Parameters</strong> - (OPTIONAL) Specifies values that you can pass in to your template at runtime (when you create or update a stack). You can refer to parameters in the Resources and Outputs sections of the template.</li>
<li><strong>Mappings</strong> - (OPTIONAL) A mapping of keys and associated values that you can use to specify conditional parameter values, similar to a lookup table. You can match a key to a corresponding value by using the Fn::FindInMap intrinsic function in the Resources and Outputs section.</li>
<li><strong>Conditions</strong> - (OPTIONAL) Defines conditions that control whether certain resources are created or whether certain resource properties are assigned a value during stack creation or update. For example, you could conditionally create a resource that depends on whether the stack is for a production or test environment.</li>
<li><strong>Resources</strong> - <span style="color: #ff0000;">(REQUIRED)</span> Specifies the stack resources and their properties, such as an Amazon Elastic Compute Cloud instance or an Amazon Simple Storage Service bucket. You can refer to resources in theResources and Outputs sections of the template.</li>
<li><strong>Outputs</strong> - (OPTIONAL) Describes the values that are returned whenever you view your stack&#39;s properties. For example, you can declare an output for an Amazon S3 bucket name and then call the aws cloudformation describe-stacks AWS CLI command to view the name.</li>
</ul>
<p>Don&#39;t let all of the above scare you, as you can see by reading through all of them, the only one that is actually needed and is required is the Resources section and that&#39;s really the meat of this whole thing. So let&#39;s get started with our template, we&#39;ll skip past all of the optional sections and only work on the resources section in this example. So let&#39;s get started.</p>
<p>The first thing that we want to do, is create the base of our CloudFormation template with the opening and closing brackets and the Resources section like we need, and it should look like this:</p>
<pre><code class="lang-json">{
  &quot;Resources&quot; : {
  }
}
</code></pre>
<p>Now that we have the base of our template, we will want to start off by creating two EC2 instances. In order to create an EC2 instance, we need to create a Security Group so our ports that we need are open. So to do that we would use the AWS::EC2::SecurityGroup type and specify the Ingress options of the instances. So your template should look like this:</p>
<pre><code class="lang-json">{
  &quot;Resources&quot; : {
    &quot;InstanceSecurityGroup&quot; : {
      &quot;Type&quot; : &quot;AWS::EC2::SecurityGroup&quot;,
      &quot;Properties&quot; : {
        &quot;GroupDescription&quot; : &quot;Enable SSH access via port 22&quot;,
        &quot;SecurityGroupIngress&quot; : [
          { &quot;IpProtocol&quot; : &quot;tcp&quot;, &quot;FromPort&quot; : &quot;22&quot;, &quot;ToPort&quot; : &quot;22&quot;, &quot;CidrIp&quot; : &quot;0.0.0.0/0&quot; },
          { &quot;IpProtocol&quot; : &quot;tcp&quot;, &quot;FromPort&quot; : &quot;80&quot;, &quot;ToPort&quot; : &quot;80&quot;, &quot;CidrIp&quot; : &quot;0.0.0.0/0&quot; }
        ]
      }
    }
  }
}
</code></pre>
<p>So, let&#39;s go through and understand what we did above. The first is as mentioned above we need to declare the type which in this case is AWS::EC2::SecurityGroup. Then we pass in a Properties section for the type and give it a description and then specify the SecurityGroupIngress rules. In the example above, for the Ingress rules we are opening port 22 (SSH) and port 80 (HTTP) for the entire world. Typically you would want to lock down SSH to a particular IP address or IP address range for security, but because this is just an example we&#39;ll keep it open.</p>
<p>Now that we have our Security Group, we can create some EC2 Instances and use the above security group. To do this we will use the AWS::EC2::Instance type (I&#39;m sure you guessed that one!) and we&#39;ll reference the SG created above. So your template should look something like this:</p>
<pre><code class="lang-json">{
  &quot;Resources&quot; : {
    &quot;Ec2Instance&quot; : {
      &quot;Type&quot; : &quot;AWS::EC2::Instance&quot;,
      &quot;Properties&quot; : {
        &quot;SecurityGroups&quot; : [ { &quot;Ref&quot; : &quot;InstanceSecurityGroup&quot; } ],
        &quot;KeyName&quot; : &quot;mykey&quot;,
        &quot;ImageId&quot; : &quot;ami-7a11e213&quot;
      }
    },

    &quot;InstanceSecurityGroup&quot; : {
      &quot;Type&quot; : &quot;AWS::EC2::SecurityGroup&quot;,
      &quot;Properties&quot; : {
        &quot;GroupDescription&quot; : &quot;Enable SSH access via port 22&quot;,
        &quot;SecurityGroupIngress&quot; : [
          { &quot;IpProtocol&quot; : &quot;tcp&quot;, &quot;FromPort&quot; : &quot;22&quot;, &quot;ToPort&quot; : &quot;22&quot;, &quot;CidrIp&quot; : &quot;0.0.0.0/0&quot; },
          { &quot;IpProtocol&quot; : &quot;tcp&quot;, &quot;FromPort&quot; : &quot;80&quot;, &quot;ToPort&quot; : &quot;80&quot;, &quot;CidrIp&quot; : &quot;0.0.0.0/0&quot; }
        ]
      }
    }
  }
}
</code></pre>
<p>Now let&#39;s understand what we just did. We used the type mentioned above AWS::EC2::Instance and passed in properties for the type. In the properties we referenced the security group we created earlier by using the { &quot;Ref&quot; : &quot;&quot; } function, this is how you can reference other resources that you have created in the template and we passed in &quot;mykey&quot; ssh key. Please note, you will need to create an SSH key before hand for this example, to create the SSH key read <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-console-create-keypair.html">this document</a>. We also are specifying an AMI (Amazon Machine Image) ID to use for the EC2 instance. Pretty neat, huh?</p>
<p>Now, the next thing we need to do is create a load balancer and reference the EC2 instance that was created. So your template should look like this now:</p>
<pre><code class="lang-json">{
  &quot;Resources&quot; : {
    &quot;Ec2Instance&quot; : {
      &quot;Type&quot; : &quot;AWS::EC2::Instance&quot;,
      &quot;Properties&quot; : {
        &quot;SecurityGroups&quot; : [ { &quot;Ref&quot; : &quot;InstanceSecurityGroup&quot; } ],
        &quot;KeyName&quot; : &quot;mykey&quot;,
        &quot;ImageId&quot; : &quot;ami-7a11e213&quot;
      }
    },

    &quot;InstanceSecurityGroup&quot; : {
      &quot;Type&quot; : &quot;AWS::EC2::SecurityGroup&quot;,
      &quot;Properties&quot; : {
        &quot;GroupDescription&quot; : &quot;Enable SSH access via port 22&quot;,
        &quot;SecurityGroupIngress&quot; : [
          { &quot;IpProtocol&quot; : &quot;tcp&quot;, &quot;FromPort&quot; : &quot;22&quot;, &quot;ToPort&quot; : &quot;22&quot;, &quot;CidrIp&quot; : &quot;0.0.0.0/0&quot; },
          { &quot;IpProtocol&quot; : &quot;tcp&quot;, &quot;FromPort&quot; : &quot;80&quot;, &quot;ToPort&quot; : &quot;80&quot;, &quot;CidrIp&quot; : &quot;0.0.0.0/0&quot; }
        ]
      }
    },

    &quot;ElasticLoadBalancer&quot; : {
      &quot;Type&quot; : &quot;AWS::ElasticLoadBalancing::LoadBalancer&quot;,
      &quot;Properties&quot; : {
        &quot;AvailabilityZones&quot; : { &quot;Fn::GetAZs&quot; : &quot;&quot; },
        &quot;Instances&quot; : [ { &quot;Ref&quot; : &quot;Ec2Instance&quot; } ],
        &quot;Listeners&quot; : [ {
          &quot;LoadBalancerPort&quot; : &quot;80&quot;,
          &quot;InstancePort&quot; : &quot;80&quot;,
          &quot;Protocol&quot; : &quot;HTTP&quot;
        } ],
        &quot;HealthCheck&quot; : {
          &quot;Target&quot; : { &quot;Fn::Join&quot; : [ &quot;&quot;, [&quot;HTTP:&quot;, &quot;80&quot;, &quot;/&quot;]]},
          &quot;HealthyThreshold&quot; : &quot;3&quot;,
          &quot;UnhealthyThreshold&quot; : &quot;5&quot;,
          &quot;Interval&quot; : &quot;30&quot;,
          &quot;Timeout&quot; : &quot;5&quot;
        }
      }
    }
  }
}
</code></pre>
<p>As you can see, the load balancer type that was used was AWS::ElasticLoadBalancing::LoadBalancer, and it needs quite a bit more properties values for it to work. The first thing is the load balancer needs to know what availability zones to use, in this example we&#39;ll use the Fn::GetAZs which will automatically populate the availability zones based on the region you are in. Then we reference the Ec2Instance, configured which load balancer port and instance port to use and what protocol. After that we needed to specify a Health Check so the load balancer knows when the EC2 instance is up or down.</p>
<p>Phew. That seems like a lot, but once you start to understand it, it becomes second nature and you&#39;ll find yourself writing this in your sleep. Now that we have our template created, we can finally run it against AWS. To do this you will need to log into the <a href="https://aws.amazon.com/console/">AWS Console</a>.</p>
<p>Once you&#39;re logged in, you will want to navigate over to CloudFormation and click on &quot;Create a Stack&quot;. In this section you&#39;ll also see the new Design template option which you can use to visually create the CloudFormation template that you just created. However, I always feel like it&#39;s best to know the manual way of doing it because when you can manually do it, you can find ways to automate it.. But that&#39;s really a personal preference. So, click on &quot;Create New Stack&quot; button that looks like this:</p>
<p><img src="/assets/articles/create-stack.png" alt="create stack"></p>
<p>Then click on &quot;Upload a template to Amazon S3&quot; and then click on &quot;Choose File&quot; and navigate and select where you saved your template on your computer.</p>
<p><img src="/assets/articles/upload-template.png" alt="upload template"></p>
<p>Then click on Next. On the next screen simply type in what you want to name the Stack, and click next.</p>
<p><img src="/assets/articles/stack-name.png" alt="stack name"></p>
<p>Then you can specify any tags you want to use, and when done click next.</p>
<p><img src="/assets/articles/stack-options.png" alt="stack options"></p>
<p>Then you&#39;ll come to a review screen and simply review all of the information you put in to ensure it&#39;s correct and then click Create and watch your EC2 instance come up with a load balancer in front of it.</p>
<p>And you&#39;re done! Let me know if you run into any issues or have any questions. I&#39;ll be writing a follow up post to this to start using some of the other sections and using a parameters file.</p>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[DataTables Hide Empty Columns Plugin – Automatically hide any columns that are empty]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/datatables-hide-empty-columns-plugin-automatically-hide-any-columns-that-are-empty/</link>
      <guid isPermaLink="true">http://localhost:3333/datatables-hide-empty-columns-plugin-automatically-hide-any-columns-that-are-empty/</guid>
      <category><![CDATA[ColVis]]></category>
      <category><![CDATA[DataTables]]></category>
      <category><![CDATA[javascript]]></category>
      <category><![CDATA[jQuery]]></category>
      <category><![CDATA[plugins]]></category>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Wed, 18 Nov 2015 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>I recently posted two other <a href="http://datatables.net">DataTables</a> plugins I created, one for <a href="/articles/datatables-keep-conditions-plugin-link-to-the-exact-settings-within-the-current-table.html">keeping the table conditions in the URL</a>, and another for <a href="/articles/datatables-live-ajax-plugin-keep-your-ajax-sourced-tables-up-to-date.html">keeping AJAX sourced tables up-to-date</a>. Well, I created another one!</p>
<p>One project I&#39;m working on has a lot of dynamic tables, which can have any number of columns. Since the number of columns doesn&#39;t have a limit, I wanted to automatically hide any columns that aren&#39;t populated with any data. Once the columns are hidden (which is on the table initiation), the columns can easily be set back to visible by either the <em>column().visible()</em> API method, or the <a href="http://datatables.net/reference/button/colvis">Column Visibility</a> button.</p>
<h3 id="-git-repo-https-github-com-jhyland87-datatables-hide-empty-columns-live-demos"><a href="https://github.com/jhyland87/DataTables-Hide-Empty-Columns">GIT Repo</a> - Live Demos</h3>
<hr>
<h2 id="configuration">Configuration</h2>
<p>If you just set the hideEmptyCols_ (or <em>hideEmptyColumns</em>) setting to <em>true</em>, then <em>all</em> columns will be targeted and processed, thus hiding any empty column. However, you do have the option to garget specific columns, with either the column name, or the column index. You can also have this targeted array set to be processed, making it a <em>white-list</em> (default), or you can set it to be ignored, making it a <em>black-list</em>.</p>
<p>Here are the existing parameters
|Parameter|Type|Default|Description|
|--- |--- |--- |--- |
|hideEmptyCols|boolean/object|true|Enable/Disable hideEmptyCols plugin (<em>hideEmptyColumns</em> works as well)|
|hideEmptyCols.columns|array|<em>All Columns</em>|Determine which columns to target, can either use the <a href="http://datatables.net/reference/option/columns.name">column name</a>, the <a href="http://datatables.net/reference/api/column(">index</a>.index()), or a negative integer to target columns starting from the right side of the table|
|hideEmptyCols.whiteList|boolean|true|Determine if the targets listed in <em>hideEmptyCols.columns</em> should be treated as a whitelist or blacklist (<em>false</em> will target all columns except those listed)|</p>
<hr>
<h2 id="examples">Examples</h2>
<p><strong>Basic Initialization</strong> - Hide any columns with no values (Since no button was used, the columns would need to be toggled via an API call)</p>
<pre><code class="lang-javascript">$(&#39;#example-1&#39;).DataTable({
    hideEmptyCols: true
});
</code></pre>
<p><strong>Basic w/ ColVis Button</strong> - Target <strong>all</strong> columns &amp; use DataTables <a href="https://datatables.net/reference/button/columnsToggle">ColumnsToggle</a> button</p>
<pre><code class="lang-javascript">$(&#39;#example-1&#39;).DataTable({
    dom: &#39;Bt&#39;,
    buttons: [ &#39;columnsToggle&#39; ],
    hideEmptyCols: true
});
</code></pre>
<p><strong>Targeting Columns via <a href="http://datatables.net/reference/api/column(">indexes</a>.index()) or the position</strong> - This would target the column indexes <em>0</em> and <em>3</em>, as well as the column on the far right side of the table</p>
<pre><code class="lang-javascript">$(&#39;#example-1&#39;).DataTable({
    hideEmptyCols: [ 0, 3, -1 ]
});
</code></pre>
<p><strong>Targeting Columns (Names)</strong> - Target columns via name (Useful for when using JSON or AJAX data src with Objects)</p>
<pre><code class="lang-javascript">$(&#39;#example-1&#39;).DataTable( {
    hideEmptyCols: [&#39;extn&#39;, 5], // Target extension col, and 5th col (salary)
    data: dataSet,
    columns: [
        { title: &amp;quot;Name&amp;quot;, data: &amp;quot;name&amp;quot; },
        { title: &amp;quot;Position&amp;quot;, data: &amp;quot;position&amp;quot; },
        { title: &amp;quot;Office&amp;quot;, data: &amp;quot;office&amp;quot; },
        { title: &amp;quot;Extn.&amp;quot;, data: &amp;quot;extn&amp;quot; },
        { title: &amp;quot;Start date&amp;quot;,  data: &amp;quot;start_date&amp;quot; },
        { title: &amp;quot;Salary&amp;quot;,  data: &amp;quot;salary&amp;quot; }
    ]
} );
</code></pre>
<p><strong>Blacklisting Columns</strong> - Target <em>all</em> columns except column indexes <strong>1</strong> and <strong>3</strong></p></p>
<pre><code class="lang-javascript">$(&#39;#example-1&#39;).DataTable({
    hideEmptyCols: {
        columns: [1,3],
        whiteList: false
    }
});
</code></pre>
<hr>
<h2 id="the-end">The End</h2>
<p>Well, thats it! If you find any bugs or issues, just create an issue in the Git repo, same for feature requests. I dont always see comments on these blog posts</p>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[DataTables Live Ajax Plugin - Keep your AJAX sourced tables up to date]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/datatables-live-ajax-plugin-keep-your-ajax-sourced-tables-up-to-date/</link>
      <guid isPermaLink="true">http://localhost:3333/datatables-live-ajax-plugin-keep-your-ajax-sourced-tables-up-to-date/</guid>
      <category><![CDATA[DataTables]]></category>
      <category><![CDATA[javascript]]></category>
      <category><![CDATA[jQuery]]></category>
      <category><![CDATA[plugins]]></category>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Tue, 17 Nov 2015 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>I&#39;m a big fan of the Open Source jQuery plugin <a href="http://datatables.net/">DataTables</a>. Out of all the jQuery plugins I have used, I feel comfortable saying it&#39;s one of the most stable and powerful.  For those of you who don&#39;t know what DataTables is...</p>
<p>DataTables is a plug-in for the <a href="http://jquery.com/">jQuery</a> Javascript library. It is a highly flexible tool, based upon the foundations of progressive enhancement, and will add advanced interaction controls to any HTML table.</p>
<p>Pretty basic, but very powerful and easy to use. It also comes with an extremely diverse and powerful API, that let&#39;s you control almost every aspect of the table, as well as write your own plugins, which is what I have been getting into lately.</p>
<p>DataTables allows you to pull the source of the tables from the <a href="http://datatables.net/manual/data#DOM">DOM</a>, from a <a href="http://datatables.net/manual/data#Javascript">JSON</a> data structure, or an <a href="http://datatables.net/manual/data#Ajax">AJAX</a> Source. This plugin takes advantage of the AJAX sourced data tables.</p>
<p>I hang out in the <a href="http://datatables.net/forums/">DataTables forums</a> every so often, and I saw a couple requests from people who wanted a way to update just the rows that need updating in the table, and while there is a useful API method, <em><a href="http://datatables.net/reference/api/ajax.reload(">ajax.reload()</a>)</em>, that method will reload the entire table, which may be a little overboard if you have thousands of rows and only a single row needs to be added/updated/removed.</p>
<p>I went ahead and created a new plugin to handle this, <a href="https://github.com/jhyland87/DataTables-Live-Ajax/">DataTables Live Ajax</a>.</p>
<p>To use it, just checkout the git repository, include the JS file called <em>dataTables.liveAjax.js</em>, then when you initialize your DataTables instance, configure the setting called <em>liveAjax</em>.</p>
<h3 id="-git-repo-https-github-com-jhyland87-datatables-live-ajax-"><a href="https://github.com/jhyland87/DataTables-Live-Ajax/">GIT Repo</a></h3>
<hr>
<h2 id="how-it-works">How It Works</h2>
<p>The concept is pretty basic really. Every <em>n</em> milliseconds (default is <em>5000</em>, which is 5 seconds), there will be a new AJAX request to the data source that was configured in the DataTables instance. The new data will be compared to the current data, and the changes will be made accordingly.</p>
<p>Ideally, it would be better to use <a href="https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API">WebSockets</a>, as opposed to iterating over AJAX calls, which is much more &quot;expensive&quot; in terms of resources, but that may not always be optional. I went with the AJAX calls initially because it was quicker than having to setup a WebSocket server, but without a doubt, the WebSockets would be the way to go, and I will be switching over to a WebSocket server in the next revamp (soon!).</p>
<p>One thing you need to realize though, is that the plugin needs to have a way to compare each row to it&#39;s exact counterpart within the new JSON data. If the AJAX source contains <a href="http://datatables.net/examples/ajax/simple.html">arrays for the row data</a>, then that isn&#39;t possible. So what is recommended, is to use <a href="http://datatables.net/examples/ajax/objects.html">objects for the row data</a>, and set the unique row identifier via the <em><a href="http://datatables.net/reference/option/rowId">rowId</a></em> setting, or you can use the default one, which is _DT<em>RowId</em>.  If you don&#39;t set a row ID for your data, then theres no way for the plugin to know what rows need updating, thus the <em>entire</em> table is reloaded if <em>any</em> discrepancies are found in the new data source.. Which is basically the same thing as the <em>ajax.reload()</em> API method.</p>
<p>If you are already familiar with DataTables, and using an AJAX source for your table, then you probably know what I&#39;m talking about.</p>
<hr>
<h2 id="examples">Examples</h2>
<p><em>Basic Example</em> - This will check for any changes in 5 second intervals, and update the data accordingly.</p>
<pre><code class="lang-javascript">$(&#39;#example&#39;).DataTable({
    ajax: &#39;dataSrc.php&#39;,
    rowId: &#39;emp_id&#39;,
    liveAjax: true
});
</code></pre>
<p>Advanced Example - This ones a bit more complicated. Just to show you how you can take advantage of some of the existing options for the plugin.</p>
<pre><code class="lang-javascript">$(&#39;#example&#39;).DataTable({
    ajax: {
        url: &#39;dataSrc.php&#39;,
        type: &#39;POST&#39;,
        data: { dataSrc: &#39;something&#39;},
        dataSrc: &#39;something&#39;
    },
    rowId: &#39;emp_id&#39;,
    liveAjax: {
        // Update every 4.5 seconds
        interval: 4500,
        // Enable DT XHR Callbacks for all AJAX requests
        dtCallbacks: true,
        // Abort the XHR Polling if one of these errors were encountered
        abortOn: [&#39;error&#39;, &#39;timeout&#39;, &#39;parsererror&#39;, &#39;abort&#39;]
    }
});
</code></pre>
<hr>
<h2 id="initialization-options">Initialization Options</h2>
<p>While I was making this, I added an option for almost everything I could think of. This plugin was just created, so while I am sure I may be making some updates, I will try to keep the options the same throughout all the versions, but heres a list of the existing options:
|Parameter|Type|Default|Description|
|--- |--- |--- |--- |
|liveAjax|boolean|true|Enable/Disable liveAjax plugin|
|liveAjax.interval|number|5000|Interval to check for updates (in milliseconds)|
|liveAjax.dtCallbacks|boolean|false|This will determine if the DataTables xhr callbacks should be executed for <em>every</em> AJAX Request|
|liveAjax.abortOn|array|error, timeout, parsererror|Cease all future AJAX calls if one of these statuses were encountered|
|liveAjax.noUpdate|function|<em>N/A</em>|Callback executed when <em>no</em> discrepancies were found in the new JSON data; (Parameters: <em>[object]</em> DataTables Settings, <em>[object]</em> JSON Data for table; <em>[object]</em> XHR Object)|
|liveAjax.onUpdate|function|<em>N/A</em>|Callback executed when discrepancies were found in the new JSON data, and the table was updated; (Parameters: <em>[object]</em> DataTables Settings, <em>[object]</em> Updated/Deleted/Created row data, <em>[object]</em> New JSON Data for table; <em>[object]</em> XHR Object)|</p>
<hr>
<h2 id="liveajax-events">LiveAjax Events</h2>
<p>The LiveAjax plugin fires off it&#39;s own events. Just as all the DataTables events are named <strong>_eventName_.dt</strong>, the LiveAjax events are named <strong>_eventName_.liveAjax</strong>. Heres the current list of events:</p>
<table>
<thead>
<tr>
<th>Event</th>
<th>Description</th>
<th>Parameters</th>
</tr>
</thead>
<tbody>
<tr>
<td>init.liveAjax</td>
<td>Triggered when liveAjax was initiated on a new table</td>
<td><em>[object]</em> Event, <em>[object]</em> DataTables Settings, <em>[object]</em> XHR Object</td>
</tr>
<tr>
<td>xhrErr.liveAjax</td>
<td>Triggered for all XHR Errors</td>
<td><em>[object]</em> Event, <em>[object]</em> DataTables Settings, <em>[object]</em> XHR Object, <em>[string]</em> Error Thrown</td>
</tr>
<tr>
<td>xhrErrTimeout.liveAjax</td>
<td>Triggered when an XHR <em>timeout</em> was encountered</td>
<td><em>[object]</em> Event, <em>[object]</em> DataTables Settings, <em>[object]</em> XHR Object, <em>[string]</em> Error Thrown</td>
</tr>
<tr>
<td>xhrErrError.liveAjax</td>
<td>Triggered when an XHR <em>error</em> was encountered</td>
<td><em>[object]</em> Event, <em>[object]</em> DataTables Settings, <em>[object]</em> XHR Object, <em>[string]</em> Error Thrown</td>
</tr>
<tr>
<td>xhrErrParseerror.liveAjax</td>
<td>Triggered when an XHR <em>parsererror</em> was encountered</td>
<td><em>[object]</em> Event, <em>[object]</em> DataTables Settings, <em>[object]</em> XHR Object, <em>[string]</em> Error Thrown</td>
</tr>
<tr>
<td>xhrErrAbort.liveAjax</td>
<td>Triggered when an xhr <em>abort</em> was encountered</td>
<td><em>[object]</em> Event, <em>[object]</em> DataTables Settings, <em>[object]</em> XHR Object, <em>[string]</em> Error Thrown</td>
</tr>
<tr>
<td>xhrErrUnknown.liveAjax</td>
<td>Triggered when an unknown XHR error was encountered</td>
<td><em>[object]</em> Event, <em>[object]</em> DataTables Settings, <em>[object]</em> XHR Object, <em>[string]</em> Error Thrown</td>
</tr>
<tr>
<td>xhrSkipped.liveAjax</td>
<td>Triggered when an XHR call was skipped</td>
<td><em>[object]</em> Event, <em>[object]</em> DataTables Settings, <em>[string]</em> Reason for skip (<em>paused</em> or <em>processing</em>)</td>
</tr>
<tr>
<td>setInterval.liveAjax</td>
<td>Triggered when the polling interval was changed</td>
<td><em>[object]</em> Event, <em>[object]</em> DataTables Settings, <em>[number]</em> New interval</td>
</tr>
<tr>
<td>clearTimeout.liveAjax</td>
<td>Triggered when the loop timeout has been cleared</td>
<td><em>[object]</em> Event, <em>[object]</em> DataTables Settings, <em>[object]</em> XHR Object</td>
</tr>
<tr>
<td>abortXhr.liveAjax</td>
<td>Triggered when an XHR request is aborted</td>
<td><em>[object]</em> Event, <em>[object]</em> DataTables Settings, <em>[object]</em> XHR Object</td>
</tr>
<tr>
<td>setPause.liveAjax</td>
<td>Triggered when the polling was paused or unpaused</td>
<td><em>[object]</em> Event, <em>[object]</em> DataTables Settings, <em>[boolean]</em> Pause Status</td>
</tr>
<tr>
<td>onUpdate.liveAjax</td>
<td>Triggered when the new JSON changes were implemented</td>
<td><em>[object]</em> Event, <em>[object]</em> DataTables Settings, <em>[object]</em> Created/Deleted/Updated row data, <em>[object]</em> DataTable JSON data, <em>[object]</em> XHR Object</td>
</tr>
<tr>
<td>noUpdate.liveAjax</td>
<td>Triggered when the the table did not need updating</td>
<td><em>[object]</em> Event, <em>[object]</em> DataTables Settings, <em>[object]</em> DataTable JSON, <em>[object]</em> XHR Object</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="liveajax-api-methods">LiveAjax API Methods</h2>
<p>In addition to the already awesome DataTables API, LiveAjax comes with it&#39;s own set of API calls. I added an API method for just about anything I could think of:</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Description</th>
<th>Return</th>
<th>Parameters</th>
</tr>
</thead>
<tbody>
<tr>
<td>iveAjax.initiate()</td>
<td>Start XHR Polling</td>
<td><em>[object]</em> DataTables API</td>
<td><em>None</em></td>
</tr>
<tr>
<td>iveAjax.abortXhr()</td>
<td>Abort Current XHR request</td>
<td><em>[object]</em> DataTables API</td>
<td><em>None</em></td>
</tr>
<tr>
<td>liveAjax.clearTimeout()</td>
<td>Clear the polling loop</td>
<td><em>[object]</em> DataTables API</td>
<td><em>[boolean]</em> Abort current XHR request</td>
</tr>
<tr>
<td>liveAjax.xhrStatus()</td>
<td>Retrieve latest XHR Status</td>
<td><em>[object]</em> DataTables API, <em>[string]</em> XHR Text status</td>
<td><em>None</em></td>
</tr>
<tr>
<td>liveAjax.resume()</td>
<td>Resume Updates</td>
<td><em>[object]</em> DataTables API</td>
<td><em>None</em></td>
</tr>
<tr>
<td>liveAjax.togglePause()</td>
<td>Toggle Pause Status</td>
<td><em>[object]</em> DataTables API</td>
<td><em>None</em></td>
</tr>
<tr>
<td>liveAjax.pause()</td>
<td>Pause XHR Polling</td>
<td><em>[object]</em> DataTables API</td>
<td><em>None</em></td>
</tr>
<tr>
<td>liveAjax.isPaused()</td>
<td>Pause XHR Polling</td>
<td><em>[object]</em> DataTables API, <em>[boolean]</em> Pause Status</td>
<td><em>None</em></td>
</tr>
<tr>
<td>liveAjax.reload()</td>
<td>Reload table</td>
<td>DataTables API Object</td>
<td><em>[function]</em> Callback, <em>[boolean]</em> Reset pagination (default <em>false</em>), <em>[boolean]</em> Force through paused status</td>
</tr>
<tr>
<td>liveAjax.setInterval()</td>
<td>Change update interval</td>
<td>DataTables API Object</td>
<td><em>[integer]</em> New interval (use <em>null</em> to reset to default or config value)</td>
</tr>
</tbody>
</table>
<hr>
<p>Thats it! If you have any issues, I urge you to go to the <a href="https://github.com/jhyland87/DataTables-Live-Ajax/issues">issues</a> section of the <a href="https://github.com/jhyland87/DataTables-Live-Ajax">LiveAjax GIT Repo</a> and submit any questions there.</p>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[Adding JS &amp; CSS Files To CKEditor Content]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/adding-js-css-files-to-ckeditor-content/</link>
      <guid isPermaLink="true">http://localhost:3333/adding-js-css-files-to-ckeditor-content/</guid>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Mon, 24 Aug 2015 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>Recently, I&#39;ve been working on a web-development project (Which I&#39;m omitting from this post), which uses the popular WYSIWYG editor, <a href="http://ckeditor.com/">CKEditor</a>.</p>
<p>A popular issue with WYSIWYG editors, is even though they have display a very accurate depiction of what the content will look like as pure HTML, the content still looks different from what it will be when it&#39;s actually injected into your page, this is typically because your page has CSS and JS to aid in styling, where as the WYSIWYG editor does not.</p>
<p>Like CKEditor, most of the common RTE&#39;s use an iFrame for the content, which is why the CSS/JS is not reflected in the content. I think this actually is done intentionally, to show what it will look like as html exactly, and only html, without any outside interference.</p>
<p>However, I wanted the CSS/JS inside the content. After looking a little bit, I did find a plugin on the CKEditor website, called <a href="http://ckeditor.com/addon/doksoftinclude">CKEditor Include CSS &amp; JS</a>, which seemed to accomplish exactly what I wanted.</p>
<p>Two problems with this plugin...</p>
<ol>
<li>I don&#39;t like to pay for such little software packages. I don&#39;t at all mind paying for things like Jira for example, but this just seemed to minor</li>
<li>We aren&#39;t absolutely sure if the project I&#39;m working on will be Open Source or not, or a mixture. If it&#39;s not, then this would need to be a factor in any 3rd party software, and if I move forward as if its going to be Open Source, and its not, then that means I would have to go back and re-factor a lot of code.</li>
</ol>
<p>So, I decided to just do it myself, which was actually just as easy as it sounds. I can&#39;t believe people pay for software like this, lol.</p>
<p>Before I show the code below and explain it, let me stress that I am NOT a professional web developer, I&#39;m a Linux Engineer, I just so happen to  know a good amount of web development skills...</p>
<h3 id="how-to-use-">How to use...</h3>
<pre><code>1. Set the ckeditor_id value to the ID of the textarea using CKEditor
2. Set the value of container_classes to whatever classes the final HTML will be displayed in when displayed on the web-page. Since the ALL CSS/JS of the page will be reflected in the content of the textarea, it&#39;s important to set this value correctly, or it may look completely different.
3. Copy/Paste the code to wherever you want it to execute.
</code></pre><h3 id="whats-it-do-">Whats it do...</h3>
<ol>
<li>Look through the current page for any CSS files, specifically looking for any <strong><em>&lt;link&gt;</em></strong> tags that have the attribute/value of <strong><em>type=&quot;text/css&quot;</em></strong>, which if done properly, all of the link tags should have (if loading CSS), load the value of <em> <strong>href</strong> </em> into an array.</li>
<li>Do the same thing with Javascript files, looking for any <strong><em>&lt;script&gt;</em></strong> tags with the attribute/value of <em> <strong>type=&quot;text/javascript&quot;</strong> </em>, load the value of <em> <strong>src</strong> </em> into an array.</li>
<li>Find the CKEditor instance by looking for an instance named by the value of the variable <strong>_ckeditor<em>id</em></strong></li>
<li>If you have any extra CKEditor settings, apply them here....</li>
<li>Waits for the CKEditor instance to be fully initialized, this takes longer than window.load or document.ready, so wait for CKEditors <strong><em>instanceReady</em></strong> event handler.</li>
<li>Once initialized, look for the correct frame associated to the CKEditor you specified in the _ <strong>ckeditor_id</strong> <em> variable. This will look for the correct iframe, not just any iframe. Then create a handler referencing the **</em>&lt;head&gt;_** of that iframe</li>
<li>Loop through the array created earlier with the CSS files and create a new <strong><em>&lt;link&gt;</em></strong> tag with the stylesheet source as the source collected earlier</li>
<li>Do the same thing with the array created earlier with the JS files, creating a <em> <strong>&lt;script&gt;</strong> </em> tag with the javascript href as the href collected earlier</li>
</ol>
<p>And finally, you can view the code here:</p>
<pre><code class="lang-javascript">// I find its best to do this in window.load as opposed to document.ready, since all the
// DOM elements are then loaded
$( window ).load(function(){
    // ID of the cKeditor instance
    var ckeditor_id = &#39;your_ckeditor_id&#39;;
    var container_classes = &#39;panel-body panel&#39;;

    var $head = $(&#39;head&#39;);
    var css_files = [];
    var js_files = [];

    // Get all CSS files of the current page
    $head.find(&#39;link[type=&amp;quot;text/css&amp;quot;]&#39; ).each(function(i,css){
        css_files.push($(css).attr(&#39;href&#39;));
    });

    // Then get all the JS files
    $head.find(&#39;script[type=&amp;quot;text/javascript&amp;quot;]&#39; ).each(function(i,js){
        js_files.push($(js).attr(&#39;src&#39;));
    });

    // CKeditor handler
    var $ckeditor = CKEDITOR.instances[ckeditor_id];

    // Add whatever class(es) are of the DIV/SPAN that you will be displaying the content in
    $ckeditor.config.bodyClass = container_classes;

    // Wait for the CKeditor instance to be initiated, it takes a bit longer
    // than just window.load, but it has an event handler
    CKEDITOR.on(&#39;instanceReady&#39;, function(){
        // Find the exact iframe thats used by this ckeditor_id textarea
        var $ckeditor_frame = $(&#39;#cke_&#39; + ckeditor_id).find(&#39;.cke_wysiwyg_frame&#39;);

        // Head element of CKEditors iFrame
        var $ckeditor_head  = $ckeditor_frame.contents().find(&amp;quot;head&amp;quot;);

        // Add all the CSS files..
        $.each(css_files, function(i,css_file){
            $ckeditor_head.append($(&amp;quot;&amp;lt;link/&amp;gt;&amp;quot;, {
                rel: &amp;quot;stylesheet&amp;quot;,
                href: css_file,
                type: &amp;quot;text/css&amp;quot;
            }));
        });

        // Then add all the JS files..
        $.each(js_files, function(i,js_file){
            $ckeditor_head.append($(&amp;quot;&amp;lt;script/&amp;gt;&amp;quot;, {
                src: js_file,
                type: &amp;quot;text/javascript&amp;quot;
            }));
        });

        // DONE!
    });
});
</code></pre>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[Beats Studio Wireless vs Bose AE2W Headset Review]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/beats-studio-wireless-vs-bose-ae2w-headset-review/</link>
      <guid isPermaLink="true">http://localhost:3333/beats-studio-wireless-vs-bose-ae2w-headset-review/</guid>
      <category><![CDATA[apple]]></category>
      <category><![CDATA[beats]]></category>
      <category><![CDATA[wireless]]></category>
      <category><![CDATA[bose]]></category>
      <category><![CDATA[ae2w]]></category>
      <category><![CDATA[headset]]></category>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Sat, 16 May 2015 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>Recently, I went shopping for a new set of headphones. I wanted a pair that I could use for listening to music (obviously), as well as have the ability to use as a BlueTooth headset for phone calls. I knew that BlueTooth was a must, since I hate having those pestering wires hanging from my headset, they always get snagged on everything and just turn into a nightmare. After doing a lot of research and reading, the final decision was between the <a href="http://www.amazon.com/Beats-Studio-Wireless-Headphone-White/dp/B00GBVKM5U">Beats Studio Wireless</a> and the <a href="http://www.amazon.com/Bose-SoundLink-Around-Ear-Bluetooth-Headphones/dp/B00CD1FB26">Bose AE2W</a>. Both sets have very similar sets of features and perform nearly the same. I decided to buy both and try them both out, and return the set I wasn&#39;t pleased with (or at least AS pleased as the set I did decide to keep). I kept both pairs for about a month, trying each one of them for a day or two, then switching to the other pair.</p>
<h2 id="beats-studio-wireless">Beats Studio Wireless</h2>
<p>![Beats_Studio_Wireless][/assets/articles/Beats_Studio_Wireless.jpg]
Beats By Dre, if you&#39;ve been outside of your house and spent more than a couple hours in public, you&#39;re almost definitely going to see some trendy teenager walking around with a set of Beats headphones blasting as loud as can be. I typically try to stay away from trendy technology, due to the fact that the product is usually over priced and designed poorly. However, I will admit that these headphones are a very good product. Right from the beginning, its obvious that Apple tries to portray the quality of the headset from the moment the box is open. Even the box seems high quality, as well as the case that comes with the headset for storing it while its not in use.</p>
<p><strong>MSRP:</strong>
<a href="http://www.beatsbydre.com/headphones/beats-studio-wireless.html?cid=PS_google_+studio%20+wireless%20+beats%20+by%20+dre_BR|USA|X|Headphones|BMMEVERGREEN_USBR|X|Studio-Wireless|WL|X_NA_na&amp;matchtype=b&amp;migCampaign=BR%7CUSA%7CX%7CHeadphones%7CBMM&amp;mtid=901fci19672&amp;cvo_campaign=BR%7CUSA%7CX%7CHeadphones%7CBMM&amp;muid=41D8FE85-E7B2-42AC-8986-7D7D27F811FA&amp;cvosrc=ppc.google.%2Bstudio+%2Bwireless+%2Bbeats+%2Bby+%2Bdre&amp;migMedium=paid-search&amp;migContent=s&amp;Matchtype=b&amp;aosid=p238&amp;migTerm=%2Bstudio+%2Bwireless+%2Bbeats+%2Bby+%2Bdre&amp;cvo_crid=55451302977&amp;migSource=google">$379.95</a>
<strong>BlueTooth:</strong> Yes
<strong>Weight:</strong> 5.3 oz
<strong>Battery Life:</strong> 10~12 Hrs (Wireless)
<strong>Microphone:</strong> Yes
<strong>Battery Guage: </strong>Yes
<strong>Bluetooth Pair Limit:</strong> 1
<strong>Noise Canceling</strong>: Yes</p>
<h3 id="pros-">Pros:</h3>
<ul>
<li><strong>Design</strong> - The Beats are with out a doubt designed very well, I can see these headsets lasting a long while, and even taking a beating.</li>
<li><strong>Sound</strong> - Definitely a requirement for headsets, especially headsets that range in prices close to four hundred dollars!</li>
<li><strong>Battery Life</strong> - The battery life on the beats among the best, I didn&#39;t once have the headset die on me while in use (Granted, I didn&#39;t listen to it for longer than 10-12 hours a day)</li>
</ul>
<h3 id="cons-">Cons:</h3>
<ul>
<li><strong>Comfortability</strong> - Initially, they felt very comfortable, they felt like little pillows on my ears! But after a couple hours of use, I ended up getting a headache. Granted, my ears are larger than the average persons, but I&#39;m not sure that was a factor.</li>
<li><strong>Price</strong> - While you are definitely buying an overall good product, it&#39;s not really any better than some other pairs that cost much less, (EG: <a href="http://www.amazon.com/gp/product/B0076Z78AM?pldnSite=1">Sennheiser MM 550-X</a> or <a href="http://www.amazon.com/gp/product/B00M58CMYC?pldnSite=1">Bose Soundlink</a>). Basically, you&#39;re paying somewhere between $50 to $150 extra just for the brand!</li>
<li><strong>Microphone</strong> - I tested the BlueTooth feature on plenty of calls, the majority of the calls, I ended up having to disable BlueTooth and use the phone itself because the person on the other end couldn&#39;t hear me very well.</li>
</ul>
<hr>
<h2 id="bose-ae2w">Bose AE2W</h2>
<p><img src="/assets/articles/Bose_AE2W.jpg" alt="Bose_AE2W"></p>
<p>I chose the AE2W headphones by Bose as a runner up to the Beats because after reading up on a few reviews or checking out the features, it seemed like a very viable candidate. Plus, I&#39;ve owned a few Bose products in my day, and I have yet to have any issues with a single one of them. One of my favorite features that the AE2W has that I couldn&#39;t find on the Beats (Or a number of other BlueTooth headphones), would be the ability to connect with more than one device via BlueTooth. Aside from the not-so-pretty BlueTooth unit that noticeably protrudes from the left side, I had a hard time finding anything about this unit that I didn&#39;t like.</p>
<p><strong>MSRP:</strong> <a href="http://www.amazon.com/Bose-SoundLink-Around-Ear-Bluetooth-Headphones/dp/B00CD1FB26">$249.95</a>
<strong>BlueTooth:</strong> Yes
<strong>Weight:</strong> 9.17 oz
<strong>Battery Life:</strong> 8~10 Hrs (Wireless)
<strong>Microphone:</strong> Yes
<strong>Battery Guage:</strong> No
<strong>Bluetooth Pair Limit:</strong> 2
<strong>Noise Canceling:</strong> No</p>
<h3 id="pros-">Pros:</h3>
<ul>
<li><strong>Design</strong> - The Boes are with out a doubt designed very well, I can see these headsets lasting a long while, and even taking a beating.</li>
<li><strong>Sound</strong> - These are created by Bose... need I say more?</li>
<li><strong>Price</strong> - For all of the bells and whistles included with the AE2W headset, the price is very reasonable. After purchasing the Beats shown above, I had buyers remorse almost the very next day! However, I was very happy with my purchase of the Bose.</li>
<li><strong>Comfortability</strong> - With the Beats, I got a headache after a couple hours of use, especially if I fell asleep with them on. But with the Bose, I could wear them all day, and at the end of the day, there was no headache or earache to complain about.</li>
<li><strong>BlueTooth Multi Pair</strong> - One of my favorite features, the ability to pair multiple devices with the headset. I can pair my iPod and listen to music, and have my Android phone paired as well. When I&#39;m listening to music on my iPod and receive a call, the headset will alert me to the incoming call, and allow me to switch over to my phone and accept the call.</li>
<li><strong>BlueTooth Unit</strong> - The BlueTooth unit actually can detach from the headset itself. This is useful because if the battery dies, you can just detach it from the headset to charge it, and plug your headset into your audio device for use. Most headphones that come with BlueTooth don&#39;t come with this feature.</li>
<li><strong>Microphone</strong> - Worried that this microphone might be just as inefficient as the microphone on the Beats, I tested it extensively. Thankfully, I wasn&#39;t let down at all. Most of the people I talked to couldn&#39;t tell the difference from when I was using BlueTooth as opposed to using the phone without BlueTooth.</li>
</ul>
<h3 id="cons-">Cons:</h3>
<ul>
<li><strong>Appearance -</strong> Though it&#39;s definitely not an ugly pair of headphones, it&#39;s not nearly as pretty or presentable as the Beats or some other comparable sets of headphones.</li>
<li><strong>No Battery Gauge</strong> - This may not be a very big con, but it&#39;s a con. It does get a little annoying not knowing how long you have left on your battery, and having your headset just die on you.</li>
<li><strong>Not Noise Canceling</strong> - Yep, no noise canceling on these babies, sometimes you find yourself listening to music while &quot;in the zone&quot;, only to get interrupted by the slight noise of the outside world. <em>Solution? Turn up the volume!</em></li>
</ul>
<hr>
<h2 id="conclusion">Conclusion</h2>
<p>At the end of the day, I ended up going with the Bose. Why? After using both of the headsets extensively for about a month, the only feature that the Beats had that was noticeably absent in the Bose, would be the noise canceling feature. The features that the AE2W had that the Bose lacked, were: sound quality, comfortability and the ability to pair to multiple devices at once. So while the noise canceling feature is handy, it&#39;s definitely not a deal breaker, and doesn&#39;t out weigh the other features that the AE2W came with, especially if you include the price difference between the two headsets. Theres no doubt that both headsets are high quality headphones, but with the Beats, you&#39;re paying around $100 extra just for the &quot;B&quot; on the side, which some people don&#39;t mind paying extra just to show off the brand, It&#39;s something of a fashion statement... But I decided to take the logical route and get more for less!</p>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[Installing Docker and running your First Image]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/installing-docker-and-running-your-first-image/</link>
      <guid isPermaLink="true">http://localhost:3333/installing-docker-and-running-your-first-image/</guid>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Mon, 06 Apr 2015 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>The technology world moves so fast, especially now. It also seems that technology comes and then goes, but then comes back even better than it use to be. Take Docker for an example, it&#39;s a container technology that allows developers and sysadmins to build, ship, and run distributed applications. This is much like LXC, but on steroids. If you haven&#39;t played with Docker yet, now is the time to do so because it is extremely exciting!</p>
<h3 id="installing-docker">Installing Docker</h3>
<p>There are a couple of ways to install docker, depending on the operating system that you&#39;re using. All of which make it super stupid simple.</p>
<p>On RHEL/CentOS 6:</p>
<pre><code class="lang-bash">$ sudo yum install epel-release
$ sudo yum install docker-io
</code></pre>
<p>(Note, for CentOS-6 there is a package name conflict with a system tray application and its executable, so the Docker RPM package was called).</p>
<p>On RHEL/CentOS 7:</p>
<pre><code class="lang-bash">$ sudo yum install docker
</code></pre>
<p>If you want the instructions for other operating systems, visit the docker page by clicking <a href="https://docs.docker.com/installation/">here</a>.</p>
<h3 id="getting-your-first-image">Getting your first image</h3>
<p>Now that you have docker installed, you can search the hub, which is the public repository where everyone can submit images to:</p>
<pre><code class="lang-bash">$ docker search nginx
NAME                              DESCRIPTION                                     STARS     OFFICIAL   AUTOMATED
nginx                             Official build of Nginx.                        746       [OK]
jwilder/nginx-proxy               Automated Nginx reverse proxy for docker c...   177                  [OK]
maxexcloo/nginx-php               Docker framework container with Nginx and ...   31                   [OK]
...
</code></pre>
<p>Once you&#39;ve found the name of the image you want to get, then simply pull it down:</p>
<pre><code class="lang-bash">$ docker pull nginx
</code></pre>
<p>You&#39;ll see a lot of hashes and &quot;Downloading&quot; and &quot;Download completes&quot; while it downloads.. and then once it&#39;s complete, you should see something similar to:</p>
<pre><code class="lang-bash">nginx:latest: The image you are pulling has been verified. Important: image verification is a tech preview feature and should not be relied on to provide security.
Status: Downloaded newer image for nginx:latest
</code></pre>
<p>Then you can run the following to list all of your local images available:</p>
<pre><code class="lang-bash">$ docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
nginx               latest              224873bdcaa1        6 days ago          93.44 MB
</code></pre>
<h3 id="using-your-image">Using your image</h3>
<p>Now that you have your image locally, you can initiate a shell prompt in interactive mode by running the following command:</p>
<pre><code class="lang-bash">$ docker run -t -i nginx:latest /bin/bash
root@b58f7e38c058:/#
</code></pre>
<p>Note, that you can run standard linux commands based on the image that you pulled. This particular nginx image is actually debian based so you can run apt-get update or any other debian commands you&#39;d like. However, none of your changes will be saved once you exit out of the interactive shell. In the next article, I&#39;ll go over writing changes to your images.</p>
<p>Alliteratively to the interactive shell, you can run the image in daemon mode like so:</p>
<pre><code class="lang-bash">$ docker run --name some-nginx -d nginx
</code></pre>
<p>There is more to the nginx image that you&#39;ll need to do, to get your content there, but that&#39;s the general idea. The best way to understand what all you can and can&#39;t do with an image is to visit the docker hub website and view the readme file which will explain how to do things with the image. You can see that at: <a href="https://hub.docker.com">https://hub.docker.com</a></p>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[Alternatives To Commonly Used Linux Commands]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/alternatives-to-commonly-used-linux-commands/</link>
      <guid isPermaLink="true">http://localhost:3333/alternatives-to-commonly-used-linux-commands/</guid>
      <category><![CDATA[dig]]></category>
      <category><![CDATA[host]]></category>
      <category><![CDATA[htop]]></category>
      <category><![CDATA[less]]></category>
      <category><![CDATA[more]]></category>
      <category><![CDATA[mtr]]></category>
      <category><![CDATA[nslookup]]></category>
      <category><![CDATA[top]]></category>
      <category><![CDATA[traceroute]]></category>
      <category><![CDATA[tracert]]></category>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Tue, 01 Jul 2014 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>In my opinion, one of the best things about Unix/Linux, is there&#39;s typically always more than one correct way to do something. Sometimes, some ways are better than others, or quicker or more efficient or reliable, but it&#39;s common to get locked into doing something one specific way just because you&#39;ve done it repeatedly. There&#39;s always alternative methods of accomplishing a task, like if you wanted to find out what ports are open on a server, depending on if you are on the server itself or not, you have lsof, nmap, netstat</p>
<p>I like to research alternative methods of accomplishing common tasks, just to see if there&#39;s a better way of doing.... whatever it is I&#39;m doing.</p>
<h2 id="traceroute-vs-mtr">Traceroute vs. MTR</h2>
<p>The <em><a href="http://linux.die.net/man/8/traceroute" title="Traceroute Man Page">traceroute</a></em> command is probably one of the most commonly used commands for diagnosing network related issues. It&#39;s a fairly simple command, you just run something like &#39;traceroute google.com&#39; and it</p>
<p>Screenshot of the difference between the output of traceroute and the output of mtr
<img src="/assets/articles/traceroute_vs_mtr.png" alt="Screenshot of the difference between the output of traceroute and the output of mtr"></p>
<p>will print the route packets trace to whatever host you specified (google.com, in my example).</p>
<p>What if you are waiting for something to change though? Most administrators would combined <em>traceroute</em> with the <em><a href="http://linux.die.net/man/1/watch" title="Linux Watch Man Page">watch</a></em> command, or even worse, hit the up arrow and hit enter about a thousand times.</p>
<p>There&#39;s a great alternative to this, it&#39;s a command called <a href="http://linux.die.net/man/8/mtr" title="Linux Man Page MTR">mtr</a>. <em>mtr</em> isn&#39;t always installed by default, so you may need to use your package installer to install it.</p>
<p><em>mtr</em> is essentially a live feed of the <em>traceroute</em> output (Screenshot above). It also will show you packet loss on each hop in a live feed, which is what you would alternatively be using <em>ping</em> to do, so you could say this accomplishes both <em>traceroute</em> and <em>ping</em>. This is very useful when you&#39;re waiting for something to change, or if you need to actively monitor the path to a server, etc.</p>
<hr>
<h2 id="top-vs-htop">Top vs. Htop</h2>
<p>Even more popular than the <em>traceroute</em> command would be top. I think top is probably the first command you learn as a sys admin, as far as diagnostics goes. Top will show you the load averages, the top</p>
<p>Difference in the output of the commands top and htop
<img src="/assets/articles/top_vs_htop.png" alt="Command outputs of TOP and HTOP"></p>
<p>Resource intensive processes, the users, pids, memory utilization, CPU utilization, pretty much everything you need to know. If you want to manipulate the output (Such as sort by CPU or Memory, only show specific user processes, or show a tree view for example), that can only be accomplished using the arguments associated with top when you run the command itself.</p>
<p>An awesome alternative to <em>top</em>, is <a href="http://linux.die.net/man/1/htop" title="Linux Command Htop Man Page">htop</a>. Htop will provide you with essentially the same functionally as top, except the interface of htop is fully interactive! You have the ability to actively manipulate the output or interact with the processes in the output, using the F-keys:</p>
<p><strong>F4</strong> lets you filter the output so you can look for specific commands; <strong>F5</strong> will display the processes in a tree view, showing each parent and child process; <strong>F6</strong> lets you pick a column to sort by, and how to sort it; <strong>F7</strong> and <strong>F8</strong> let you manipulate the nice level of the process; <strong>F9</strong> lets you pick a process to terminate, as well as which kill signal to send, and you can escape htop with <strong>F10</strong></p>
<p>Just like mtr (and most of the commands listed here), you will need to install this using your package manager.</p>
<p><strong>Note</strong>: <em>htop</em> will only display processes that it has permissions to, so if you want to see all of the processes on the system, be sure to use sudo. I would just recommending adding an alias to your ~/.bash_profile to alias htop=&quot;sudo htop&quot;</p>
<p>Also...  Another alternative to <em>top</em>, for monitoring system utilization and resources, would be <em><a href="http://linux.die.net/man/1/atop" title="Linux Man Page Command atop">atop</a></em>. The output of <em>atop</em> is much more informational than the standard <em>top</em> command, but its not at all interactive.</p>
<hr>
<h2 id="less-vs-more">Less vs More</h2>
<p>This ones pretty simple actually. Both less and <em>more</em> are just simply pagers. The key difference between Less and More is, More permits you to view the contents of the file, while scrolling down only, you can not scroll up in More. However, with Less, you can scroll up AND down.</p>
<p>I think the irony in the names is just a simple pun, <em>less</em> is <em>more</em>, and <em>more</em> is <em>less</em>!..</p>
<hr>
<h2 id="nslookup-vs-dig-host">Nslookup vs Dig &amp; Host</h2>
<p><img src="/assets/articles/Nslookup_VS_Dig_VS_Host.png" alt="Nslookup_VS_Dig_VS_Host"></p>
<p><a href="http://linux.die.net/man/1/nslookup" title="Nslookup man page">Nslookup</a> is probably one of the most popular commands for any sysadmin. It&#39;s a fairly simple command, all it does is return the DNS record of whatever is provided to it as an argument.</p>
<p><a href="http://linux.die.net/man/1/dig" title="Dig man page">Dig</a> is pretty simple to use, and a lot more informational than <em>nslookup</em>. It gives you a more in-depth response about the answer provided; Regurgitates the query you requested back to you; Gives you the answers under the answer section; As well as which server was used to query, and the time the query was performed. Yes, some of this is provided by <em>nslookup</em>, but not all of it.</p>
<p>The <a href="http://linux.die.net/man/1/host" title="Host command man page">host</a> command basically simplifies both nslookup and dig about as much as it can, which is great for scripting or whenever you need to get right down to the point.</p>
<p>If this isn&#39;t enough as it is... <em>nslookup</em> is deprecated. The organization that maintains the code for <em>nslookup</em>, Internet Systems Consortium, has very clearly stated so. So I would think that says enough as it is.</p>
<p><strong>Source:</strong> <a href="http://blog.smalleycreative.com/linux/nslookup-is-dead-long-live-dig-and-host/">blog.smalleycreative.com</a></p>
<hr>
<h2 id="di-vs-df-and-du-">Di vs Df (and Du)</h2>
<p><img src="/assets/articles/df_vs_du_vs_di.png" alt="Output comparison between DU, DF and DI" title="DU, DF and DI"></p>
<p>Pretty much everyone knows about <a href="http://linux.die.net/man/1/df">df</a> but theirs one I just learned about. it&#39;s called <a href="http://linux.die.net/man/1/di">di</a>, which means Disk Information, (I bet you could have guessed that :-P)</p>
<p>df is a very popular command, used by administrators and scripts very frequently, but still it doesn&#39;t provide some excellent and useful features like actual disk space that is available to each user, various useful display formats etc.</p>
<p>You can see in the screenshot on the right, the information provided by the commands <em>df</em>, <em>du</em> and <em>di</em>. <em>Di</em> provides more information, and is just as quick as <em>df</em>, and accurate as <em>du</em>, (I&#39;m not quite sure how that&#39;s possible... yet, if you know, please comment on this post).</p>
<hr>
<h2 id="vi-vs-vim">Vi vs VIM</h2>
<p><img src="/assets/articles/youcompleteme.gif" alt="VIM Plugin YouCompleteMe" title="VIM Plugin YouCompleteMe"></p>
<p>At first glance to me, VIM was nothing more than VI with syntax highlighting. But after further research, I found some differences than count for more than just syntax highlighting.</p>
<p>VI comes installed on most OS&#39;s by default, but its not nearly as powerful as VIM.</p>
<p>VIM allows plugins, plugins that won&#39;t work with just the simple VI binary by itself. Getting the list of plugins is a while other post, but ill tell you right now that one worth checking into is called <a href="https://github.com/Valloric/YouCompleteMe">You Complete Me</a>. Heres a quick snippet of what YCM is...</p>
<blockquote>
<p>YouCompleteMe is a fast, as-you-type, fuzzy-search code completion engine for <a href="http://www.vim.org/">Vim</a>. It has several completion engines: an identifier-based engine that works with every programming language, a semantic, <a href="http://clang.llvm.org/">Clang</a>-based engine that provides native semantic code completion for C/C++/Objective-C/Objective-C++ (from now on referred to as &quot;the C-family languages&quot;), a <a href="https://github.com/davidhalter/jedi">Jedi</a>-based completion engine for Python, an <a href="https://github.com/nosami/OmniSharpServer">OmniSharp</a>-based completion engine for C# and an omnifunc-based completer that uses data from Vim&#39;s omnicomplete system to provide semantic completions for many other languages (Ruby, PHP etc.).</p>
</blockquote>
<p>Just a couple more cool things that VIM can accomplish that you can&#39;t do directly with VI.. You may have to have all of the VIM packages vim-full. I would just use yum or apt-get to install vim-*, I think  thats around 25 packages, depending on your distro.</p>
<ul>
<li>Editing files on other locations using network protocols such as SSH, SCP or even HTTP</li>
<li>You can modify files inside compressed archives. as-is</li>
<li>VIM is becoming more and more common, and being ported to much more distros of OS than VI is</li>
<li>VIM comes with something called &#39;vimdiff&#39;. which allows you to compare two files</li>
<li>With VIM, you can split the screen for editing multiple files, which is actually very handy when having to do things at the console, and you cant use terminator. Just have to learn the controls, somewhat like <a href="http://linux.die.net/man/1/screen">screen</a>.</li>
</ul>
<p>Thats all I have for now. I definitely prefer VIM to VI, but as long as you don&#39;t use NANO... you&#39;re OK in my book ;-)</p>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[Recover Deleted Files With LSOF]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/recover-deleted-files-with-lsof/</link>
      <guid isPermaLink="true">http://localhost:3333/recover-deleted-files-with-lsof/</guid>
      <category><![CDATA[deleted]]></category>
      <category><![CDATA[descriptors]]></category>
      <category><![CDATA[file]]></category>
      <category><![CDATA[files]]></category>
      <category><![CDATA[forensics]]></category>
      <category><![CDATA[recovery]]></category>
      <category><![CDATA[Linux]]></category>
      <category><![CDATA[lsof]]></category>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Mon, 23 Jun 2014 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p><img src="/assets/articles/foren.jpg" alt="Forensics"></p>
<p>Linux has a very lengthy list of powerful commands and tools, some are specific to certain distributions and thus, can&#39;t be used on every server. One of my favorite commands would be <a href="http://linux.die.net/man/8/lsof" title="Man Page - lsof (List Open Files)">lsof</a>, which stands for <em>List Open Files</em>. It&#39;s an extremely useful and powerful command, which means it takes some practice to get familiar with it, but it&#39;s worth it! <em>It&#39;s one of those commands that every Linux Sysadmin should know!</em> Today, I made a mistake, thank God I was able to do some research and recover the data I deleted. I typically use <a href="http://linuxcommand.org/man_pages/tar1.html">tar</a> to create tar.gz format files, to archive and compress, very rarely do I gzip files or deal with just gzip&#39;d files.</p>
<p>I gunzip&#39;d a gz&#39;d log file, and opened it up in less and started poking around. Then I went into another terminal and deleted the file I was looking at with less.</p>
<p>I forgot that when you gunzip a file, you delete the .gz version of the file itself, so you shouldn&#39;t just delete it, you should gzip it again.</p>
<p>So at first thought, the file was gone!... Until I learned a little trick. You can recover files with open file descriptors using lsof.</p>
<p>Basically, when you delete a file that&#39;s open by another process, it deletes the inode, not the data on the file that the inode contains the location to.</p>
<p>Lets show by example...</p>
<p>Execute..</p>
<pre><code class="lang-bash">$ echo &#39;this is a test file&#39; &gt; /tmp/delete-me.txt &amp;&amp; less /tmp/delete-me.txt
</code></pre>
<p>Now open another console session <strong>(important)</strong>, sudo into root, and cd into /tmp and delete delete-me.txt</p>
<pre><code class="lang-bash">$ rm /tmp/delete-me.txt
</code></pre>
<p>Now if you know much about the lsof command, you know the command  <em>lsof +L1</em> will show you all the open file descriptors to deleted files (<a href="http://www.linuxdigest.org/2014/02/commands-du-df-whats-difference/">Also covered in this article</a>) , as well as the process ID, command and the file descriptor. So run this command and grep for your deleted file.</p>
<pre><code class="lang-bash">$ lsof +L1 | grep delete-me.txt
less 12780 jhyland 4r REG 8,6 20 0 4980757 /tmp/delete-me.txt (deleted)
</code></pre>
<p>Pay attention to the columns with the PID (second) and the File Descriptor (fourth).</p>
<p>This information helps you find the information inside the /proc directory, using the PID, and the FD, you can recover the data (if its still being held open by whatever command is using it, in this case, less)</p>
<p>Lets take a look at the file descriptor...</p>
<pre><code class="lang-bash">$ sudo cat /proc/12780/fd/4
this is a test file
</code></pre>
<p>There ya go! The &#39;deleted&#39; data is right there. You can cat the file descriptor and redirect the output to another file.</p>
<pre><code class="lang-bash">$ sudo cat /proc/12780/fd/4 &gt; /tmp/recovered-file.txt
</code></pre>
<p>File recovered :)</p>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[Setting Up Conky On Debian Linux]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/setting-up-conky-on-debian-linux/</link>
      <guid isPermaLink="true">http://localhost:3333/setting-up-conky-on-debian-linux/</guid>
      <category><![CDATA[conky]]></category>
      <category><![CDATA[debian]]></category>
      <category><![CDATA[desktop]]></category>
      <category><![CDATA[graphics]]></category>
      <category><![CDATA[statistics]]></category>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Fri, 06 Jun 2014 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>What is <em><strong><a href="http://conky.sourceforge.net/">Conky</a></strong></em>?..</p>
<blockquote>
<p>Conky is a free, light-weight system monitor for X, that displays any information on your desktop. Conky is licensed under the GPL and runs on Linux and BSD.</p>
</blockquote>
<p>I stumbled upon Conky when I was setting up my new laptop with Linux Mint, I was looking for the Linux equivalent to <a href="http://projects.tynsoe.org/en/geektool/">GeekTool for Mac</a>. While Conky is much more complicated than GeekTool is, it&#39;s much more powerful and provides a wide range of options, all very customizable.</p>
<h1 id="conky-setup-and-configured-on-a-debian-os">Conky Setup and configured on a Debian OS</h1>
<p><img src="/assets/articles/conky-screenshot.png" alt="Conky Setup and configured on a Debian OS"></p>
<p>Here&#39;s a screenshot of my desktop to the right, after I got Conky setup and configured correctly...</p>
<p><strong>To Install Conky</strong>... It&#39;s pretty basic, you can install it via APT...</p>
<pre><code class="lang-bash">$ sudo apt-get install conky
</code></pre>
<p><strong>Customizing Conky</strong> can get a little tricky. Start by creating a ~/.conkyrc file, which may or may not exist after you install Conky, (I don&#39;t think it gets created, but I don&#39;t remember for sure. If it&#39;s not there, then Conky will assume default settings for everything.. which is pretty ugly). Here&#39;s my custom Conky configuration file: <a href="/conky_config.txt">Conky Configuration File</a>  </p>
<p>At the top of the file are the settings for the Conky application iself, (As opposed to the display content/settings). Everything prior to the line that just contains &quot;<strong>TEXT</strong>&quot;.  </p>
<p><a href="http://conky.sourceforge.net/config_settings.html">Heres</a> a page on Conkys website that reference each of the setting variables, and what they represent. Most of these settings are pretty self explanatory, but be sure to poke around a bit just to be sure.</p>
<p><strong>Note</strong>: You will notice that if you edit the ~/.conkyrc file while Conky is running, then save it, Conky will automatically reload with the new settings.</p>
<hr>
<h2 id="examples">Examples</h2>
<p>These are some creative examples of how customizable Conky can be.</p>
<p><img src="/assets/articles/Conky_Example.jpg" alt="Conky Example">
<img src="/assets/articles/Conky_Example-1.jpg" alt="Conky Example">
<img src="/assets/articles/Conky_Example-2.png" alt="Conky Example">
<img src="/assets/articles/Conky_Example-3.png" alt="Conky Example">
<img src="/assets/articles/Conky_Example-4.jpg" alt="Conky Example">
<img src="/assets/articles/Conky_Example-5.png" alt="Conky Example">
<img src="/assets/articles/Conky_Example-6.jpg" alt="Conky Example">
<img src="/assets/articles/Conky_Example-7.png" alt="Conky Example">
<img src="/assets/articles/Conky_Example-8.png" alt="Conky Example">
<img src="/assets/articles/Conky_Example-9.png" alt="Conky Example">
<img src="/assets/articles/Conky_Example-10.jpg" alt="Conky Example">
<img src="/assets/articles/Conky_Example-11.png" alt="Conky Example">
<img src="/assets/articles/Conky_Example-12.jpg" alt="Conky Example">
<img src="/assets/articles/Conky_Example-13.jpg" alt="Conky Example">
<img src="/assets/articles/Conky_Example-15.png" alt="Conky Example"></p>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[How to check if ETags are being served]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/how-to-check-if-etags-are-being-served/</link>
      <guid isPermaLink="true">http://localhost:3333/how-to-check-if-etags-are-being-served/</guid>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Sat, 24 May 2014 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>We all know of the Apache ETag vulnerability, and if you don&#39;t know how to fix it, it&#39;s pretty quite simple -- all you need to do is add: ETag None to your apache vhost configuration and restart apache. But one of the questions came across of, how do we validate that this fix actually happened or not? It&#39;s actually quite simple, all you need to do is curl and look at the headers that are returned from the server to see if it&#39;s there or not..</p>
<p>Example of before the fix:</p>
<pre><code class="lang-bash">$ curl -I https://hostname.com/ -k
HTTP/1.1 200 OK
Date: Thu, 20 Feb 2014 17:45:10 GMT
Server: Apache
Last-Modified: Tue, 12 Aug 2008 22:07:29 GMT
ETag: &amp;quot;802a3-83b-99a202a5&amp;quot;
Accept-Ranges: bytes
Content-Length: 875
Content-Type: text/html; charset=UTF-8
</code></pre>
<p>Example of after the fix:</p>
<pre><code class="lang-bash">$ curl -I https://hostname.com/ -k
HTTP/1.1 200 OK
Date: Thu, 20 Feb 2014 17:45:22 GMT
Server: Apache
Last-Modified: Tue, 12 Aug 2008 18:17:46 GMT
Accept-Ranges: bytes
Content-Length: 875
Content-Type: text/html; charset=UTF-8
</code></pre>
<p>As you can see above, the first curl command returns the ETag information, and the second one does not and this will allow you to close out that vulnerability! :)</p>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[Estimate how long shred or rm will take on millions of files]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/estimate-how-long-shred-or-rm-will-take-on-millions-of-files/</link>
      <guid isPermaLink="true">http://localhost:3333/estimate-how-long-shred-or-rm-will-take-on-millions-of-files/</guid>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Sun, 18 May 2014 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>Today I came across a challenge, try and figure out how long it will take to remove millions of files from our systems. We are all about security, so we need to run shred rather than rm when we remove files to ensure they are securely deleted. If you don&#39;t know what shred is, it&#39;s an application on linux that allows you to securely delete any file from your system. It will do, by default, 3 passes over the file to ensure that the file is completely unrecoverable if anyone tried to. The problem though is, when you&#39;re doing this to millions of files (in our case, it was roughly 43million) it is extremely slow, and people will ask you to try and figure out how long it would take for the command to finish..</p>
<p>I was asked this very question quite a few times and told them I would try and get some sort of estimate, and this is the best way that I managed to do it, and it seemed to be close to the actual time it took for the files to be shredded. Please note, there is one very HUGE issue with this estimate.. and that is that it can CHANGE AT ANY GIVEN TIME based on the load and the I/O of the system. So when you provide this information to someone, tell them that it is an extremely rough estimate just to tell you what the current speed of shredding or deletion at that point in time.</p>
<p>Anyway, here is the script that we used to tell us this information. As a note, start running your shred or rm command in a screen window and then run this script on the same server/node.</p>
<pre><code class="lang-bash">#!/bin/bash

a=$(df | grep &quot;/folder&quot; | awk &#39;{print $3}&#39;)
sleep 60
b=$(df | grep &quot;/folder&quot; | awk &#39;{print $3}&#39;)

blocks_per_minute=$((a-b))
minutes_left=$((blocks_left/blocks_per_minute))
hours_left=$((minutes_left/60))
days_left=$((hours_left/24))

echo &quot;Shred Estimate&quot;
echo &quot;------------------------------------&quot;
echo &quot;Blocks per minute: $blocks_per_minute&quot;
echo &quot;Blocks left: $blocks_left&quot;
echo &quot;Minutes left: $minutes_left&quot;
echo &quot;Hours left: $hours_left&quot;
echo &quot;Days left: $days_left&quot;
</code></pre>
<p>The above will produce the following output:</p>
<p>[```bash</p>
<h2 id="shred-estimate">Shred Estimate</h2>
<p>Blocks per minute: 70632
Blocks left: 805646648
Minutes left: 11406
Hours left: 190
Days left: 7</p>
<p>```</p>
<p>I hope this helps someone at some point, and I am definitely not claiming to be the expert at this, but this seemed to work for us.</p>
<p>Let us know if you know of a better way of doing this in the comments.</p>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[Bash Trick - Display Timestamp For Each Command In History]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/bash-trick-display-timestamp-for-each-command-in-history/</link>
      <guid isPermaLink="true">http://localhost:3333/bash-trick-display-timestamp-for-each-command-in-history/</guid>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Thu, 24 Apr 2014 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>Ever go through your <em>history</em>, and wish you knew the exact date and time a given command was typed? Well theres actually a fairly simple trick. It&#39;s the global <em>HISTTIMEFORMAT</em> variable.</p>
<p>To test it out, copy and paste the following into your bash prompt, type a few commands, then type <em>history</em> again.</p>
<pre><code class="lang-bash">$ HISTTIMEFORMAT=&quot;%d/%m/%y %T &quot;
</code></pre>
<p>You should see the typical history output, with the date and time prepended to each line. Heres an example from my <em>history</em> output.</p>
<pre><code class="lang-bash">  497  24/04/14 01:16:46 man nmap
  498  24/04/14 01:16:46 sudo su -
  499  24/04/14 01:16:46 sudo su -
  500  24/04/14 01:16:46 exit
  501  24/04/14 01:16:53 clear
  502  24/04/14 01:16:54 HISTTIMEFORMAT=&quot;%d/%m/%y %T &quot;
  503  24/04/14 01:16:57 ls
  504  24/04/14 01:17:04 whoami
  505  24/04/14 01:17:17 ps aux |grep bla
  506  24/04/14 01:17:20 history |tail
</code></pre>
<p>Now if you want this to be permanent, then execute the following, to add it to your _.bash<em>profile</em></p>
<pre><code class="lang-bash">$ echo &#39;export HISTTIMEFORMAT=&quot;%d/%m/%y %T &quot;&#39; &gt;&gt; ~/.bash_profile
</code></pre>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[DU vs. DF - Which Ones Right? Which To Trust?]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/du-vs-df-which-ones-right-which-to-trust/</link>
      <guid isPermaLink="true">http://localhost:3333/du-vs-df-which-ones-right-which-to-trust/</guid>
      <category><![CDATA[df]]></category>
      <category><![CDATA[disk]]></category>
      <category><![CDATA[usage]]></category>
      <category><![CDATA[du]]></category>
      <category><![CDATA[partition]]></category>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Mon, 24 Feb 2014 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p><a href="http://linux.die.net/man/1/du" title="Man page for command &#39;du&#39;">du</a> and <a href="http://linux.die.net/man/1/df" title="Man page for command &#39;df&#39;">df</a> are both basic Linux commands that come pre-installed on every flavor of Linux (to the best of my knowledge).</p>
<p>Snippet from the man page for <a href="http://linux.die.net/man/1/du" title="Man page for command &#39;du&#39;">du</a>...</p>
<blockquote>
<p>NAME
    du -- display disk usage statistics</p>
<p>DESCRIPTION
    The du utility displays the file system block usage for each file argument and for each directory in the file hierarchy rooted in each directory argument.  If no file is speci-
    fied, the block usage of the hierarchy rooted in the current directory is displayed.</blockquote></p>
</blockquote>
<p>And then for <a href="http://linux.die.net/man/1/df" title="Man page for command &#39;df&#39;">df</a>...</p>
<blockquote>
<p>NAME
    df -- display free disk space</p>
<p>DESCRIPTION
    The df utility displays statistics about the amount of free disk space on the specified filesystem or on the filesystem of which file is a part.  Values are displayed in
    512-byte per block counts.  If neither a file or a filesystem operand is specified, statistics for all mounted filesystems are displayed (subject to the -t option below).</p>
</blockquote>
<p>These commands are used pretty much every day, either by a SysAdmin troubleshooting an issue or setting up an application, or scripts that need to know the available or used disk space.</p>
<p>...But have you ever noticed that sometimes (... every time) they show results are different?</p>
<p>Typically (Not always, but usually), the size reported by <em>df</em> will be more than whats reported by <em>du</em>, but it&#39;s very rare that they both report the same disk usage. Actually, personally... I have never seen them report the same size, and I&#39;ve managed more Linux servers than I can even begin to count. Both of the commands use a different &quot;ruler&quot; per say when they determine the size of said folder/mount. So depending on what you&#39;re trying to get exactly, they can both be correct.</p>
<p><strong>NOTE:</strong> In the output of any of my examples of <em>du</em>, I use the <em>-s</em> flag to display the <em>summary</em> of said directory, as opposed to outputting a tree-like display of the contents of the directory, and then the size of each of the files/folders. Then the <em>-h</em> flag will display the space in human-readable format (for both <em>du</em> and <em>df</em>).</p>
<p>There are a few reasons behind this, so lets go over it in some detail....=</p>
<hr>
<h4 id="reason-1">Reason # 1</h4>
<p>Files in memory will be included in the output of <em>df</em>. <em>du</em> doesn&#39;t account for the files in memory, just the files that are actually on disk.</p>
<hr>
<h4 id="reason-2">Reason # 2</h4>
<p>The command <em>df</em> will include the size of deleted files with open file descriptors. So hypothetically, if you were to have a large file (Lets say... a 4GB log file... because someone didnt enable logrotate), and you open that file, then someone steps on your toes and accidentally deletes that same 4GB log file (Or right then, it finally gets rotated), <em>df</em> will still include the size of the file as if it was still there, thus, reporting an improper folder/mount point size.</p>
<p><strong>NOTE:</strong> You can use the <a href="http://linux.die.net/man/8/lsof" title="LSOF man page">lsof</a> command to help find file descriptors to deleted files. The exact command is <em>lsof +L1</em></p>
<hr>
<h4 id="reason-3">Reason #3</h4>
<p>The command <em>df</em>, for the most part, get most of its info from the file systems primary superblock, so it&#39;s almost as if the results were cached, and you&#39;re pulling it from the cache. As opposed to <em>du</em>, which gathers the information for the output at the time you execute the command. You can tell this by how long it takes for the commands to execute. I&#39;ll execute both a <em>du</em> and <em>df</em> on the same machine, with the same mounts, and wrap it in a <em>time</em> command...</p>
<pre><code class="lang-bash">$ time df -h /mnt/media/
Filesystem             Size  Used Avail Use% Mounted on
//192.168.1.140/Media  5.4T  2.4T  3.1T  44% /mnt/media

real    0m0.020s
user    0m0.010s
sys    0m0.000s

$ time du -sh /mnt/media/
1.6T    /mnt/media/

real    0m6.861s
user    0m0.270s
sys    0m0.800s
</code></pre>
<p>Not only is the usage almost a full TB off in the disk usage, but there was a difference of about 6 seconds! You can test this yourself by copying or moving data, then using the <a href="http://linux.die.net/man/1/watch">watch command</a> in two different terminals and watch the differences in the sizes. The results from <em>df</em> will update much faster, but the results wont change every time it executes.</p>
<hr>
<h4 id="summary">Summary</h4>
<p>Unless you&#39;re doing something like writing a script to interact with the NFS directly or something similar like that, I wouldn&#39;t really trust the output of <em>df</em>. I think of df as somewhat of a guesstimation or a ballpark figure of the partition sizes. The only real upside of it is the fact it executes nearly immediately. The <em>du</em> command is much more reliable and accurate. So if you can spare the time it takes to execute the command, I would suggest using <em>du</em> any day.</p>
<p><strong>Why is this information useful?</strong> Sometimes (many times in my personal experience), you will execute something like an install script or something that will use one or the other to check for free disk space before it continues. I can remember more than a few times I had to use <em>lsof</em> to check for open file descriptors because a script was erroring out.</p>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[Use NMAP to Checkout Servers On Your Network]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/use-nmap-to-checkout-servers-on-your-network/</link>
      <guid isPermaLink="true">http://localhost:3333/use-nmap-to-checkout-servers-on-your-network/</guid>
      <category><![CDATA[Linux]]></category>
      <category><![CDATA[nmap]]></category>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Thu, 20 Feb 2014 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>First... <em>What is NMAP?</em></p>
<blockquote>
<p>nmap (Network Mapper) is a security scanner originally written by Gordon Lyon (also known by his pseudonym Fyodor Vaskovich)[1] used to discover hosts and services on a computer network, thus creating a &quot;map&quot; of the network. To accomplish its goal, Nmap sends specially crafted packets to the target host and then analyzes the responses.</p>
</blockquote>
<p>Most places if worked, for some reason, don&#39;t use Nmap as much as you would think. Occasionally I&#39;ll have someone walk over and ask me if I know something about an IP (what OS, what application, etc), I&#39;ll ask what they&#39;ve done so far, and typically the reply is something along the line of &quot;I ran a ping and tried to get the hostname&quot;. Well weather or not the ping was successful, it wont tell you much, and unless theres a very specific hostname convention, and a PTR record for that IP, you still won&#39;t get much.</p>
<p>One of Nmaps most famous features is OS Guessing, what it does is scans the targets open ports, and looks through an <a href="https://svn.nmap.org/nmap/nmap-os-db">OS Detection Database</a>, then spits back the ports/protocols, and which OS&#39;s it think may be on the device.  </p>
<p>Now if you work in the network, you would know right away if it&#39;s just a Linux server, Oracle appliance, MySQL server, Firewall, F5... etc etc.  </p>
<p>The following examples are me running NMAP from my local Macbook Pro against my personal <a href="http://www.justinhyland.com">www.justinhyland.com</a> hosted server, so don&#39;t expect to find anything special.  </p>
<p>The three flags you will almost always want to use are</p>
<ul>
<li><strong>-v</strong> - Verbose</li>
<li><strong>-O</strong> - OS GUess</li>
<li><strong>-Pn</strong> - Skip the ping step (incase the target has ICMP disabled)</li>
</ul>
<p>So let&#39;s start with a simple Nmap query using the flags listed above...</p>
<pre><code class="lang-bash">$ sudo nmap -v -Pn -O 23.227.177.25

Starting Nmap 6.25 ( http://nmap.org ) at 2014-02-19 23:32 MST
Initiating Parallel DNS resolution of 1 host. at 23:32
Completed Parallel DNS resolution of 1 host. at 23:32, 0.07s elapsed
Initiating SYN Stealth Scan at 23:32
Scanning 23-227-177-25-vpsdime.com (23.227.177.25) [1000 ports]
Discovered open port 22/tcp on 23.227.177.25
Discovered open port 80/tcp on 23.227.177.25
Discovered open port 8090/tcp on 23.227.177.25
Discovered open port 3322/tcp on 23.227.177.25
Completed SYN Stealth Scan at 23:33, 4.46s elapsed (1000 total ports)
Initiating OS detection (try #1) against 23-227-177-25-vpsdime.com (23.227.177.25)
Nmap scan report for 23-227-177-25-vpsdime.com (23.227.177.25)
Host is up (0.050s latency).
Not shown: 992 closed ports
PORT     STATE    SERVICE
22/tcp   open     ssh
25/tcp   filtered smtp
80/tcp   open     http
135/tcp  filtered msrpc
139/tcp  filtered netbios-ssn
445/tcp  filtered microsoft-ds
3322/tcp open     active-net
8090/tcp open     unknown
Device type: general purpose
Running: Linux 3.X
OS CPE: cpe:/o:linux:linux_kernel:3
OS details: Linux 3.1 - 3.4
Uptime guess: 27.680 days (since Thu Jan 23 07:13:26 2014)
Network Distance: 14 hops
TCP Sequence Prediction: Difficulty=260 (Good luck!)
IP ID Sequence Generation: All zeros
</code></pre>
<p>So from the above Nmap result, you can pretty much immediately tell its a Linux server, with some abnormal ports open (8090, 3322), as well as a good guess of what application is running on those ports. It also gives you a pretty accurate OS guess. Heres the output of uname from the target server itself</p>
<pre><code class="lang-bash"># # uname -r -o
2.6.32-042stab084.12 GNU/Linux
</code></pre>
<p>So it got the kernel version pretty close <em>(Linux 2.6.19 - 2.6.35)</em>, and it got the OS correct <em>(Linux)</em></p>
<p>Lets add some more useful flags..</p>
<ul>
<li><strong>--allports</strong> - Don&#39;t exclude any ports from version detection</li>
<li><strong>--reason</strong> - Host and port state reasons</li>
<li><strong>-n</strong> - Never do DNS resolution (Also makes it quicker)</li>
<li><strong>-sV</strong> - Probe open ports to determine service/version info</li>
</ul>
<pre><code class="lang-bash">$ sudo nmap -v -Pn -n -sV --allports -O --reason 23.227.177.25

Starting Nmap 6.25 ( http://nmap.org ) at 2014-02-19 23:33 MST
NSE: Loaded 19 scripts for scanning.
Initiating SYN Stealth Scan at 23:33
Scanning 23.227.177.25 [1000 ports]
Discovered open port 22/tcp on 23.227.177.25
Discovered open port 80/tcp on 23.227.177.25
Discovered open port 3322/tcp on 23.227.177.25
Discovered open port 8090/tcp on 23.227.177.25
Completed SYN Stealth Scan at 23:33, 2.10s elapsed (1000 total ports)
Initiating Service scan at 23:33
Overriding exclude ports option! Some undesirable ports may be version scanned!
Scanning 4 services on 23.227.177.25
Completed Service scan at 23:33, 31.00s elapsed (4 services on 1 host)
Initiating OS detection (try #1) against 23.227.177.25
NSE: Script scanning 23.227.177.25.
Nmap scan report for 23.227.177.25
Host is up, received user-set (0.049s latency).
Not shown: 992 closed ports
Reason: 992 resets
PORT     STATE    SERVICE      REASON      VERSION
22/tcp   open     ssh          syn-ack     OpenSSH 5.3 (protocol 2.0)
25/tcp   filtered smtp         no-response
80/tcp   open     http         syn-ack     Apache httpd 2.2.15 ((CentOS))
135/tcp  filtered msrpc        no-response
139/tcp  filtered netbios-ssn  no-response
445/tcp  filtered microsoft-ds no-response
3322/tcp open     ssl/http     syn-ack     ZNC IRC bouncer http config 0.097 or later
8090/tcp open     http         syn-ack     Apache Tomcat/Coyote JSP engine 1.1
Device type: general purpose
Running: Linux 3.X
OS CPE: cpe:/o:linux:linux_kernel:3
OS details: Linux 3.1 - 3.4
Uptime guess: 27.681 days (since Thu Jan 23 07:13:25 2014)
Network Distance: 14 hops
TCP Sequence Prediction: Difficulty=257 (Good luck!)
IP ID Sequence Generation: All zeros
</code></pre>
<p>Granted, all of this information is nothing more than openly accessible information from the system, it sure tells you a lot more than you would be able to find out on your own.</p>
<p>You can also use Nmap to scan entire subnets, to find any hosts that you don&#39;t know are on your network. Or say you just plugged in a new device, cant login to it, your router hasn&#39;t cached the network layout yet, you could use nmap to simply scan the subnet. The example command is..</p>
<pre><code class="lang-bash">$ sudo nmap -sS -O 192.168.1.1/24
</code></pre>
<p>Another feature of Nmap that I like a lot is the <em>-oX filename.xml</em> flag, which outputs all of the information into XML format in the filename filename.xml. This is very useful if you plan to script against your results. Much more useful than just using <em>grep</em>, <em>awk</em> or <em>sed</em> against the output.  </p>
<p>Thats it! Now before you run to someone else about a system you have no idea about, use <strong>Nmap</strong>!</p>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[MacbookPro SD Write Problem - Solved... Kinda?]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/macbookpro-sd-write-problem-solved-kinda/</link>
      <guid isPermaLink="true">http://localhost:3333/macbookpro-sd-write-problem-solved-kinda/</guid>
      <category><![CDATA[raspberry-pi]]></category>
      <category><![CDATA[SD-card]]></category>
      <category><![CDATA[xbmc]]></category>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Thu, 20 Feb 2014 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>So lately, I&#39;ve been playin around with <a href="http://www.raspberrypi.org/">Raspberry PI</a> a lot. In doing so, (Assuming you are on a Macbook), you have to use the Disk Utility quite a bit.</p>
<p>You have to plug in your SD card, unmount it&#39;s partitions (without ejecting the whole SD Card), then use DD to write the image to the SD card.</p>
<p>The problem i&#39;ve been having, is that the SD cards are mounted in read only mode, even when the side switch isn&#39;t on <em>locked</em>. And according to The Google Gods, I&#39;m not the only one with this problem.</p>
<p>Whenever I mount the SD card, and open Disk Util, then look at the Info tab, it shows it&#39;s not writeable. I was able to go out and get an external SD card reader to fix the issue, but I wanted the one that came on my Macbook pro to work... because it should work, like a PRO.</p>
<p>After many nights of Googling and link hopping, I came across the dumbest solution that I never thought would work. I read it, and I thought &quot;This guys gotta be doing something else wrong&quot;.</p>
<p>Solution: You know the switch on the side that some OS&#39;s ignore, but Macbook doesn&#39;t? Don&#39;t switch it to on, or off, leave it in the middle... yeah... leave it in the middle. I did that, and guess what.... It was writable again!</p>
<p>If you find some other solution, please let me know!</p>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[Spice Up Your Default Index Page with h5a1]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/spice-up-your-default-index-page-with-h5a1/</link>
      <guid isPermaLink="true">http://localhost:3333/spice-up-your-default-index-page-with-h5a1/</guid>
      <category><![CDATA[apache]]></category>
      <category><![CDATA[default-index]]></category>
      <category><![CDATA[h5a1]]></category>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Tue, 18 Feb 2014 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>It&#39;s a very bad idea to list the contents of a web directory. That being said, there are always exceptions to the rule. For example, I have an Apache server at my house that you can only get to from within the network, and I like to just browse to it and download whatever I need, without having to know the exact path or filename.</p>
<h4 id="features">Features</h4>
<p>It provides a much richer interface, and some extra features that Apache/Nginx or any other HTTP service.</p>
<ul>
<li>Breadcrumb and tree view for faster browsing</li>
<li>Different view modes</li>
<li>Auto refresh of folder content</li>
<li>Custom header and/or footer for each directory</li>
<li>Packaged download of selected content (tar and zip supported)</li>
<li>Filter for displayed files and folders</li>
<li>Folder sizes</li>
<li>Localization with lots of languages already included</li>
<li>Image and text file preview (including Markdown rendering)</li>
<li>Thumbnails for images, movies and PDFs</li>
<li>QR codes on hovering files</li>
<li>Sorting by name, date or size</li>
</ul>
<p><strong><a href="http://larsjung.de/h5ai/sample/">Heres a sample default index n the h5a1 page</a></strong> And if you&#39;re too lazy to go to it, heres a screenshot [<img src="/assets/articles/h5a1_directory_index_sample-small.png" alt="h5a1 demo screenshot"></p>
<h4 id="installation">Installation</h4>
<p>The installation is pretty straight forward, you just download the zip file (or even better, use wget), then copy the _<em>h5ai</em> folder and its contents to the root of the virtual hosts directory. Then add _/<em>h5ai/server/php/index.php</em> to the <em>DirectoryIndex</em> attribute (assuming you&#39;re using Apache). Thats pretty much it. There are more details on the install on the <a href="http://larsjung.de/h5ai/"><strong>homepage</strong></a>.</p>
<h4 id="configuration">Configuration</h4>
<p>There really isn&#39;t much to configure, but all of the configuration directives are located at in the <strong>_h5ai/conf/options.json</strong> file. The only change I had to make was to enable the <em>foldersize</em> attribute. But there are all kinds of things, like enabling the ability to delete file/folders. The other configuration attributes are: _autorefresh, crumb, custom, delete, dropbox, download, filter, foldersize, google-analytics, l10n, link-hover-states, mode, piwik-analytics, preview-img, preview-txt, qrcode, rename, select, sort, statusbar, thumbnails, title, tree</p>
<h4 id="source">Source</h4>
<p><strong>Website:</strong> <a href="http://larsjung.de/h5ai">http://larsjung.de/h5ai</a> <strong>Demo:</strong> <a href="http://larsjung.de/h5ai/sample">http://larsjung.de/h5ai/sample</a> <strong>Download:</strong> <a href="https://github.com/lrsjng/h5ai">https://github.com/lrsjng/h5ai</a>_</p>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[Customize Your Bash Prompt]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/customize-your-bash-prompt/</link>
      <guid isPermaLink="true">http://localhost:3333/customize-your-bash-prompt/</guid>
      <category><![CDATA[bash]]></category>
      <category><![CDATA[bash-prompt]]></category>
      <category><![CDATA[ps1]]></category>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Thu, 13 Feb 2014 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<h3 id="bash-prompt">Bash Prompt</h3>
<p>The bash prompt, when customized, can be more useful than most people think. The default one that comes with most distros is pretty ugly and boring. The default is...</p>
<pre><code class="lang-bash">PS1=&#39;\s-\v\$ &#39;
</code></pre>
<p>Which looks like this when you&#39;re in the console:
<img src="/assets/articles/PS1/default_bash_prompt.png" alt="Default Bash Prompt">
About as lame as it gets huh?</p>
<p>Heres what the modified one looks like. This was created by myself and some of the other administrators/moderators of this website. Whats cool about this one, (as opposed to the other one), is...</p>
<ul>
<li>It shows the current time in the console prompt</li>
<li>It shows if the last command was a failure or a success (green/red checkbox)</li>
<li>Shows the current working directory that you&#39;re in</li>
<li>Shows the hostname of the server you&#39;re on</li>
<li>Displays your UID (To prevent mistakingly executing commands as others)</li>
</ul>
<pre><code class="lang-bash">$ export PS1=&quot;\[\e[0m\][\[&amp;#92;&amp;#48;33[1;30m\]\d \T\[\e[0m\]] \[\e[0m\]\[\e[0;92m\]\u@\h\[\e[0m\]:\[\e[0;94m\]\w\[&amp;#92;&amp;#48;33[1;30m\]($(if [[ $? == 0 ]]; then echo &quot;\[\e[0;92m\]\342\234\223\[\e[0m\]&quot;; else echo &quot;\[\e[0;91m\]\342\234\227\[\e[0m\]&quot;; fi)\[&amp;#92;&amp;#48;33[1;30m\])\[\e[0m\]$ &quot;
</code></pre>
<p>Heres a picture of it with some examples from my ver own bash prompt!
<img src="/assets/articles/PS1/bash_example.png" alt="Example"></p>
<h3 id="changing-your-prompt">Changing Your Prompt</h3>
<p>You can test out new ones by just setting the <em>$PS1</em> variable in your current shell session, then once you find the one you like, you make sure to set that prompt as the value of <em>PS1</em> within _~/.bash<em>profile</em></p>
<h4 id="more-prompt-strings">More Prompt Strings</h4>
<p><strong>Example #1</strong></p>
<pre><code class="lang-bash">export PS1=&quot;\s-\v\$ &quot;
</code></pre>
<p><img src="/assets/articles/PS1/bash_example_1.png" alt="Example"></p>
<p><strong>Example #2</strong></p>
<pre><code class="lang-bash">export PS1=&quot;&amp;#92;&amp;#48;33[0;36m\T \[&amp;#92;&amp;#48;33[1;30m\][\[&amp;#92;&amp;#48;33[1;34m\]\u@\h\[&amp;#92;&amp;#48;33[1;30m\]\[&amp;#92;&amp;#48;33[0;32m\]&amp;#92;&amp;#48;33[1;30m\]] \[&amp;#92;&amp;#48;33[1;37m\]\w\[&amp;#92;&amp;#48;33[0;37m\] \$ &quot;
</code></pre>
<p><img src="/assets/articles/PS1/bash_example_2.png" alt="Example"></p>
<p><strong>Example #3</strong></p>
<pre><code class="lang-bash">export PS1=&quot;${Color_Off}[${Grey}\d \T${Color_Off}] ${Color_Off}${IGreen}\u@\h${Color_Off}:${IBlue}\w${Grey}(\$(if [[ \$? == 0 ]]; then echo \&quot;${IGreen}\342\234\223${Color_Off}\&quot;; else echo \&quot;${IRed}\342\234\227${Color_Off}\&quot;; fi)${Grey})${Color_Off}\$ &quot;
</code></pre>
<p><img src="/assets/articles/PS1/bash_example_3.png" alt="Example"></p>
<p><strong>Example #4</strong></p>
<pre><code class="lang-bash">export PS1=&quot;\[&amp;#92;&amp;#48;33[0;37m\]\342\224\214\342\224\200\$([[ \$? != 0 ]] &amp;&amp; echo \&quot;[\[&amp;#92;&amp;#48;33[0;31m\]\342\234\227\[&amp;#92;&amp;#48;33[0;37m\]]\342\224\200\&quot;)[$(if [[ ${EUID} == 0 ]]; then echo &#39;\[&amp;#92;&amp;#48;33[0;31m\]\h&#39;; else echo &#39;\[&amp;#92;&amp;#48;33[0;33m\]\u\[&amp;#92;&amp;#48;33[0;37m\]@\[&amp;#92;&amp;#48;33[0;96m\]\h&#39;; fi)\[&amp;#92;&amp;#48;33[0;37m\]]\342\224\200[\[&amp;#92;&amp;#48;33[0;32m\]\w\[&amp;#92;&amp;#48;33[0;37m\]]\n\[&amp;#92;&amp;#48;33[0;37m\]\342\224\224\342\224\200\342\224\200\342\225\274 \[&amp;#92;&amp;#48;33[0m\]&quot;
</code></pre>
<p><img src="/assets/articles/PS1/bash_example_4.png" alt="Example"></p>
<p><strong>Example #5</strong></p>
<pre><code class="lang-bash">export PS1=&quot;$sq_color\342\224\214\342\224\200\$([[ \$? != 0 ]] &amp;&amp; echo \&quot;[\[&amp;#92;&amp;#48;33[01;37m\]\342\234\227$sq_color]\342\224\200\&quot;)[\[&amp;#92;&amp;#48;33[01;37m\]\t$sq_color]\342\224\200[\[&amp;#92;&amp;#48;33[01;37m\]\u@\h$sq_color]\n\342\224\224\342\224\200\342\224\200&gt; \[&amp;#92;&amp;#48;33[01;37m\]\W$sq_color $ \[&amp;#92;&amp;#48;33[01;37m\]&gt;&gt;\\[\&amp;#92;&amp;#48;33[0m\\] &quot;
</code></pre>
<p><img src="/assets/articles/PS1/bash_example_5.png" alt="Example"></p>
<p><strong>Example #6</strong></p>
<pre><code class="lang-bash">export PS1=&quot;\n\$(if [[ \$? == 0 ]]; then echo \&quot;\[&amp;#92;&amp;#48;33[0;34m\]\&quot;; else echo \&quot;\[&amp;#92;&amp;#48;33[0;31m\]\&quot;; fi)\342\226\210\342\226\210 [ \W ] [ \t ]\n\[&amp;#92;&amp;#48;33[0m\]\342\226\210\342\226\210 &quot;
</code></pre>
<p><img src="/assets/articles/PS1/bash_example_6.png" alt="Example">
This ones boring</p>
<p><strong>Example #7</strong></p>
<pre><code class="lang-bash"># I like this one a lot, but it takes too long to load
# (atleast for something as simple as a prompt.. even a second is too long)
export PS1=&quot;\n\[&amp;#92;&amp;#48;33[1;37m\]\342\224\214($(if [[ ${EUID} == 0 ]]; then echo &#39;\[&amp;#92;&amp;#48;33[01;31m\]\h&#39;; else echo &#39;\[&amp;#92;&amp;#48;33[01;34m\]\u@\h&#39;; fi)\[&amp;#92;&amp;#48;33[1;37m\])\342\224\200(\[&amp;#92;&amp;#48;33[1;34m\]\$?\[&amp;#92;&amp;#48;33[1;37m\])\342\224\200(\[&amp;#92;&amp;#48;33[1;34m\]\@ \d\[&amp;#92;&amp;#48;33[1;37m\])\[&amp;#92;&amp;#48;33[1;37m\]\n\342\224\224\342\224\200(\[&amp;#92;&amp;#48;33[1;32m\]\w\[&amp;#92;&amp;#48;33[1;37m\])\342\224\200(\[&amp;#92;&amp;#48;33[1;32m\]\$(ls -1 | wc -l | sed &#39;s: ::g&#39;) files, \$(ls -sh | head -n1 | sed &#39;s/total //&#39;)b\[&amp;#92;&amp;#48;33[1;37m\])\342\224\200&gt; \[&amp;#92;&amp;#48;33[0m\]&quot;
</code></pre>
<p><img src="/assets/articles/PS1/bash_example_7.png" alt="Example"></p>
<p><strong>Example #8</strong></p>
<pre><code class="lang-bash">export PS1=&quot;\[\e[01;32m\]\u@\h \[\e[01;34m\]\W \`if [ \$? = 0 ]; then echo -e &#39;\e[01;32m:)&#39;; else echo -e &#39;\e[01;31m:(&#39;; fi\` \[\e[01;34m\]$\[\e[00m\] &quot;
</code></pre>
<p><img src="/assets/articles/PS1/bash_example_8.png" alt="Example"></p>
<p><strong>Example #9</strong> Courtesy of mr Glenney</p>
<pre><code class="lang-bash">export PS1=&quot;\[&amp;#92;&amp;#48;33[01;32m\]\u|\[&amp;#92;&amp;#48;33[01;35m\]\$?\[&amp;#92;&amp;#48;33[01;34m\] \w \[&amp;#92;&amp;#48;33[01;31m\]\$\[&amp;#92;&amp;#48;33[00m\] &quot;
</code></pre>
<p><img src="/assets/articles/PS1/bash_example_9.png" alt="Example"></p>
<p><strong>Example #10</strong>
This is my personal favorite, it shows the timestamp, the # of the commands executed in the current session, username, hostname, current working directory, and a success/fail icon for the last command</p>
<pre><code class="lang-bash">export PS1=&quot;\[\e[0m\][\[&amp;#92;&amp;#48;33[1;30m\]\d \T\[\e[0m\]]\[\e[0m\]{\[&amp;#92;&amp;#48;33[1;30m\]\#\[\e[0m\]} \[\e[0m\]\[\e[0;92m\]\u@\h\[\e[0m\]:\[\e[0;94m\]\w\[&amp;#92;&amp;#48;33[1;30m\](\`if [ \$? = 0 ]; then echo \&quot;\[\e[0;92m\]\342\234\223\[\e[0m\]\&quot;; else echo \&quot;\[\e[0;91m\]\342\234\227\[\e[0m\]\&quot;; fi\`\[&amp;#92;&amp;#48;33[1;30m\])\[\e[0m\]$ &quot;
</code></pre>
<p> <img src="/assets/articles/PS1-example.png" alt="PS1 Example"></p>
<p><strong>Example #11</strong>
This is what I have setup on most of my servers. Instead of setting up the $PS1 variable in the ~/.bash_profile or ~/.bashrc files, I created a bash script located at /etc/profile.d/prompt_color.sh, which will use the above prompt (example #10), unless the user is root, then it will use the same prompt, but it will change the username and host color from green to red. Code for the bash script is below.</p>
<pre><code class="lang-bash">#!/bin/bash

if [ $(id -u) -eq 0 &gt;/dev/null 2&gt;&amp;1 ]; then
 export PS1=&quot;\[\e[0m\][\[&amp;#92;&amp;#48;33[1;30m\]\d \T\[\e[0m\]]\[\e[0m\]{\[&amp;#92;&amp;#48;33[1;30m\]\#\[\e[0m\]} \[\e[31;1m\]\u@\h\[\e[0m\]:\[\e[0;94m\]\w\[&amp;#92;&amp;#48;33[1;30m\](\`if [ \$? = 0 ]; then echo \&quot;\[\e[0;92m\]\342\234\223\[\e[0m\]\&quot;; else echo \&quot;\[\e[0;91m\]\342\234\227\[\e[0m\]\&quot;; fi\`\[&amp;#92;&amp;#48;33[1;30m\])\[\e[0m\]# &quot;
else
 export PS1=&quot;\[\e[0m\][\[&amp;#92;&amp;#48;33[1;30m\]\d \T\[\e[0m\]]\[\e[0m\]{\[&amp;#92;&amp;#48;33[1;30m\]\#\[\e[0m\]} \[\e[0m\]\[\e[0;92m\]\u@\h\[\e[0m\]:\[\e[0;94m\]\w\[&amp;#92;&amp;#48;33[1;30m\](\`if [ \$? = 0 ]; then echo \&quot;\[\e[0;92m\]\342\234\223\[\e[0m\]\&quot;; else echo \&quot;\[\e[0;91m\]\342\234\227\[\e[0m\]\&quot;; fi\`\[&amp;#92;&amp;#48;33[1;30m\])\[\e[0m\]$ &quot;
fi
</code></pre>
<p><img src="/assets/articles/profiled-ps1-example.png" alt="Profiled PS1 Example"></p>
<p><strong>Example #12</strong></p>
<p>This one may be a little overwhelming, but I think it&#39;s pretty useful. It displays:</p>
<ul>
<li>Total number of sessions on the server</li>
<li>Date and time</li>
<li>Execution time of last command</li>
<li>Number of commands executed in the current session</li>
<li>Username</li>
<li>Hostname</li>
<li>Current working directory</li>
<li>Status code of last command (Green checkbox on success, otherwise, red numeric value)</li>
<li>Also, the username@host changes from green to red if you sudo to root, just like in the previous examples</li>
</ul>
<pre><code class="lang-bash">#!/bin/bash
# Create /etc/profile.d/prompt.sh and add the content of this gist to it.
#
# Prompt example:
#   (2)[Fri May 06 10:00:30|  0:003]{4}root@ip-172-31-1-226:~(0)#
# Format is:
#   (sessions on server)[date time| Last command exec time]{sessions on server}username@hostname:working_directory(exit code of last cmd)#
#
# Example Output: http://d.pr/i/19B87
#
# Credit: Original version was taken from http://stackoverflow.com/a/8464508/5154806
#

bold=&#39;\[\e[1m\]&#39;
plain=&#39;\[\e[0m\]&#39;

set_begin() {
    [[ -z &quot;$begin&quot; ]] &amp;&amp; begin=&quot;$(date +&quot;%s %N&quot;)&quot;
}

calc_elapsed() {
    # Thresholds for command execution time (seconds)
    warn_threshold=&#39;300&#39;      # 5 minutes
    danger_threshold=&#39;3600&#39; # 1 hour

    read begin_s begin_ns &lt;&lt;&lt; &quot;$begin&quot;
    begin_ns=&quot;${begin_ns##+(0)}&quot;
    # PENDING - date takes about 11ms, maybe could do better by digging in
    # /proc/$$.  
    read end_s end_ns &lt;&lt;&lt; $(date +&quot;%s %N&quot;)
    end_ns=&quot;${end_ns##+(0)}&quot;
    local s=$((end_s - begin_s))
    local ms

    [$end_ns&quot; -ge &quot;$begin_ns&quot; ]] &amp;&amp; ms=$(((end_ns - begin_ns) / 1000000)) || ( s=$((s - 1)); ms=$(((1000000000 + end_ns - begin_ns) / 1000000)) )

    elapsed=&quot;$(printf &quot; %2u:%03u&quot; $s $ms)&quot;

    [$s&quot; -ge 300 ]] &amp;&amp; elapsed=&quot;$elapsed [$(human_time $s)]&quot;

    # If the last execution elapsed time is greater than one of the above thresholds, then
    # set the color to red or yellow
    if [[ $s -gt $danger_threshold ]]; then
        elapsed=&quot;\[\e[31;1m\]$elapsed\[\e[0m\]&quot;
    elif [[ $s -gt $warn_threshold ]]; then
        elapsed=&quot;\[\e[33m\]$elapsed\[\e[0m\]&quot;
    fi
}

human_time() {
    local s=$1
    local days=$((s / (60*60*24)))
    s=$((s - days*60*60*24))
    local hours=$((s / (60*60)))
    s=$((s - hours*60*60))
    local min=$((s / 60))

    [$days&quot; != 0 ]] &amp;&amp; local day_string=&quot;${days}d &quot;

    printf &quot;$day_string%02d:%02d\n&quot; $hours $min
}  

timer_prompt() {
    status=$?
    local size=16
    sess_count=$(who | wc -l)
    calc_elapsed

    [${#PWD}&quot; -gt $size ]] &amp;&amp; pwd_string=&quot;${PWD: -$size}&quot; || pwd_string=&quot;$(printf &quot;%${size}s&quot; $PWD)&quot;

    if [[ $(id -u) -eq 0 ]]; then
        color=&#39;\[\e[31;1m\]&#39;
        suffix=&#39;#&#39;
    else
        color=&#39;\[\e[0m\]\[\e[0;92m\]&#39;
        suffix=&#39;$&#39;
    fi

    PS1=&quot;(\[&amp;#92;&amp;#48;33[1;30m\]$sess_count\[\e[0m\])\[\e[0m\][\[&amp;#92;&amp;#48;33[1;30m\]\d \T\[\e[0m\]|\[\e[0m\]$bold$elapsed]\[\e[0m\]{\[&amp;#92;&amp;#48;33[1;30m\]\#\[\e[0m\]}$color\u@\h\[\e[0m\]:\[\e[0;94m\]\w\[&amp;#92;&amp;#48;33[1;30m\](\`if [ \$? = 0 ]; then echo \&quot;\[\e[0;92m\]\342\234\223\[\e[0m\]\&quot;; else echo \&quot;\[\e[0;91m\]$status\[\e[0m\]\&quot;; fi\`\[&amp;#92;&amp;#48;33[1;30m\])\[\e[0m\]$suffix &quot;

    begin=
}

set_begin
trap set_begin DEBUG
PROMPT_COMMAND=timer_prompt
</code></pre>
<p><img src="/assets/articles/custom-ps1-1.png" alt="Custom PS1"></p>
<hr>
<p><a href="http://www.kirsle.net/wizards/ps1.html">I found a useful PS1 generator here, incase anyones interested</a></p>
<p><strong>Instead of changing the <em>$PS1</em> variable inside your </strong>~/.bash<em>prompt<strong> file every time you find a new one you like</strong>, I decided to throw multiple </em>$PS1_ values inside of an array, that way, whenever you want to change it, you can change the array key.. Check er out!..</p>
<pre><code class="lang-bash"># Multiple bash prompts
bash_prompt[0]=&quot;${Color_Off}[${Grey}\d \T${Color_Off}] ${Color_Off}${IGreen}\u@\h${Color_Off}:${IBlue}\w${Grey}(\$(if [[ \$? == 0 ]]; then echo \&quot;${IGreen}\342\234\223${Color_Off}\&quot;; else echo \&quot;${IRed}\342\234\227${Color_Off}\&quot;; fi)${Grey})${Color_Off}\$ &quot;
bash_prompt[1]=&quot;\s-\v\$ &quot;
bash_prompt[2]=&quot;\[\e[0m\][\[&amp;#92;&amp;#48;33[1;30m\]\d \T\[\e[0m\]] \[\e[0m\]\[\e[0;92m\]\u@\h\[\e[0m\]:\[\e[0;94m\]\w\[&amp;#92;&amp;#48;33[1;30m\]($(if [[ $? == 0 ]]; then echo &quot;\[\e[0;92m\]\342\234\223\[\e[0m\]&quot;; else echo &quot;\[\e[0;91m\]\342\234\227\[\e[0m\]&quot;; fi)\[&amp;#92;&amp;#48;33[1;30m\])\[\e[0m\]$&quot;
bash_prompt[3]=&quot;\s-\v\$ &quot;
bash_prompt[4]=&quot;&amp;#92;&amp;#48;33[0;36m\T \[&amp;#92;&amp;#48;33[1;30m\][\[&amp;#92;&amp;#48;33[1;34m\]\u@\h\[&amp;#92;&amp;#48;33[1;30m\]\[&amp;#92;&amp;#48;33[0;32m\]&amp;#92;&amp;#48;33[1;30m\]] \[&amp;#92;&amp;#48;33[1;37m\]\w\[&amp;#92;&amp;#48;33[0;37m\] \$ &quot;
bash_prompt[5]=&quot;\[&amp;#92;&amp;#48;33[0;37m\]\342\224\214\342\224\200\$([[ \$? != 0 ]] &amp;&amp; echo \&quot;[\[&amp;#92;&amp;#48;33[0;31m\]\342\234\227\[&amp;#92;&amp;#48;33[0;37m\]]\342\224\200\&quot;)[$(if [[ ${EUID} == 0 ]]; then echo &#39;\[&amp;#92;&amp;#48;33[0;31m\]\h&#39;; else echo &#39;\[&amp;#92;&amp;#48;33[0;33m\]\u\[&amp;#92;&amp;#48;33[0;37m\]@\[&amp;#92;&amp;#48;33[0;96m\]\h&#39;; fi)\[&amp;#92;&amp;#48;33[0;37m\]]\342\224\200[\[&amp;#92;&amp;#48;33[0;32m\]\w\[&amp;#92;&amp;#48;33[0;37m\]]\n\[&amp;#92;&amp;#48;33[0;37m\]\342\224\224\342\224\200\342\224\200\342\225\274 \[&amp;#92;&amp;#48;33[0m\]&quot;
bash_prompt[6]=&quot;$sq_color\342\224\214\342\224\200\$([[ \$? != 0 ]] &amp;&amp; echo \&quot;[\[&amp;#92;&amp;#48;33[01;37m\]\342\234\227$sq_color]\342\224\200\&quot;)[\[&amp;#92;&amp;#48;33[01;37m\]\t$sq_color]\342\224\200[\[&amp;#92;&amp;#48;33[01;37m\]\u@\h$sq_color]\n\342\224\224\342\224\200\342\224\200&gt; \[&amp;#92;&amp;#48;33[01;37m\]\W$sq_color $ \[&amp;#92;&amp;#48;33[01;37m\]&gt;&gt;\\[\&amp;#92;&amp;#48;33[0m\\] &quot;
bash_prompt[7]=&quot;\n\$(if [[ \$? == 0 ]]; then echo \&quot;\[&amp;#92;&amp;#48;33[0;34m\]\&quot;; else echo \&quot;\[&amp;#92;&amp;#48;33[0;31m\]\&quot;; fi)\342\226\210\342\226\210 [ \W ] [ \t ]\n\[&amp;#92;&amp;#48;33[0m\]\342\226\210\342\226\210 &quot;
bash_prompt[8]=&quot;\n\[&amp;#92;&amp;#48;33[1;37m\]\342\224\214($(if [[ ${EUID} == 0 ]]; then echo &#39;\[&amp;#92;&amp;#48;33[01;31m\]\h&#39;; else echo &#39;\[&amp;#92;&amp;#48;33[01;34m\]\u@\h&#39;; fi)\[&amp;#92;&amp;#48;33[1;37m\])\342\224\200(\[&amp;#92;&amp;#48;33[1;34m\]\$?\[&amp;#92;&amp;#48;33[1;37m\])\342\224\200(\[&amp;#92;&amp;#48;33[1;34m\]\@ \d\[&amp;#92;&amp;#48;33[1;37m\])\[&amp;#92;&amp;#48;33[1;37m\]\n\342\224\224\342\224\200(\[&amp;#92;&amp;#48;33[1;32m\]\w\[&amp;#92;&amp;#48;33[1;37m\])\342\224\200(\[&amp;#92;&amp;#48;33[1;32m\]\$(ls -1 | wc -l | sed &#39;s: ::g&#39;) files, \$(ls -sh | head -n1 | sed &#39;s/total //&#39;)b\[&amp;#92;&amp;#48;33[1;37m\])\342\224\200&gt; \[&amp;#92;&amp;#48;33[0m\]&quot;
bash_prompt[9]=&quot;\[\e[01;32m\]\u@\h \[\e[01;34m\]\W \`if [ \$? = 0 ]; then echo -e &#39;\e[01;32m:)&#39;; else echo -e &#39;\e[01;31m:(&#39;; fi\` \[\e[01;34m\]$\[\e[00m\] &quot;
bash_prompt[10]=&quot;\[&amp;#92;&amp;#48;33[01;32m\]\u|\[&amp;#92;&amp;#48;33[01;35m\]\$?\[&amp;#92;&amp;#48;33[01;34m\] \w \[&amp;#92;&amp;#48;33[01;31m\]\$\[&amp;#92;&amp;#48;33[00m\] &quot;

# Change the key valye to match the desired PS1 value above
export PS1=${bash_prompt[2]}
</code></pre>
<p>Makes it <strong>MUCH</strong> easier!</p>
<h3 id="what-are-the-other-prompts">What Are The Other Prompts</h3>
<p>I guess since I&#39;m explaining <em>$PS1</em> n depth, I could go a bit further.
<strong><em>PS#..</em></strong></p>
<ol>
<li><strong>PS1</strong> - Default interaction prompt. The default interactive prompt on your Linux can be modified as shown below to something useful and informative.</li>
<li><strong>PS2</strong> - Continuation interactive prompt. A very long unix command can be broken down to multiple line by giving \ at the end of the line. The default interactive prompt for a multi-line command is “&gt; “.</li>
<li><strong>PS3</strong> - Prompt used by &quot;select&quot; inside shell script. You can define a custom prompt for the select loop inside a shell script, using the PS3 environment variable.</li>
<li><strong>PS4</strong> - Used by &quot;set -x&quot; to prefix tracing output. The PS4 shell variable defines the prompt that gets displayed, when you execute a shell script in debug mode.</li>
<li><strong>PS5</strong> - PROMPT_COMMAND. Bash shell executes the content of the PROMPT_COMMAND just before displaying the PS1 variable.</li>
</ol>
<p>Sources: <a href="http://www.kirsle.net/wizards/ps1.html">kristie.net</a></p>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[Bash Tips And Tricks (Part 1)]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/bash-tips-and-tricks-part-1/</link>
      <guid isPermaLink="true">http://localhost:3333/bash-tips-and-tricks-part-1/</guid>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Wed, 12 Feb 2014 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>I have always thought of bash as a &quot;Quick &#39;n Dirty&quot; way of taking care of things, it isn&#39;t by any means a powerful language. It&#39;s not meant for writing programs, not meant for creating games, not meant for much other than linux system administration. My &quot;Rule Of Thumb&quot;, is if someone gives me a task to complete, I do it manually, if they come twice or three times, I automate it. If I think it will take more than 100 lines of code, I use something other than bash.... Perl or Python perhaps. I don&#39;t usually use PHP for CLI apps, but sometimes there are exceptions to the rule. Below are a few tricks I have learned while creating some bash scripts.Feel free to add your own in the comments!</p>
<h4 id="1-getting-the-correct-error-code-of-a-command-thats-before-a-pipe">1) Getting The Correct Error Code Of A Command Thats Before A Pipe</h4>
<p>A lot of bash scripting consists of executing one thing, and having the output piped ( <strong>|</strong> ) into another command, for example...</p>
<pre><code class="lang-bash">$ /usr/bin/w | grep jhyland | wc -l
1
</code></pre>
<p>This executes the <em>/usr/bin/w</em> binary, then pipes it to the <em>grep</em> command, which will only return lines that has jhyland in it, then counts the number of lines. So now we know that jhyland is logged into this server only once. Another commonly used tactic is getting the exit code of the last command, which is the $? variable. You can execute a command, then with a conditional statement, see what the exit code was, and process something else accordingly, example below.</p>
<pre><code class="lang-bash">#!/bin/bash

mkdir /tmp/jhyland

if [[ $? -ne 0 ]]
then
 echo &quot;Error while creating /tmp/jhyland&quot;
 exit 1
else
 echo &quot;/tmp/jhyland has been created&quot;
fi
</code></pre>
<p>The $? will either return 0 (success) or 1 (fail). But what about that command I showed you above? The /usr/bin/w | grep jhyland | wc -l, if that gets executed, then $? will show the exit code for the wc -l part, when what you really want, is the exit code for the /usr/bin/w command. The solution is the system variable $PIPESTATUS, (Well, I guess it&#39;s an array). This holds all of the exit codes for every command that was previously ran (Keep in mind, this gets reset every time you run a new command. Heres an example:</p>
<pre><code class="lang-bash">#!/bin/bash

/usr/bin/w | grep jhyland | wc -l

if [[ ${PIPESTATUS[0]} -ne 0 ]]
then
echo &quot;Error while running /usr/bin/w&quot;
exit 1
else
echo &quot;/usr/bin/w ran just fine&quot;
fi
</code></pre>
<p>Here&#39;s a slightly better example of how to leverage it:</p>
<pre><code class="lang-bash">#!/bin/bash

user=&quot;jhyland&quot;
ip=&quot;192.168.1&quot;

who | grep $user | grep $ip &amp;&gt;/dev/null

if [[ ${PIPESTATUS[1]} -ne 0 ]]; then
 echo &quot;No $user on this server&quot;
elif [[ ${pipestatus[2]} -ne 0 ]]; then
 echo &quot;The user $user is on this server, just not from an IP matching $ip&quot;
else
 echo &quot;The user $user is on this server, from an IP matching $ip&quot;
fi
</code></pre>
<p><a href="/articles/viewing-bash-exit-status-codes-with-pipes.html">Heres a more in-depth tutorial.</a></p>
<hr>
<h4 id="2-using-_-_-and-_-_-instead-of-conditional-statements">2) Using <em>&amp;&amp;</em> and <em>||</em> Instead of Conditional Statements</h4>
<p>Most Bash scripting is comprised of executing commands, then executing more commands based off of the output or result of the previous command.</p>
<p>If you learn how to use the <em>&amp;&amp;</em> and <em>||</em> operators, you&#39;ll find you can convert a lot of scripts to one-liners, or reduce the amount of lines in some of your existing scripts.</p>
<p>The <em>||</em> operator is basically &quot;OR&quot;, it&#39;s somewhat like using the <em>-ne</em> operator in a conditional statement.</p>
<p>Small example... This following script just creates a directory, then backsup (tar&#39;s) a directory and throws the tar into the newly created directory</p>
<pre><code class="lang-bash">mkdir -p  /some/test/directory &amp;&gt;/dev/null
if [[ $? -ne 0 ]]; then
    exit 1
fi

tar cvfz /some/test/directory/backup_file.tar.gz /tmp &amp;&gt;/dev/null
if [[ $? -ne 0 ]]; then
    exit 1
fi
</code></pre>
<p>Now this little script can actually be reduced down to two lines of code, using the <em>||</em> operator. Heres the example..</p>
<pre><code class="lang-bash">mkdir -p /some/test/directory &amp;&gt;/dev/null || exit 1
tar cvfz /some/test/directory/backup_file.tar.gz /tmp &amp;&gt;/dev/null ||  exit 1
</code></pre>
<p>Those two snippets do the exact same thing!</p>
<p>Let&#39;s get a little more complicated. Lets add confirmations into the script.</p>
<pre><code class="lang-bash">mkdir -p  /some/test/directory &amp;&gt;/dev/null
if [[ $? -ne 0 ]]; then
    echo &quot;Failed to mkdir&quot;
    exit 1
else
    echo &quot;Successfully mkdir&#39;d&quot;
fi

tar cvfz /some/test/directory/backup_file.tar.gz /tmp &amp;&gt;/dev/null
if [[ $? -ne 0 ]]; then
    echo &quot;Failed to tar&quot;
    exit 1
else
    echo &quot;Successfully tar&#39;d&quot;
fi
</code></pre>
<p>That does the same thing as the previous snippets, only it&#39;s a little more verbose. You wouldn&#39;t immediately think that you can convert that into a two liner, but you actually can...</p>
<pre><code class="lang-bash">(mkdir -p /some/test/directory &amp;&gt;/dev/null &amp;&amp; echo &quot;Successfully mkdir&#39;d&quot;) || (echo &quot;Failed to mkdir&quot; &amp;&amp; exit 1)
(tar cvfz /some/test/directory/backup_file.tar.gz /tmp &amp;&gt;/dev/null &amp;&amp; echo &quot;Successfully tar&#39;d&quot;) || (echo &quot;Failed to tar&quot; &amp;&amp; exit 1)
</code></pre>
<p>You can group commands using parentheses, then use the <em>&amp;&amp;</em> and <em>||</em> operators.</p>
<p>One more small example... this is a one liner that ensures the username AND password os set previously in the script, if not, displays whatsup and exits with the proper exit code</p>
<pre><code class="lang-bash">[[ -z $username || -z $password ]] &amp;&amp; (echo &quot;Username or password null&quot; &amp;&amp; exit 1)
</code></pre>
<p>It&#39;s a little more difficult to read, but very useful!</p>
<hr>
<h4 id="3-brace-expressions">3) Brace Expressions</h4>
<p>Utilizing the Brace Expression around a list of worts, allows you to print a list of words with the same prefix and suffix to the words. Here is an example via the CLI of me using the Brace Expression with a suffix:</p>
<pre><code class="lang-bash">echo {inspi,admi,ado,abju,ac,adhe,inji}red inspired admired adored abjured acred adhered injired
</code></pre>
<p>Example with a prefix:</p>
<pre><code class="lang-bash">echo red{act,an,argue,der,dens,bay} redact redan redargue redder reddens redbay
</code></pre>
<p>Now this is a pretty useful trick, but for a while, I just wasn&#39;t sure what to use it for, but I guess you can do something like this...</p>
<pre><code class="lang-bash">#!/bin/bash

# Get all of the hostnames out of the apache settings output, replace the returns with ,
hosts=$(/usr/sbin/httpd -S 2&gt;&amp;1 | grep namevhost | awk &#39;{print $4}&#39; | tr &#39;\n&#39; &#39;,&#39;)

# Compress all apache logs
tar cvfz apache_logs.tar.gz $(eval echo /var/log/httpd/{$hosts}.log)
</code></pre>
<p>That would tar and compress any  of the log files for any active sites into apache_logs.tar.gz <strong> NOTE:</strong> If you are using a variable within the curly braces, like above (The $hosts variable within {}), then you need to use the eval statement.</p>
<hr>
<h4 id="4-backticks-vs-">4) Backticks vs $()</h4>
<p>I was originally taught to use backticks for command substitution, heres a small example:</p>
<pre><code class="lang-bash">[jhyland@svr2 ~]$ echo &quot;My name is `whoami` and I am in `pwd`&quot;;
 My name is jhyland and I am in /home/jhyland
</code></pre>
<p>But if you do enough Bash scripting, you will see an issue... How do you nest commands? Meaning, if you need to execute backticks within backticks, it gets pretty messy. You have to escape every nested backtick, and that would be a pretty ugly script! But if you choose to use <strong>$()</strong> instead of backticks, then it gets much simpler.</p>
<pre><code class="lang-bash">echo &quot;The file you are looking for is $(find /home/$(whoami)/Downloads/ -name \&#39;bla-install*.tar.gz)\&#39;&quot;
</code></pre>
<p>See how you can execute the find command in a subshell, as well as the whoami statement, which is inside of a subshell, inside of another subshell? Thats why you should use $() and not ``</p>
<hr>
<h4 id="5-using-arrays-instead-of-multiple-variables">5) Using Arrays Instead Of Multiple Variables</h4>
<p>You don&#39;t have to be a very experienced developer to know that using an array is better than using a ton of variables. Arrays are better for multiple reasons... You can manipulate them in nearly any way. So instead of doing this...</p>
<pre><code class="lang-bash">#!/bin/bash
color1=&#39;Blue&#39;
color2=&#39;Red&#39;
color3=&#39;Yellow&#39;
color4=&#39;Black&#39;
echo &quot;Colors: $color1, $color2, $color3, $color4.&quot;
# Output: Colors: Blue, Red, Yellow, Black.
</code></pre>
<p>Why not do it the smart way? You can use an array to store the colors. Try this:</p>
<pre><code class="lang-bash">#!/bin/bash
colors=(&#39;Blue&#39; &#39;Red&#39;,&#39;Yellow&#39;,&#39;Black&#39;)
echo &quot;Colors: ${colors[0]}, ${colors[1]}, ${colors[2]}, ${colors[3]}&quot;
# Output: Colors: Blue, Red, Yellow, Black.
</code></pre>
<p>Same output, but less lines of code, and you can add values to it, delete values, reorder it, etc etc. NOTE: The only thing that sucks about arrays in Bash... is the keys have to be numeric. Meaning you cant have ${name[a]} or anything of that sort. The keys are always numeric, never alphabetic.</p>
<hr>
<h4 id="6-the-test-utility">6) The &#39;test&#39; Utility</h4>
<p>Not many people are aware of the test command. Basically, it lets you test a condition, without being in a conditional statement. You just use it to test a condition and it will return an exit code (1 or 0). Here is an example of how you could use it.</p>
<pre><code class="lang-bash">#!/bin/bash

dir=$(test -d $1)

if [[ $dir -eq &quot;0&quot; ]]
then
 the dir exists
fi
</code></pre>
<p>Now I realize you could just put the condition inside of the if statement itself, but the point is that now you can use $dir all over your script, or use it to compare to other test results.</p>
<hr>
<h4 id="7-default-bash-variable-values">7) Default Bash Variable Values</h4>
<p>There are many instances where you want to set a default value for a variable. A lot of people will do something like this:</p>
<pre><code class="lang-bash">#!/bin/bash

first=$1
last=$2

if [[ ! $first ]]
then
 first=&quot;John&quot;
fi

if [[ ! $last ]]
then
 last=&quot;Doe&quot;
fi

echo &quot;Your name is $first $last&quot;
</code></pre>
<p>But theres a much easier way:</p>
<pre><code class="lang-bash">#!/bin/bash

# Preferred way of setting a default value to a variable
# (If $1 isn&#39;t null, set value of $first to $1, otherwise,
# set value of $first to &quot;John&quot;)
first=${1:-John}

# Old school method, whats used in some older bash scripts
# (if $2 is non-zero, then set $last to value of $2, if
# $2 is empty, then set $last to &quot;doe&quot;)
[ -n &quot;$2&quot; ] &amp;&amp; last=$2 || last=&quot;doe&quot;

echo &quot;Your name is $first $last&quot;
</code></pre>
<p>Using the <strong>${variable:-default value}</strong> method saves you time and space, and is a much easier to utilize variables with default values.</p>
<hr>
<h4 id="8-align-your-content-make-it-pretty">8) Align Your Content, Make It Pretty</h4>
<p>Typically, when you need to align your output to make it pretty, you would use tabs, or printf, but I found a better way to do so, and i&#39;ve used it ever since. The command is called <strong>column</strong>, specifically &quot;<strong>column -t</strong>&quot;. Heres an example of a simple command of me looking at the passwd file. Lets take a look at the output:</p>
<pre><code class="lang-bash"># egrep &quot;^(geoff|justin|kyle)&quot; /etc/passwd | awk -F: &#39;{print &quot;User&quot;, $1, &quot;Home:&quot;, $6}&#39; User geoff Home: /home/geoff User justin Home: /home/justin User kyle Home: /home/kyle
</code></pre>
<p>Pretty basic, just a little ugly huh? Lets try adding <strong>column -t</strong> to the end of it...</p>
<pre><code class="lang-bash"># egrep &quot;^(geoff|justin|kyle)&quot; /etc/passwd | awk -F: &#39;{print &quot;User&quot;, $1, &quot;Home:&quot;, $6}&#39; | column -t User geoff Home: /home/geoff User justin Home: /home/justin User kyle Home: /home/kyle
</code></pre>
<p>You can see that theres a difference in the way the columns are laid out. The <strong>column -t</strong> aligns the columns perfectly. This is useful for the <strong>/bin/mount</strong> and <strong>/bin/df</strong> commands as well, we all know how ugly those are, but if you add <em>column -t</em>, it turns it into something somewhat representable:</p>
<pre><code class="lang-bash">$ mount | column -t
/dev/simfs  on  /                         type  simfs
(rw,relatime,usrquota,grpquota)
proc        on  /proc                     type  proc         (rw,relatime)
sysfs       on  /sys                      type  sysfs        (rw,relatime)
none        on  /dev                      type  tmpfs        (rw,relatime)
none        on  /dev/pts                  type  devpts       (rw,relatime)
none        on  /proc/sys/fs/binfmt_misc  type  binfmt_misc  (rw,relatime)
$ df | column -t
Filesystem  1K-blocks  Used     Available  Use%  Mounted  on
/dev/simfs  52428800   6981724  45447076   14%   /
none        1048576    4        1048572    1%    /dev
</code></pre>
<hr>
<h4 id="9-use-double-brackets-for-conditionals-more-superior-">9) Use double brackets for conditionals, more &quot;superior&quot;</h4>
<p>You can code an if statement in bash with either single brackets..</p>
<pre><code class="lang-bash">if [ $this == \&#39;that\&#39; ];
</code></pre>
<p>or you can use the double brackets</p>
<pre><code class="lang-bash">if [[ $this == &#39;that&#39; ]];
</code></pre>
<p>Both of these will operate the same way if $this is set to &#39;that&#39;, but if its not set, or its null, then the first one will fail, first one will throw a syntax error in the if statement.</p>
<p>The double brackets also allow regex matching as well, which can only be accomplished in if statements with single brackets if you execute a subshell and using some form of grep.</p>
<p>Double brackets are also backwards compatible. So theres nothing extra that needs to be done other than a simple find/replace to replace the single brackets to double.</p>
<h4 id="10-hide-your-grep-result-more-efficiently-with-grep">10) Hide Your Grep Result More Efficiently With Grep</h4>
<p>You know how when you grep for something, the actual grep line shows up? Example...</p>
<pre><code class="lang-bash"># ps aux |grep something
root      4155  0.0  0.0  13632   952 pts/2    S+   23:16   0:00 grep --colour=auto something
jdoe  29731  1.0  0.0  20256  1176 pts/0    S+   20:30   1:47 tar cvfz something.tar.gz directory
</code></pre>
<p>Typically people just add <em>| grep -v grep</em>, but what if that hides something that you want to see?</p>
<p>Well heres a little tip, if you surround the first character in brackets, then it will exclude the grep command itself.. Example:</p>
<pre><code class="lang-bash"># ps aux |grep [s]omething
jdoe  29731  1.0  0.0  20256  1176 pts/0    S+   20:30   1:47 tar cvfz something.tar.gz directory
</code></pre>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[Macbook - Find whats eating up your HDD]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/macbook-find-whats-eating-up-your-hdd/</link>
      <guid isPermaLink="true">http://localhost:3333/macbook-find-whats-eating-up-your-hdd/</guid>
      <category><![CDATA[apple]]></category>
      <category><![CDATA[diagnostics]]></category>
      <category><![CDATA[mac]]></category>
      <category><![CDATA[macbook]]></category>
      <category><![CDATA[troubleshooting]]></category>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Mon, 10 Feb 2014 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>I know... this isn&#39;t Linux, and a lot of Linux folk hate Mac. Putting Apple Inc aside, Mac is actually a great operating system to use to manage Linux servers in a corporate environment. I HATED Macbooks for the longest time, until work gave me the option of using Windows or getting a nice Mac. My primary reason for getting Mac was because if I chose Windows, I would have gotten a crappy Dell, and I was going to install some type of virtualization and use Mint anyhow.</p>
<p>I do a lot of development on my Mac, but my root partition is 698Gi large, which is fairly large, so when one day I got an error that my disk space had only 10% left, I was a but surprised.</p>
<p>I had to hop around all day and use _<a href="http://linux.about.com/library/cmd/blcmdl1_du.htm">du</a>_ and delete files and folders that I had downloaded, but I still had a large amount taken, and it didn&#39;t make sense to me.</p>
<p>After some looking around, and trying some applications that would help, I found the one that was actually super cool, it&#39;s called <em><a href="http://www.derlien.com/">Disk Inventory X</a></em>. It analyzes all (or selected volumes), and shows you a very nice graphical display. Heres an example (Not from my Macbook):
<img src="/assets/articles/Disk_Inventory_X.jpg" alt="Disk Inventory X"></p>
<p>Pretty useful huh? It shows you a color coded graphical representation.</p>
<p>The application is free to download, so give it a shot!</p>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[RHEL / CentOS: Safely remove old unused Kernels]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/rhel-centos-safely-remove-old-unused-kernels/</link>
      <guid isPermaLink="true">http://localhost:3333/rhel-centos-safely-remove-old-unused-kernels/</guid>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Fri, 31 Jan 2014 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>I have ran into it before when you do a yum update, it prompts you telling you that there isn&#39;t enough space free on the /boot partition. You have two options at this point, either increase the space the /boot partition has.. or clean up your old unused kernels that are taking up the space. Personally, I don&#39;t see a reason in keeping more than 2 kernels at a time, you&#39;ll more than likely never go back to a previous version once you&#39;ve upgraded.</p>
<p>So, to remove the old kernels first you will want to check to see what version you&#39;re currently running, that way we know not to remove it:</p>
<pre><code class="lang-bash">$ uname -r
2.6.32-220.13.1.el6.x86_64
</code></pre>
<p>So in this case, we know our kernel version is 2.6.32-220.13.1.el6.x86_64. Next, we want to list all kernels that are installed, we can do this by running the following command:</p>
<pre><code class="lang-bash">$ rpm -q kernel
kernel-2.6.32-220.el6.x86_64
kernel-2.6.32-220.7.1.el6.x86_64
kernel-2.6.32-220.13.1.el6.x86_64
</code></pre>
<p>So as you can see, there are 3 total installed. We know our current version is stable and works for us, so let&#39;s remove the other two. We can do so by simply yum removing them:</p>
<pre><code class="lang-bash">$ yum remove kernel-2.6.32-220.el6.x86_64 kernel-2.6.32-220.7.1.el6.x86_64
</code></pre>
<p>And that&#39;s it! You can rerun the rpm -q kernel command and see that there is only one left and then you can check your free space on the /boot partition.</p>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[RHEL5 / CentOS5: File Conflicts when Upgrading VMWare Tools with Yum]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/rhel5-centos5-file-conflicts-when-upgrading-vmware-tools-with-yum/</link>
      <guid isPermaLink="true">http://localhost:3333/rhel5-centos5-file-conflicts-when-upgrading-vmware-tools-with-yum/</guid>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Thu, 30 Jan 2014 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>One of the issues that I have ran into is when you&#39;re running VMWare virtual machines, and a new kernel comes out, it seems to break the vmware-tools upgrade processes. It was extremely frustrating, because you would go to update and see the following issues:</p>
<pre><code class="lang-bash">Transaction Check Error:

file /lib/modules/2.6.18-8.el5/extra/vmware-tools-vmxnet3/vmxnet3.ko from install of kmod-vmware-tools-vmxnet3-1.0.47.0-2.6.18.8.el5.3.x86_64 conflicts with file from package kmod-vmware-tools-vmxnet3-1.0.37.0-2.6.18.8.el5.3.x86_64
 file /lib/modules/2.6.18-8.el5/extra/vmware-tools-vmxnet/vmxnet.ko from install of kmod-vmware-tools-vmxnet-2.0.9.1-2.6.18.8.el5.3.x86_64 conflicts with file from package kmod-vmware-tools-vmxnet-2.0.9.0-2.6.18.8.el5.3.x86_64
</code></pre>
<p>The way that I was fixing it, was simply removing all of VMWare Tools packages, doing my updates and then reinstalling.. I would use a simple for loop for this:</p>
<pre><code class="lang-bash">$ for i in $(rpm -qa | grep -I vmware);do yum -y remove $i;done
</code></pre>
<p>This worked, but it was extremely annoying when you needed to upgrade a bunch of systems, and there was always that issue of forgetting to re-install the yum packages after and then you would reboot the node and the vmxnet3 driver wasn&#39;t installed and your network interface wouldn&#39;t come up.</p>
<p>I then stumbled upon a fancy package that you can install to relieve you from these hassles, it&#39;s called yum-kmod. Once I found this package, it saved from having to remove the vmware packages completely. Just simply install yum-mod and it will handle the conflicts for you.</p>
<pre><code class="lang-bash">$ yum -y install yum-kmod
</code></pre>
<p>Please note, this is completely different on RHEL6 because yum-kmod is built into RHEL6. It&#39;s really a pain, and I&#39;ll write an additional article on this once I get to that point. I hope this helps you guys if you run into this issue, which is extremely frustrating.</p>
<p>If you know of a better way to resolve this, let us know.. but this seems to do the job, hassle-free.</p>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[RHEL: Cannot retrieve repository metadata (repomd.xml) for repository]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/rhel-cannot-retrieve-repository-metadata-repomd-xml-for-repository/</link>
      <guid isPermaLink="true">http://localhost:3333/rhel-cannot-retrieve-repository-metadata-repomd-xml-for-repository/</guid>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Wed, 29 Jan 2014 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>I&#39;ve ran into this issue a few times where I currently work, and it was extremely frustrating trying to figure out why I was unable to update our systems via yum, and kept receiving this error.</p>
<pre><code class="lang-bash">$ yum update
Loaded plugins: fastestmirror, rhnplugin, security
This system is receiving updates from RHN Classic or RHN Satellite.
Loading mirror speeds from cached hostfile
Error: Cannot retrieve repository metadata (repomd.xml) for repository: rhel-x86_64-server-5. Please verify its path and try again
</code></pre>
<p>What we found for this issue, is the server was unable to retrieve the metadata from the repository, just like it said. Meaning that a firewall or network issue was happening with the node, or the repository is unavailable. In our case, it was simply because we didn&#39;t configure the node to use the proxy that we had set up to reach the repository. So we made the following change:</p>
<p>Enable the http proxy settings in the up2date config file:</p>
<pre><code class="lang-bash">$ vim /etc/sysconfig/rhn/up2date
And set these values:

enableProxy[comment]=Use a HTTP Proxy
enableProxy=1

httpProxy[comment]=HTTP proxy in host:port format, e.g. squid.redhat.com:3128
httpProxy=proxy.address.com:3128
</code></pre>
<p>Hopefully this works for you, if it doesn&#39;t and you find other ways to resolve this, please post in the comments letting us know!</p>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[Simple Bash Script to Sort File Types into Directories]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/simple-bash-script-to-sort-file-types-into-directories/</link>
      <guid isPermaLink="true">http://localhost:3333/simple-bash-script-to-sort-file-types-into-directories/</guid>
      <category><![CDATA[automation]]></category>
      <category><![CDATA[bash]]></category>
      <category><![CDATA[scripting]]></category>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Wed, 04 Sep 2013 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>Often times I find myself needing to sort a lot of files into certain directories, for example I have downloaded a lot of free ebooks and a lot of them have different file types such as epub, mobi, azw, or even pdf and I want to keep them sorted into their respective folders to make it easier to pull up particular file types. To quickly do this I wrote a very simple and basic bash script that will allow me to do this.</p>
<pre><code class="lang-bash">#!/bin/bash

array=( epub mobi pdf azw )
bookdir=&quot;/books&quot;
sourcedir=&quot;/home/user/&quot;

for i in &quot;${array[@]}&quot;
do
 echo &quot;Moving books with file type: $i&quot;
 find $sourcedir -type f -iname &quot;*.$i&quot; -print0 | xargs -0 -I file mv -v &quot;file&quot; &quot;$bookdir/$i/&quot;
 chown -R apache.apache &quot;$bookdir/$i&quot;
done
</code></pre>
<p>All it does is look for particular files that end in the extension that is in the array, and then move them into the bookdir/extension folder. So as an example if I had file1.epub in the /home/user/ directory, it would find that file and then move it to /books/epub.</p>
<p>This is a very basic script, and is just here to help people get the idea of how to automate tasks that you would normally do manually.</p>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[10 Simple Quick Keys in BASH to Make Your Life Easier]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/10-simple-quick-keys-in-bash-to-make-your-life-easier/</link>
      <guid isPermaLink="true">http://localhost:3333/10-simple-quick-keys-in-bash-to-make-your-life-easier/</guid>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Sun, 01 Sep 2013 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>Knowing how to quickly manipulate the command line in the BASH shell will save you a lot of time and frustration.  Here are 10 Simple key combinations that will make your life easier and speed up your line editing skills.</p>
<p>CTRL + a - Places your cursor at the beginning of the line you are on</p>
<p>CTRL + e - Places your cursor at the end of the current line you are on</p>
<p>CTRL + u - Clears the line of everything <b>BEFORE</b> the cursor</p>
<p>CTRL + k - Clears the line of everything <b>AFTER</b> the cursor</p>
<p>CTRL + d - Deletes the character under the curser</p>
<p>CTRL + p - Goes back a previous command in history.  This is the same as using the up arrow.</p>
<p>CTRL + n - Goes forward a command in history.  This is the same as using the down arrow.</p>
<p>ALT + f - Move forward one word</p>
<p>ALT + b - Move backward one word</p>
<p>ALT + . - Inserts the last word from the previous history entry</p>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[Disabling root login on Linux]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/disabling-root-login-on-linux/</link>
      <guid isPermaLink="true">http://localhost:3333/disabling-root-login-on-linux/</guid>
      <category><![CDATA[root]]></category>
      <category><![CDATA[login]]></category>
      <category><![CDATA[disable]]></category>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Wed, 14 Aug 2013 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>One of the biggest mistakes any Linux Administrator can make is allowing root to log into the server directly via SSH, the reason this is a huge mistake is root is an account that everyone knows the name of so any cracker can brute force your server and obtain the root password and cause a load of headaches for you. This security vulnerability, if you want to call it that is very easy to fix. It&#39;s a lot better to create a separate account that you use and then you can grant that user sudo rights to root.</p>
<p><strong>Note: Make sure you have a regular user account and they have sudo privileges before you disable the root login. I will go over how to do this first, before disabling root login.</strong></p>
<p>Create and enable sudo rights to a new user</p>
<pre><code class="lang-bash">$ useradd -G wheel &lt;new username&gt;
</code></pre>
<p>Now set a password for the newly created user</p>
<pre><code class="lang-bash">passwd &lt;new username&gt;
</code></pre>
<p>Then you must make sure that the wheel group is enabled, to do this you can use the visudo command which will open the sudoers file up in the VI editor</p>
<pre><code class="lang-bash">$ visudo
</code></pre>
<p>Inside the file, go down towards the bottom and you&#39;ll see something like this:</p>
<pre><code class="lang-bash">## Allows people in group wheel to run all commands
# %wheel ALL=(ALL) ALL
</code></pre>
<p>All you need to do is simply remove the hash before the %wheel line and then write, save, and quit the file.</p>
<pre><code class="lang-bash">## Allows people in group wheel to run all commands
%wheel ALL=(ALL) ALL
</code></pre>
<p>Now let&#39;s disable root login. To do this, we&#39;ll need to edit the sshd_config file which is the configuration file that SSHD uses. Depending on your distro, will depend on where this file is located typically it is located in /etc/ssh/, sudo to root as you new user and edit this file</p>
<pre><code class="lang-bash">$ vi /etc/ssh/sshd_config
</code></pre>
<p>Inside this file search for a line that says &quot;PermitRootLogin&quot; it will be in a block that looks like this:</p>
<pre><code class="lang-bash">
# Authentication:

#LoginGraceTime 2m
#PermitRootLogin yes
#StrictModes yes
#MaxAuthTries 6
#MaxSessions 10
</code></pre>
<p>Make the PermitRootLogin line look like this, which will disable logging in as root via ssh:</p>
<pre><code class="lang-bash">PermitRootLogin no
</code></pre>
<p>Now we need to make the changes take affect, to do this we simply restart the sshd service (your connection will not be lost)</p>
<pre><code class="lang-bash">$ /etc/init.d/sshd restart
</code></pre>
<p>Once it reloads, root is now disabled on your system and you have a user that you can sudo as.</p>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[How to add a new Linux user account]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/how-to-add-a-new-linux-user-account/</link>
      <guid isPermaLink="true">http://localhost:3333/how-to-add-a-new-linux-user-account/</guid>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Mon, 15 Jul 2013 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>Adding a new user to a Linux machine is one of the simplest things to do. The useradd command works under any linux distro. You will however need to be root to add the new user via the useradd command, the syntax is as follows:</p>
<p><strong>useradd [options] (username)</strong></p>
<p>By using the above command, it will do everything for you including creating the user&#39;s home directory, adding them to /etc/passwd and /etc/shadow. So let&#39;s say you wanted to add a new user named digest, you would simply do the following:</p>
<pre><code class="lang-bash">$ useradd digest
</code></pre>
<p>One example that you can use for the options as well is to add a user to a particular group when you add them.. For example, let&#39;s say we wanted the digest user to be in the wheel group so they can sudo to root. We would simply issue the following command:</p>
<pre><code class="lang-bash">$ useradd -G wheel digest
</code></pre>
<p>Or if you wanted to create a group with the same username you can use the -U flag like so:</p>
<pre><code class="lang-bash">$ useradd -U digest
</code></pre>
<p>If you added the user, you then need to set a user&#39;s password. If you do not set a user&#39;s password the account will be in a locked status. To unlock it, simply issue the following command to set a password:</p>
<pre><code class="lang-bash">$ passwd digest
</code></pre>
<p>It then will prompt you to input the new password, and to confirm the new password.</p>
<p>There are many additional features and things you can do with the useradd command, simply run man useradd on a linux server to see all the different flags.</p>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[Remove weird special characters from your WordPress Database]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/remove-weird-special-characters-from-your-wordpress-database/</link>
      <guid isPermaLink="true">http://localhost:3333/remove-weird-special-characters-from-your-wordpress-database/</guid>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Mon, 08 Jul 2013 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>A few months ago I had to restore our website from a backup, and noticed that when I did so almost all of the posts that we have done had weird special characters inside them. I started to manually edit the posts and remove the special characters and when I realized how many there were I decided to do it a quick and easy way. With a little MySQL magic you can easily replace any of the weird special characters in all of your posts saving you a ton of time. The special character that I personally had floating around was: Â  -- So I simply logged into my MySQL database via a command line:</p>
<pre><code class="lang-bash">$ mysql -u username -p &lt;database&gt;
</code></pre>
<p>Then you want to use the database for your wordpress blog:</p>
<pre><code class="lang-bash">&gt; use &lt;database&gt;;
</code></pre>
<p>Then all you need to do is paste the following query to replace the weird Â character with a blank space:</p>
<pre><code class="lang-bash">&gt; UPDATE wp_posts SET post_content = REPLACE(post_content, &#39;Â&#39;, &#39;&#39;);
</code></pre>
<p>One thing to note, though. If you changed your prefix from the standard wp_ you will want to update the above query to reflect whatever you prefixed your tables with. If you have any other weird characters, simply copy it from a post and replace it in the query and re-run it against your database and you will quickly clear them out. If you have them in your comments then simply replace the post_content items with comment_content and make sure you change wp_post to wp_comments.</p>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[How to install a LAMP (Linux, Apache, MySQL, PHP) stack on CentOS/RHEL]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/how-to-install-a-lamp-linux-apache-mysql-php-stack-on-centos-rhel/</link>
      <guid isPermaLink="true">http://localhost:3333/how-to-install-a-lamp-linux-apache-mysql-php-stack-on-centos-rhel/</guid>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Fri, 19 Apr 2013 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<h3 id="what-is-a-lamp-stack-">What is a LAMP Stack?</h3>
<p>A LAMP stack is a group of software that you install on a server to serve your own websites. Much like how this blog is being served to you, I purchased a VPS and then made a LAMP stack out of the server. LAMP stands for Linux, Apache, MySQL and PHP. Typically the Linux part is already taken care of for you when you purchase a new VPS or dedicated server from a hosting provider so we&#39;ll skip the Linux portion (I&#39;ll write up an article later on how install Linux). So with that being said, you should have a Linux server up and running and you should be able to log in as root (or use sudo) to install and configure the rest of the stack.</p>
<p>You will require root privileges to install the stack, if you don&#39;t have this then you will not be able to complete this.</p>
<h2 id="lamp-installation">LAMP Installation</h2>
<p>With all of the above said, it&#39;s now time to install the required components.</p>
<h3 id="-step-1-install-apache">* Step 1: Install Apache</h3>
<p>To install Apache, simply type the following in a open terminal /shell</p>
<pre><code class="lang-bash">$ sudo yum -y install httpd
</code></pre>
<p>Once the install finishes you then will want to start the process which there are two ways of doing it.. I&#39;ll provide both commands, but both of them do the exact same thing just in a different manner.</p>
<pre><code class="lang-bash">$ sudo service httpd start
</code></pre>
<p>or</p>
<pre><code class="lang-bash">$ /etc/init.d/httpd start
</code></pre>
<p>Once you have done the above, you then can verify that apache has installed correctly and it is running by opening up a web browser and navigating to your server&#39;s IP address such as <a href="http://192.168.1.1">http://192.168.1.1</a> and it should display the apache test page which looks like this:</p>
<p><img src="/assets/articles/apache-test-page.jpg" alt="apache-test-page"></p>
<p>If you want apache to automatically start when you reboot your server just simply chkconfig it to on like so:</p>
<pre><code class="lang-bash">$ sudo chkconfig httpd on
</code></pre>
<h3 id="-step-2-install-mysql">* Step 2: Install MySQL</h3>
<p>To install MySQL, simply type the following in a open terminal / shell</p>
<pre><code class="lang-bash">$ sudo yum -y install mysql-server
</code></pre>
<p>Please note, when MySQL is installing it will ask you two questions, just simply type yes for both of them and MySQL will complete it&#39;s installation.</p>
<p>Once the install finishes you then will want to start the MySQL process by issuing the following command, and much like apache there are two ways of doing it and I&#39;ll show you both.</p>
<pre><code class="lang-bash">$ sudo service mysqld start
</code></pre>
<p>or</p>
<pre><code class="lang-bash">$ sudo /etc/init.d/mysqld start
</code></pre>
<p>Once you have started the MySQL server you should then set a MySQL root password and run the mysql_secure_installation to set credentials and security for your MySQL instance, to start the setup simply run the following command:</p>
<pre><code class="lang-bash">$ sudo /usr/bin/mysql_secure_installation
</code></pre>
<p>By default the root password for MySQL is blank so when prompts you for a password, just hit enter. After you hit enter, it will ask you if you want to set a root password to which you will want to type Y and follow the instructions.</p>
<p>The actual setup of MySQL is pretty much automatic, it only asks you a few questions and then does the rest of the configuration. You simply need to answer yes or no to a few questions and you&#39;ll be good to go for MySQL. Typically for a easy installation, you just say yes to all the options. The options will look like this:</p>
<pre><code class="lang-bash">By default, a MySQL installation has an anonymous user, allowing anyone
to log into MySQL without having to have a user account created for
them. This is intended only for testing, and to make the installation
go a bit smoother. You should remove them before moving into a
production environment.

Remove anonymous users? [Y/n] y
 ... Success!

Normally, root should only be allowed to connect from &#39;localhost&#39;. This
ensures that someone cannot guess at the root password from the network.

Disallow root login remotely? [Y/n] y
... Success!

By default, MySQL comes with a database named &#39;test&#39; that anyone can
access. This is also intended only for testing, and should be removed
before moving into a production environment.

Remove test database and access to it? [Y/n] y
 - Dropping test database...
 ... Success!
 - Removing privileges on test database...
 ... Success!

Reloading the privilege tables will ensure that all changes made so far
will take effect immediately.

Reload privilege tables now? [Y/n] y
 ... Success!

Cleaning up...

All done! If you\&#39;ve completed all of the above steps, your MySQL
installation should now be secure.

Thanks for using MySQL!
</code></pre>
<p>If you want MySQL to automatically start when you reboot your server just simply chkconfig it to on like so:</p>
<pre><code class="lang-bash">$ sudo chkconfig mysqld on
</code></pre>
<h3 id="-step-3-install-php">* Step 3: Install PHP</h3>
<p>To install PHP, simply type the following in a open terminal /shell</p>
<pre><code class="lang-bash">$ sudo yum -y install php php-mysql
</code></pre>
<p>Once the install finishes you have successfully installed PHP and the PHP-MySQL plugin. You now will want to restart apache, so apache then will be able to use php that you just installed and you can then create php pages. Simply restart apache by issuing the following command:</p>
<pre><code class="lang-bash">$ sudo /etc/init.d/httpd restart
</code></pre>
<p>And you&#39;re done! You now have a complete LAMP stack. See my future posts on good ways to setup your vhost structure to serve multiple domains on one IP and more. Let me know if you have any questions or run into any snags!</p>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[Check SSL certificate from command line]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/check-ssl-certificate-from-command-line/</link>
      <guid isPermaLink="true">http://localhost:3333/check-ssl-certificate-from-command-line/</guid>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Fri, 12 Oct 2012 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>To check if a SSL certificate is valid or not from a command line, simply run the following command:</p>
<pre><code class="lang-bash">$ openssl s_client -connect google.com:443 | openssl x509 -text
</code></pre>
<p>And it will return all of the SSL certificate information:</p>
<pre><code class="lang-bash">
depth=1 /C=US/O=Google Inc/CN=Google Internet Authority
verify error:num=20:unable to get local issuer certificate
verify return:0
Certificate:
 Data:
 Version: 3 (0x2)
 Serial Number:
 16:6f:13:92:00:00:00:00:6a:19
 Signature Algorithm: sha1WithRSAEncryption
 Issuer: C=US, O=Google Inc, CN=Google Internet Authority
 Validity
 Not Before: Sep 27 01:23:05 2012 GMT
 Not After : Jun 7 19:43:27 2013 GMT
 Subject: C=US, ST=California, L=Mountain View, O=Google Inc, CN=*.google.com
 Subject Public Key Info:
 Public Key Algorithm: rsaEncryption
 RSA Public Key: (1024 bit)
 Modulus (1024 bit):
 00:e1:85:7a:f3:c0:96:0b:61:65:5c:ff:f5:ff:99:
 45:b9:1c:ce:e9:1c:22:5d:2d:23:06:8d:18:b0:ba:
 28:10:75:c1:dd:71:b6:72:28:cb:50:54:c7:b7:fc:
 9b:72:d9:db:62:20:40:aa:c9:46:95:da:bc:c1:62:
 14:cb:4f:4a:db:69:52:de:3d:af:56:34:31:75:02:
 9e:b5:64:ca:23:fc:00:6f:ee:bc:9b:21:ae:dc:dc:
 6d:3e:13:7b:c9:83:ee:e1:44:fa:d0:c2:15:89:ae:
 3f:23:9e:9b:2a:6a:26:e4:da:c6:2b:55:ec:70:34:
 8e:2a:95:75:57:23:9c:83:31
 Exponent: 65537 (0x10001)
 X509v3 extensions:
 X509v3 Extended Key Usage:
 TLS Web Server Authentication, TLS Web Client Authentication
 X509v3 Subject Key Identifier:
 26:27:CE:D1:93:A3:4D:84:6E:BF:1D:82:13:49:9D:59:15:50:7A:5B
 X509v3 Authority Key Identifier:
 keyid:BF:C0:30:EB:F5:43:11:3E:67:BA:9E:91:FB:FC:6A:DA:E3:6B:12:24

X509v3 CRL Distribution Points:
 URI:http://www.gstatic.com/GoogleInternetAuthority/GoogleInternetAuthority.crl

Authority Information Access:
 CA Issuers - URI:http://www.gstatic.com/GoogleInternetAuthority/GoogleInternetAuthority.crt

X509v3 Basic Constraints: critical
 CA:FALSE
 X509v3 Subject Alternative Name:
 DNS:*.google.com, DNS:google.com, DNS:*.youtube.com, DNS:youtube.com, DNS:*.youtube-nocookie.com, DNS:youtu.be, DNS:*.ytimg.com, DNS:*.google.com.br, DNS:*.google.co.in, DNS:*.google.es, DNS:*.google.co.uk, DNS:*.google.ca, DNS:*.google.fr, DNS:*.google.pt, DNS:*.google.it, DNS:*.google.de, DNS:*.google.cl, DNS:*.google.pl, DNS:*.google.nl, DNS:*.google.com.au, DNS:*.google.co.jp, DNS:*.google.hu, DNS:*.google.com.mx, DNS:*.google.com.ar, DNS:*.google.com.co, DNS:*.google.com.vn, DNS:*.google.com.tr, DNS:*.android.com, DNS:android.com, DNS:*.googlecommerce.com, DNS:googlecommerce.com, DNS:*.url.google.com, DNS:*.urchin.com, DNS:urchin.com, DNS:*.google-analytics.com, DNS:google-analytics.com, DNS:*.cloud.google.com, DNS:goo.gl, DNS:g.co, DNS:*.gstatic.com, DNS:*.googleapis.cn
 Signature Algorithm: sha1WithRSAEncryption
 b9:f8:3a:fc:89:6d:ef:57:df:67:5d:17:48:87:50:d5:df:4d:
 54:dc:e9:ff:24:be:1b:3e:c9:21:49:26:de:c0:06:bd:84:5f:
 ce:eb:c0:d6:6d:88:5f:b4:4d:cc:de:f1:bd:42:86:fb:dc:66:
 3c:14:f8:73:0e:52:93:5d:2d:97:0b:f1:4f:74:7a:1e:0d:a9:
 bf:c3:de:96:72:64:4b:0b:ea:22:3a:40:6e:77:2f:e1:13:0c:
 6c:f4:e6:e9:54:d3:43:cc:38:0c:55:3c:47:9c:73:99:3a:cb:
 bf:0e:49:84:f9:7a:33:f5:3a:a5:ea:25:44:6d:79:c9:be:a2:
 9f:cd
-----BEGIN CERTIFICATE-----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-----END CERTIFICATE-----
</code></pre>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[Having fun with MAMP, Automator and Growlnotify on my Macbook]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/having-fun-with-mamp-automator-and-growlnotify-on-my-macbook/</link>
      <guid isPermaLink="true">http://localhost:3333/having-fun-with-mamp-automator-and-growlnotify-on-my-macbook/</guid>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Sat, 25 Aug 2012 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<h3 id="1-automator-http-www-tekdefense-com-automater-">1) <a href="http://www.tekdefense.com/automater/">Automator</a></h3>
<p>I still consider myself somewhat new to the whole <em>MacBook</em> thing, and I discovered something that seemed kind of fun, called Automator, something that only comes with Apple products.</p>
<p>From the Apple website:</p>
<blockquote>
<p>With Automator you can automate much of what you do with your computer. Create and print a family directory with the contacts in your Address Book. Find and add images from your favorite websites to iPhoto. Print your documents to your iPad. Instantly rename dozens of files in the Finder. Even perform scheduled backups of important information. There&#39;s no limit to what you can do, and Automator can do in seconds and minutes what could take you hours to do by hand.</p>
</blockquote>
<p>Basically, you can automate anything you do manually, and save it into an &quot;Application Bundle&quot;, (AKA, a .app file).</p>
<h3 id="2-mamp-http-mamp-info-">2) <a href="http://mamp.info">MAMP</a></h3>
<p>Also, I do a lot of local development on my MacBook, most of which gets pushed to a basic LAMP stack setup. To emulate the LAMP stack as closely as possible, I use the free version of MAMP. MAMP is a very handy tool if you are developing for a LAMP environment, with a little bit of customization, you can just drop in new VirtualHost config files and Host file entries and you have yourself a local version of whatever website you are editing.</p>
<h3 id="3-growl-http-growl-info-">3) <a href="http://growl.info">Growl</a></h3>
<p>Growl is a popular app in the appstore, unfortunately it isn&#39;t free, but cmon, you already bought an Apple product which was ridiculously overpriced, you can afford $1.99 application. A lot of other applications tie into this, and it basically just shows notifications on your desktop. It has an API that you can tie into, as well as a binary tool &quot;<a href="http://growl.info/downloads">growlnotify</a>&quot; that you can use to display notifications.</p>
<hr>
<p>Down to the point. When creating PHP/Apache/MySQL based application, I don&#39;t like to have the error displayed on the website, but then that means that I need to monitor the error logs to make sure everything is solid.</p>
<p>Well, if you combine the 3 applications, you can accomplish the log monitoring pretty easily. I created an Automator Application with the action: &quot;Run Shell Script&quot;. That in itself is a whole new blog post, but id rather just give you a link to someone who has already documented how to create a basic .app to run a shell script, <a href="http://arstechnica.com/apple/2011/03/howto-build-mac-os-x-services-with-automator-and-shell-scripting/">here</a>.</p>
<p>So there are 3 different .app&#39;s that I made, one for the MySQL logs, one for the Apache logs, and then one for PHP, the PHP one is a little more sophisticated than the other two.</p>
<p>All 3 of these scripts run with basic logic, just tail the log file and pipe the new output to an action that will use growlnotify to alert me for the new log entries.</p>
<p><strong>Apache Log Monitor</strong></p>
<pre><code class="lang-bash">/usr/bin/tail -fn0 /Applications/MAMP/logs/apache_error.log | while read line
do
 /usr/local/bin/growlnotify --title &quot;Apache Notice&quot; --message &quot;$line&quot; --image /Users/my.name/Pictures/Apache_Icon.png
done
</code></pre>
<p><strong>MySQL Log Monitor</strong></p>
<pre><code class="lang-bash">/usr/bin/tail -fn0 /Applications/MAMP/logs/mysql_error_log.err | while read line
do
    /usr/local/bin/growlnotify --title &quot;MySQL Notice&quot; --message &quot;$line&quot; --image /Users/my.name/Pictures/MySQL_Icon.png
done
</code></pre>
<p><strong>PHP Log Monitor</strong></p>
<pre><code class="lang-bash">/usr/bin/tail -fn0 /Applications/MAMP/logs/php_error.log | while read line
do
    msg=$(echo $line | awk -F: &#39;{$1=&quot;&quot;; $2=&quot;&quot;; $3=&quot;&quot;; print $0}&#39;| sed -e &#39;s/^[ t]*//&#39;)
    if [ -n &quot;$(echo $line | grep &#39;PHP Fatal&#39;)&quot; ]
    then
        /usr/local/bin/growlnotify -t &quot;PHP Fatal Error&quot; -m &quot;$msg&quot; -s --image /Users/my.name/Pictures/PHP_Fatal.jpg
    elif [ -n &quot;$(echo $line | grep &#39;PHP Parse&#39;)&quot; ]
    then
        /usr/local/bin/growlnotify -t &quot;PHP Parser Error&quot; -m &quot;$msg&quot; --image /Users/my.name/Pictures/PHP_Parser.jpg
    elif [ -n &quot;$(echo $line | grep &#39;PHP Warning&#39;)&quot; ]
    then
        /usr/local/bin/growlnotify -t &quot;PHP Warning&quot; -m &quot;$msg&quot; --image /Users/my.name/Pictures/PHP_Warning.jpg
    elif [ -n &quot;$(echo $line | grep &#39;PHP Notice&#39;)&quot; ]
    then
        /usr/local/bin/growlnotify -t &quot;PHP Notice&quot; -m &quot;$msg&quot; --image /Users/my.name/Pictures/PHP_Notice.jpg
    else
        /usr/local/bin/growlnotify -t &quot;PHP Log&quot; -m &quot;$msg&quot; --image /Users/my.name/Pictures/PHP_Other.jpg
    fi
done
</code></pre>
<p>The PHP log monitor will detect what type of error it is, (Fatal, Parser, Warning, Notice, etc), and show a different/specific icon for the error type. It will also add the &quot;sticky&quot; option to the Fatal errors, which just makes the notice stay in place until you click the acknowledgement button.</p>
<p>So just put these into an Apple Application Stack, and the next step is to get these to run when you login, which is obviously optional, but it&#39;s a lot easier than running these manually every time you want to do some developing.</p>
<p>To get them to run when you login, just add all 3 of these .app&#39;s to the Login Items list under your system preferences.</p>
<p>Have fun!</p>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[CodeIgniter URI Associative Array Parsing]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/codeigniter-uri-associative-array-parsing/</link>
      <guid isPermaLink="true">http://localhost:3333/codeigniter-uri-associative-array-parsing/</guid>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Mon, 20 Aug 2012 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>When using CI (CodeIgniter), I ran into a problem that would put my entire application out of play unless fixed;</p>
<p>I pass arrays through GET, you may ask why I would do such a thing, and I really don&#39;t care to explain.</p>
<p>Anyways,</p>
<blockquote>
<p>index.php?foo[]=a&amp;foo[]=b&amp;bar[]=c&amp;bar[]=d</p>
</blockquote>
<p>Would transform into:</p>
<pre><code class="lang-php">array(
    [foo]   =&gt; array(
                    &#39;a&#39;, &#39;b&#39;
                    ),
    [bar]   =&gt; array(
                    &#39;c&#39;, &#39;d&#39;
                    )
);
</code></pre>
<p>However, with the <strong>url_to_assoc</strong>, it would turn it into</p>
<pre><code class="lang-php">$foo = &#39;b&#39;;
$bar = &#39;d&#39;;
</code></pre>
<p>This turns into a problem when you need arrays passed through the URL. So a couple of friends and I started to look at it, and we found out that it was the <strong>_url_to_assoc</strong> function within <strong>/syscore/core/URI.php</strong>.</p>
<p>With a little modification from Joshya Flyer, we were able to get it to work. Now, if you pass /foo/a/foo/b/bar/c/baz/d/baz/e, foo will be an array with both a and b, bar will be string with just c, and baz will be an array with just e.
The code is below.</p>
<pre><code class="lang-php">    function _uri_to_assoc($n = 3, $default = array(), $which = &#39;segment&#39;)
    {
        if ($which == &#39;segment&#39;)
        {
            $total_segments = &#39;total_segments&#39;;
            $segment_array = &#39;segment_array&#39;;
        }
        else
        {
            $total_segments = &#39;total_rsegments&#39;;
            $segment_array = &#39;rsegment_array&#39;;
        }

        if ( ! is_numeric($n))
        {
            return $default;
        }

        if (isset($this-&gt;keyval[$n]))
        {
            return $this-&gt;keyval[$n];
        }

        if ($this-&gt;$total_segments() &lt; $n)
        {
            if (count($default) == 0)
            {
                return array();
            }

            $retval = array();
            foreach ($default as $val)
            {
                $retval[$val] = FALSE;
            }
            return $retval;
        }

        $segments = array_slice($this-&gt;$segment_array(), ($n - 1));

        $var = array_chunk($segments, 2);

        $result = array();

        /* Custom modificatioms begin here
        */
        foreach ($var as $eachSet)
        {
            if (isset($eachSet[0]) and isset($eachSet[1]))
                if (isset($result[$eachSet[0]]))
                {
                    if (is_array($result[$eachSet[0]]))
                    {
                        $result[$eachSet[0]][] = $eachSet[1];
                    }
                    else
                    {
                        $result[$eachSet[0]] = array($result[$eachSet[0]]);
                        $result[$eachSet[0]][] = $eachSet[1];
                    }
                }
                else
                {
                    $result[$eachSet[0]] = $eachSet[1];
                }
        }

        // Cache the array for reuse
        $this-&gt;keyval[$n] = $result;

        /*
        print &quot;URI TO ARR&lt;br&gt;&quot;;
        print_r($result);
        print &quot;&lt;hr&gt;&quot;;
        */
        return $result;
    }
</code></pre>
<p>Now, of course, this entails doing just the opposite. taking an array and making it into a URI. THe code for that is below.</p>
<pre><code class="lang-php">function assoc_to_uri($array)
    {
        $temp = array();
        foreach ($array as $key =&gt; $val)
        {
            if(is_array($val))
            {
                foreach($val as $subval)
                {
                    $temp[] = $key;
                    $temp[] = $subval;
                }
            }
            else
            {
                $temp[] = $key;
                $temp[] = $val;
            }
        }
        return implode(&#39;/&#39;, $temp);
    }
</code></pre>
<p>It seems to be working well for us, if you have any questions, just let us know.</p>
<p>I posted a thread about it on CI, they didn&#39;t seem very helpful at all: <a href="http://codeigniter.com/forums/viewthread/222168/">http://codeigniter.com/forums/viewthread/222168/</a></p>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[Marking ALL Your Google Voice Mails as Read]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/marking-all-your-google-voice-mails-as-read/</link>
      <guid isPermaLink="true">http://localhost:3333/marking-all-your-google-voice-mails-as-read/</guid>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Tue, 24 Jul 2012 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>Are you like me and never actually go into google voice to mark them as read as you do email? I just look at the email that I get in my gmail box and look at the transcription. Hardly ever actually log into the google.com/voice site.</p>
<p>Well with the new Jelly Bean voice integration into SMS and Call log, all those &quot;missed&quot; calls are annoying. And Googles web interface would only allow you to mark 10 read at a time. (Take WAYYYY to long to do the 1600+ that I had). Sooo, as any nerd would do... there has to be a better way right?</p>
<p>Well, I&#39;m not much of a Python person but this might turn me onto some cool ideas and things you can do with Google Voice and Python in general.</p>
<p>Here is how you can mark all you unread msg&#39;s as read. No matter how many you have.</p>
<ul>
<li>First Install the packages you need.</li>
</ul>
<p>From Site: <a href="http://sphinxdoc.github.com/pygooglevoice/install.html#setups">http://sphinxdoc.github.com/pygooglevoice/install.html#setups</a></p>
<pre><code class="lang-bash">$ yum install python python-setuptools
$ sudo easy_install simplejson
$ sudo easy_install -U pygooglevoice
</code></pre>
<ul>
<li>Next, you must make a change to the LOGIN value of the settings.py file in the pygooglevoice egg package.</li>
<li>Its the settings.py file: In my case it was located here: /usr/lib/python2.6/site-packages/pygooglevoice-0.5-py2.6.egg/settings.py</li>
</ul>
<pre><code class="lang-python">LOGIN = https://accounts.google.com/ServiceLogin?service=grandcentral
</code></pre>
<ul>
<li>Now create a simple script</li>
<li>Courtesy of this guy: <a href="http://webapps.stackexchange.com/a/10105">http://webapps.stackexchange.com/a/10105</a></li>
</ul>
<pre><code class="lang-python">from googlevoice import Voice,util
voice = Voice()
voice.login(&#39;YOUR USERNAME&#39;, &#39;YOUR PASSWORD&#39;)

while True :
 folder = voice.search(&#39;is:unread&#39;)
 if folder.totalSize  break
 util.print_(folder.totalSize)
 for message in folder.messages:
 util.print_(message)
 message.mark(1)
</code></pre>
<ul>
<li>Then simply execute the script and save yourself HOURS of time!</li>
</ul>
<pre><code class="lang-bash">python your_script_name.py
</code></pre>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[Perl - Executing System Commands]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/perl-executing-system-commands/</link>
      <guid isPermaLink="true">http://localhost:3333/perl-executing-system-commands/</guid>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Sun, 15 Jul 2012 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>Perl comes with the ability to execute commands at the OS level, on any OS you are running (Linux, Windows, Mac, etc etc)</p>
<p>There are three options available, all are different:</p>
<ol>
<li><strong>system</strong> - Returns an exit code of the command you ran</li>
<li><strong>exec</strong> - doesn&#39;t return anything at all</li>
<li><strong>backticks ( `` )</strong> - Returns the output of the command</li>
</ol>
<p>The most common is the backticks, you can run something, then parse the output. It&#39;s common to split it by the line returns into an array, then parse it line by line.</p>
<p>I have always believed executing system commands from a language other than the language your&#39;e actually using in the execution, (Bash, Windows DOS, etc), is bad practice. I mean having a script thats half perl and half bash just seems like it may be inconsistent or unreliable.</p>
<p>If you consult with the Google Gods, it seems thats a popular opinion, nearly every result says do NOT use it, unless...</p>
<ol>
<li>You need to capture (or supress) the output.</li>
<li>There exists no built-in function or Perl module to do the same task, or you have a good reason not to use the module or built-in.</li>
<li>You sanitise your input.</li>
<li>You check the return value.</li>
</ol>
<p>So, the 2nd one pretty much rules out a LOT of reasons to use system, exec, or backticks. (moving files, copying files, even using ssh and scp on Linux). The only time I have ever found it OK to use, was when I was using <a href="http://www.linuxdigest.org/wp-content/uploads/bb897553.aspx">PsExec</a> on a Windows OS, because I needed to execute commands on a remote Windows server, and parse the output, (Did have to pipe it all to STDOUT though).</p>
<p>While researching this info, I came across a nice perl module, <a href="http://www.linuxdigest.org/wp-content/uploads/Simple.pm">IPC::System::Simple</a>. looks like a nice and easy way to execute remote commands, using a perl module! It can do the same as the Perl commands &#39;system&#39;, &#39;exec&#39; and even the backticks. Only time that would really be useful though, is if you can install any perl module (EG: if you always run it locally). Where I work, we usually have to make the scripts compatible with all servers, and its best to do it without requiring other perl modules to be installed. So, in my opinion, use the IPC::System::Simple if you&#39;re executing the script locally, but if you need to run it on any server, it&#39;s just easy to use the backticks, system or exec commands...</p>
<p>I made this post primarily just to annoy The Geoff Hatch.</p>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[Installing Graphite on CentOS 6]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/installing-graphite-on-centos-6/</link>
      <guid isPermaLink="true">http://localhost:3333/installing-graphite-on-centos-6/</guid>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Sun, 01 Jul 2012 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>This is the fastest way I have found to get Graphite installed. It is a rough tutorial but this is the fasteset way I have found to get it up and running.</p>
<ul>
<li>Install epel repo</li>
</ul>
<pre><code class="lang-bash">rpm -ivh http://mirror.us.leaseweb.net/epel/6/i386/epel-release-6-5.noarch.rpm
</code></pre>
<ul>
<li>Install dependencies</li>
</ul>
<pre><code class="lang-bash">yum install Django pycairo bitmap bitmap-fonts python-pip gcc python-devel mod_wsgi perl django-tagging
</code></pre>
<ul>
<li>You can try and compile from source, but if it can be avoided always use some sort of package manager, in this case we will use Python Pip!</li>
</ul>
<pre><code class="lang-bash">pip-python install carbon
pip-python install carbon
pip-python install whisper
pip-python install graphite-web
</code></pre>
<ul>
<li>Setup basic conf files (you may need to configure them more later, but for now example confs will do to get the basic interface up</li>
</ul>
<pre><code class="lang-bash">cd /opt/graphite/conf
cp carbon.conf.example carbon.conf
cp storage-schemas.conf.example storage-schemas.conf
cp graphite.wsgi.example graphite.wsgi
</code></pre>
<ul>
<li>Setup httpd conf file: Use this as an example: <a href="http://bazaar.launchpad.net/~graphite-dev/graphite/trunk/view/head:/examples/example-graphite-vhost.conf">Example Graphite Vhost</a></li>
</ul>
<pre><code class="lang-bash">vi /etc/httpd/conf.d/graphite.conf
</code></pre>
<ul>
<li>Setup graphite webapp</li>
</ul>
<h1 id="enable-debug">Enable debug</h1>
<pre><code class="lang-bash">cd /opt/graphite/webapp/graphite
cp local_settings.py.example local_settings.py
vi local_settings.py
</code></pre>
<pre><code class="lang-bash">python manage.py syncdb
</code></pre>
<h1 id="start-carbon">Start carbon</h1>
<pre><code class="lang-bash">cd /opt/graphite/
./bin/carbon-cache.py start
</code></pre>
<h1 id="chown-log-file-dir-so-apache-can-write">Chown log file dir so apache can write</h1>
<pre><code class="lang-bash">chown -R apache:apache /opt/graphite/storage/
</code></pre>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[Perl Tips And Tricks (Part 1)]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/perl-tips-and-tricks-part-1/</link>
      <guid isPermaLink="true">http://localhost:3333/perl-tips-and-tricks-part-1/</guid>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Sun, 01 Jul 2012 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>A good developer tries to use as little lines of code as possible. If theres 2 ways to do something, and one takes one line, and the other takes 3, then you typically would want to go with the one thats just one line of code. Below are 20 tips and tricks I have found over my years of perl experience.</p>
<p><em><strong>Note:</strong> These are in no specific order.</em></p>
<h4 id="-1-good-way-of-assigning-default-values"><strong>1)</strong> Good Way Of Assigning Default Values</h4>
<p>Theres many times in scripting where you need to setup default values for variables. For a while, I was using just the if statement, like this:</p>
<pre><code class="lang-perl">$var = &quot;Undefined&quot; unless $var;
</code></pre>
<p>However, I found a new favorite way to do this:</p>
<pre><code class="lang-perl">$var ||= &quot;Undefined&quot;;
</code></pre>
<p>Pretty simple right??</p>
<p>####2) Count Number Of Times A String Is In A Paragraph
Sometimes you need to be able to count how many times a specific string is found within a variable, heres a pretty easy and efficient way of doing so.</p>
<pre><code class="lang-perl">
my $paragraph = &quot;one two two three three three four four four four five five five five five&quot;;

my $word = $ARGV[0];

my $numtimes = 0;
$numtimes++ while ($paragraph =~ /b $word b/gx);

print &quot;$word shows up in the page $numtimes timesn&quot;;
</code></pre>
<p>####3) Using The Double Pipe Operator ( || )
We all know about the || operator, it basically means &quot;or&quot;. Heres an example usage:</p>
<pre><code class="lang-perl">SomeModule-&gt;bla() || die &quot;SomeModule failed: $!n&quot;;
</code></pre>
<p>Which basically means if SomeModule-&gt;bla() fails, then die and show the exit code.
But you know that you can use it more than once? It can save you a TON of lines of code.</p>
<p>Lets say you have a script like below:</p>
<pre><code class="lang-perl">if($var1) {
$name = $var1;
}
elsif($var2) {
 $name = $var2;
}
elsif($var3) {
 $name = $var3;
}
elsif($var4) {
 $name = $var4;
}
else {
 $name = &#39;John Doe&#39;;
}

print &quot;Your name is $name&quot;;
</code></pre>
<p>This just basically sees if $var1 is set, if so, set $name to $var1, if not, is $var2 set? If so, set $name to $var2.... etc etc, and if its not set, then set $name to &quot;John Doe&quot;.
But maybe theres an easier way to do this?</p>
<pre><code class="lang-perl">$name = $var1 || $var2 || $var3 || $var4 || &#39;John Doe&#39;;
print &quot;Your name is $name&quot;;
</code></pre>
<p>Does the exact same thing, and just one line of code, way easier isn&#39;t it?</p>
<p>####4) Using Perl Replication
One of the most annoying things for me, is going through someones script, and seeing something like this:</p>
<pre><code class="lang-perl">print &quot;==============================n&quot;;
print &quot;Settingsn&quot;;
print &quot;==============================n&quot;;
</code></pre>
<p>I see this, and its typically all over the script, makes me wonder how long they sat there and held down the &#39;=&#39; key...</p>
<p>Lets try it with replication. Perl comes with the &#39;x&#39; operator, which will replicate a letter/number/key/whatever, as many times as you want. Heres an example if the above script, using replication</p>
<pre><code class="lang-perl">print &quot;=&quot; x 30 . &quot;n&quot;;
print &quot;Settingsn&quot;;
print &quot;=&quot; x 30 . &quot;n&quot;;
</code></pre>
<p>Much easier and quicker!</p>
<p>####<strong>5) Short-Hand IF Statements</strong>
I&#39;m a big fan of these because a simple if-else statement can take up between 5 to 7 lines of code, and you really don&#39;t need to use that much space. These are also known as &quot;Ternary Operators&quot;, nearly every language supports them, but I don&#39;t see it utilized often.</p>
<p>Take the below if-else statement into consideration:</p>
<pre><code class="lang-perl">if($female){
$name = &#39;Jane&#39;;
}
else {
 $name = &#39;John&#39;;
}
</code></pre>
<p>So thats 6 lines of code, and all it does is check if $female is true, if so, $name is Jane, else, $name is John. But this can be completed in just one line, using a short-hand if statement, like below:</p>
<pre><code class="lang-perl">$name = ($female) ? &#39;Jane&#39; : &#39;John&#39;;
</code></pre>
<p>That does the exact same thing as the standard if-else above, no slower or faster, it&#39;s just quicker and cleaner.
If you want to save even more time, then maybe you don&#39;t even need to assign the name to variable, perhaps you just want to check $female right in the middle of the print command, like this:</p>
<pre><code class="lang-perl">print &#39;Hello, my name is &#39;. ($female) ? &#39;Jane&#39; : &#39;John&#39;. &quot;n&quot;;
</code></pre>
<p>####<strong>6) Get The Execution Time</strong>
There may be some cases where you wish to know how long it takes to get to a certain point in your script, or how long it takes to run it all together. Perl has a built in system variable, <strong>$^T</strong>, which can do this for you.</p>
<p>Heres a good example of how to use it.</p>
<pre><code class="lang-perl">my $t;

print &quot;Starting $0...n&quot;;

doSomething-&gt;module1();
$t = time - $^T;
print &quot;This script has been running for $t &quot; . ($t &gt; 1) ? &#39;seconds&#39; : &#39;second&#39; .&quot;n&quot;;

doSomething-&gt;module2();
$t = time - $^T;
print &quot;This script has been running for $t &quot; . ($t &gt; 1) ? &#39;seconds&#39; : &#39;second&#39; .&quot;n&quot;;

doSomething-&gt;module3();
$t = time - $^T;
print &quot;This script has been running for $t &quot; . ($t &gt; 1) ? &#39;seconds&#39; : &#39;second&#39; .&quot;n&quot;;
</code></pre>
<p>The output would look something like what follows:</p>
<blockquote>
<p>Starting test_script.pl...
This script has been running for 1 second...
This script has been running for 2 seconds...
This script has been running for 6 seconds...</p>
</blockquote>
<p>This would help you realize that you clearly have an issue with doSomething-&gt;module3(), don&#39;t you think?</p>
<p>There are modules and other functions you can use to benchmark your scripts, but most of those are perl based. In my opinion, that means that they may not be accurate, due to the fact that the code itself is timing itself. It may not include the time it took to actually run the benchmarking functions/modules you are using. However, the <strong>$^T</strong> variable is a system variable being accessed through perl, so it should be much more accurate.</p>
<p>####7) Writing To A File - A Better Way?
When you want to write to a file in perl, typically you just open a file with a file handle, then print any information to that file handle itself, heres a short example:</p>
<pre><code class="lang-perl">open (MYFILE, &#39;&gt;&gt;&#39;.$file);
print MYFILE &quot;Hello Worldn&quot;;
close (MYFILE);
</code></pre>
<p>But what if you want to put everything that gets sent to STDOUT to a file? This is useful for if your script is generating errors, which you&#39;re having a hard time catching to read, and they aren&#39;t getting sent to a file. You can simply open STDOUT as the filehandle, then instead of specifying a file, you specify the tee command to append anything that goes to STDOUT to a file. Example:</p>
<pre><code class="lang-perl">open (STDOUT, &quot;| sudo tee -a -i $file&quot;)
|| die &quot;Failed to open $file for writing: $!n&quot;;

print &quot;Now anything in STDOUT will be send to $filen&quot;;
print Dumper(%ENV);
print &quot;Pretty useful, if you as me!n&quot;;

close (STDOUT);
</code></pre>
<p>This will append anything that gets sent to STDOUT to the value of $file.</p>
<p>This is very useful for cron jobs using perl, if you use the typical method, then you have to specifically specify what to append to $file, but using this method, anything that gets displayed, will get appended to $file, until you close the filehandle. If your script is generating an error... this will catch it.</p>
<p>Another reason I think this is better than the typical method, is if you want to both print your data to a file, as well as display it in STDOUT for the user, you would have to print to the filehandle AND print it a second time so it gets sent to STDOUT, while using the normal method. But with this trick, it gets sent to the file and to STDOUT at the same time.</p>
<p>####8) Split - A Common Misconception
Most of the articles/tutorials regarding the split command in perl, show you to use back slashes as the means to specify what you wish to split a string by and store it into a variable. However I find that that may cause issues for some developers code, I know it did in mine.</p>
<p>Heres a small example of how to use the split function, splitting a string with a few letters in it, and storing it into an array, then taking a look at the results..</p>
<pre><code class="lang-perl">my @array;
my $string = &quot; a b c &quot;;

@array = split( / /, $string );

print &quot;Split with backslash:n&quot;;
print Dumper(@array);
print &quot;n&quot;;

@array = split( &#39; &#39;, $string );

print &quot;Split with quotes:n&quot;;
print Dumper(@array);
print &quot;n&quot;;
</code></pre>
<p>So now the only difference, is that one split is using the single quote to split, and the other is using the back slashes. Here is the output</p>
<pre><code class="lang-bash">$ perl split_with_quote_and_slash.pl
Split with backslash:
$VAR1 = &#39;&#39;;
$VAR2 = &#39;a&#39;;
$VAR3 = &#39;b&#39;;
$VAR4 = &#39;c&#39;;

Split with quotes:
$VAR1 = &#39;a&#39;;
$VAR2 = &#39;b&#39;;
$VAR3 = &#39;c&#39;;
</code></pre>
<p>As you can tell by the output, the back slash will include any empty results, meaning the space in the beginning of the $string variable. Keep in mind, if you use chomp, then that would also trim the space off the front of the variable $string.</p>
<p>####9) Alternative To Using Push To Add A New Value To An Array
So theres nothing wrong with using..</p>
<pre><code class="lang-perl">push @array, &quot;New Value&quot;;
</code></pre>
<p>But alternatively, you could use..</p>
<pre><code class="lang-perl">$array[@array] = &#39;New Value&#39;;
</code></pre>
<p>####10) Using The Diagnostics Module
Almost everyone uses the warnings module, or is at least aware of it. You can either execute your perl switch with the <strong>-w</strong> switch, or you can add &quot;<strong>use warnings;</strong>&quot; to the top of your script so your script uses the warnings module every time it&#39;s executed.
But what about the diagnostics module? Not many people are aware of this. The warnings module will point you in the right direction and get you using best practices, but the diagnostics module will help you much more.
Heres an example script.</p>
<pre><code class="lang-perl">#!/usr/bin/perl
use warnings;
use strict;
#use diagnostics;

my @stuff = qw(1 2 3 4 5 6 7 8 9 10);
print &quot;@stuffn&quot; unless $stuff[10] == 5;
</code></pre>
<p>The output of the above, with the diagnostics module commented out, would be...</p>
<p>Use of uninitialized value $stuff[10] in numeric eq (==) at diagnostics.pl line 7.
1 2 3 4 5 6 7 8 9 10</p>
<p>Now if we uncomment the &quot;use diagnostics;&quot; line, you can see a version, you can see all of the diagnostic output.
Use of uninitialized value $stuff[10] in numeric eq (==) at diagnostics.pl line
7 (#1)
(W uninitialized) An undefined value was used as if it were already
defined. It was interpreted as a &quot;&quot; or a 0, but maybe it was a mistake.
To suppress this warning assign a defined value to your variables.
To help you figure out what was undefined, perl will try to tell you the
name of the variable (if any) that was undefined. In some cases it cannot
do this, so it also tells you what operation you used the undefined value
in. Note, however, that perl optimizes your program and the operation
displayed in the warning may not necessarily appear literally in your
program. For example, &quot;that $foo&quot; is usually optimized into &quot;that &quot;
. $foo, and the warning will refer to the concatenation (.) operator,
even though there is no . in your program.
1 2 3 4 5 6 7 8 9 10
This can be a HUGE aid in debugging longer scripts that you&#39;re having issues with.</p>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[Viewing Bash Exit Status Codes With Pipes]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/viewing-bash-exit-status-codes-with-pipes/</link>
      <guid isPermaLink="true">http://localhost:3333/viewing-bash-exit-status-codes-with-pipes/</guid>
      <category><![CDATA[bash]]></category>
      <category><![CDATA[tee]]></category>
      <category><![CDATA[pipe]]></category>
      <category><![CDATA[exit]]></category>
      <category><![CDATA[codestee]]></category>
      <category><![CDATA[exit]]></category>
      <category><![CDATA[code]]></category>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Mon, 25 Jun 2012 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>Recently I was executing bash scripts from within bash scripts, and executing commands based off of exit code, typically I just use</p>
<pre><code class="lang-bash">/bin/bash ./script.sh || echo &quot;script.sh failed&quot;
</code></pre>
<p>or just the typical</p>
<pre><code class="lang-bash">/bin/bash ./script.sh
if [ $? -ne 0 ]; then echo &quot;script.sh failed&quot;; fi
</code></pre>
<p>But now what if you are also piping the output of <em>script.sh</em> through <em>tee</em>? Then you will see that $? is actually the exit code of the tee command.</p>
<p>I found a nifty bash variable/array, $PIPESTATUS. This is an array that contains the exit statuses of all of the exit codes ran by the last command.</p>
<p>By default, if you just echo $PIPESTATUS, you will get the first value, which in this case would be the exit value of /bin/bash ./script.sh.</p>
<p>The exit code of the scripts/commands are placed into the $PIPESTATUS array in the same order that they are executed. Heres an example of how to properly access the exit codes:</p>
<pre><code class="lang-bash">who | wc -l | foo

if [ ${PIPESTATUS[0]} -ne &quot;0&quot; ]; then
 echo &quot;The &#39;who&#39; command failed&quot;
elif [ ${PIPESTATUS[1]} -ne &quot;0&quot; ]; then
 echo &quot;The &#39;wc -l&#39; command failed&quot;
elif [ ${PIPESTATUS[2]} -ne &quot;0&quot; ]; then
 echo &quot;The &#39;foo&#39; command failed&quot;
else
 echo &quot;it worked!&quot;
fi
</code></pre>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[VMware and Hot Adding Hard Disks to a Running CentOS or RHEL Linux System]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/vmware-and-hot-adding-hard-disks-to-a-running-centos-or-rhel-linux-system/</link>
      <guid isPermaLink="true">http://localhost:3333/vmware-and-hot-adding-hard-disks-to-a-running-centos-or-rhel-linux-system/</guid>
      <category><![CDATA[centos]]></category>
      <category><![CDATA[LVM]]></category>
      <category><![CDATA[RHEL]]></category>
      <category><![CDATA[VMware]]></category>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Thu, 21 Jun 2012 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>Its possible to extend your disk space on your running virtual machine without even needing to reboot! There are a few caveats obviously, you need to be using LVM (if you are not, SHAME ON YOU!) and be comfortable with file systems and VMware.</p>
<ul>
<li>First add the new Hard Disk in the virtual machine settings.</li>
<li><p><em>This can be done on the command line on the ESX server, but in this case I am not trying to extend the primary hard disk but just add another one.</em></p>
</li>
<li><p>Using this script (It was written by someone named Kurt Garloff) <a href="http://www.linuxdigest.org/wp-content/uploads/2012/06/rescan-scsi-bus.sh_.txt">rescan-scsi-bus.sh</a></p>
</li>
</ul>
<pre><code class="lang-bash"># ./rescan-sci-bus.sh
</code></pre>
<p>After executing run dmesg and you should see the new device added, pretty need huh?
<img src="/assets/articles/rescan-sci-bus-1.png" alt="Rescan Example 1"></p>
<p>Once the disk has been added you can execute fdisk to display the new device
<img src="/assets/articles/rescan-sci-bus-2.png" alt="Rescan Example 2"></p>
<p>Using fdisk create a partition, I&#39;m going to asume you&#39;ll want to use the entire disk but if desired you could create multiple partitions on the new disk. Here we will just be using the entire disk</p>
<pre><code class="lang-bash"># fdisk /dev/sdb
</code></pre>
<ul>
<li>Type n for new partition</li>
<li>p for primary</li>
<li>Just hit enter again to use the default last cylinder since we want to use the entire disk in this case</li>
<li>Finally enter w and enter to write the partition table to disk and sync the disk</li>
</ul>
<p><img src="/assets/articles/rescan-sci-bus-3.png" alt="Rescan Example 3"></p>
<p>After adding the new partition we have to change the partition type to LVM vs 83  Linux (regular Linux partition type)</p>
<pre><code class="lang-bash"># fdisk /dev/sdb
</code></pre>
<ul>
<li>t (change partition type)</li>
<li>8e (for Linux LVM)</li>
<li>And finally w and enter to write changes to disk</li>
<li>Now that we have a valid LVM partition we need to Initialize the Physical Volume</li>
</ul>
<pre><code class="lang-bash"># pvcreate /dev/sdb1
</code></pre>
<ul>
<li>Here using</li>
</ul>
<pre><code class="lang-bash"># vgdisplay
</code></pre>
<p>You can see the VG size prior to adding the new disk <img src="/assets/articles/rescan-sci-bus-4.png" alt="Rescan Example 4"></p>
<p>Once you have initialized the physical volume you can add it to the volume group that you wish. In this can I want to extend the root volume and I have named it vg00.</p>
<pre><code class="lang-bash"># vgextend vg00 /dev/sdb1
</code></pre>
<p><img src="/assets/articles/rescan-sci-bus-5.png" alt="Rescan Example 5"></p>
<p>Once you have added it to the volume group, you should see the newly available free space.</p>
<pre><code class="lang-bash"># vgdisplay
</code></pre>
<p><img src="/assets/articles/rescan-sci-bus-6.png" alt="Rescan Example 6"></p>
<p>You can see on the Free PE / Size line that I have 29.97 GB of free space, now lets use it!
What we have to do now is actually extend the logical volume (vg00) so that it consumes the rest of the available space.</p>
<p><em>Tip:</em> You can find out the path to your volume group with the df command and vgdisplay</p>
<pre><code class="lang-bash"># vgextend -L +29G /dev/vg00/rootvol
</code></pre>
<p><img src="/assets/articles/rescan-sci-bus-7.png" alt="Rescan Example 7"></p>
<p>Now that we hav extended the volume we have to extend the file system so the available space can be written to by the system. We do this by using the resize2fs command. Now because I am simply using ext3 this is the command for me, but if you were using a different file system such as xfs you need to use the tool available for that FS (_xfs<em>grow for example</em>)</p>
<pre><code class="lang-bash"># resize2fs /dev/mapper/vg00-rootvol
</code></pre>
<p><img src="/assets/articles/rescan-sci-bus-8.png" alt="Rescan Example 8"></p>
<p>And thats it! Checkout df to see your newly available disk space.</p>
<p>Check back again for more articles on LVM here at GeekWiki, I will be writing another one on LVM snapshots and choosing the best file system for your needs.</p>
<p>Also a side note, if you are using SAN backed storage like I am (In this case Netapp). You must make sure you align your VMDK before adding it to your vgroup. I will also be writing an article on how to do this and why its important for performance.</p>
<p>Thanks for reading!</p>
<p>Kyle</p>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[How to quickly and efficiently delete all data in a LARGE MySQL table using TRUNCATE]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/how-to-quickly-and-efficiently-delete-all-data-in-a-large-mysql-table-using-truncate/</link>
      <guid isPermaLink="true">http://localhost:3333/how-to-quickly-and-efficiently-delete-all-data-in-a-large-mysql-table-using-truncate/</guid>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Thu, 21 Jun 2012 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>If you have a very large table in MySQL and you need to delete all the data from it. Instead of using the DELETE syntax or DELETE FROM tablename; It is best to use the TRUNCATE syntax.</p>
<p>This will delete all data in the table very quickly. In MySQL the table is actually dropped and recreated, hence the speed of the query.</p>
<pre><code class="lang-bash">$ mysql&gt; TRUNCATE TABLE tablename;
</code></pre>
<p>Query OK, 0 rows affected (10.34 sec)[/bash]</p>
<p><strong>Note:</strong> The number of deleted rows for MyISAM tables returned is zero; for INNODB it returns the actual number deleted.</p>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[The Best Way to Install Nagios - Using OMD The Open Monitoring Distribution]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/the-best-way-to-install-nagios-using-omd-the-open-monitoring-distribution/</link>
      <guid isPermaLink="true">http://localhost:3333/the-best-way-to-install-nagios-using-omd-the-open-monitoring-distribution/</guid>
      <category><![CDATA[centos]]></category>
      <category><![CDATA[Check_MK]]></category>
      <category><![CDATA[Nagios]]></category>
      <category><![CDATA[OMD]]></category>
      <category><![CDATA[RHEL]]></category>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Thu, 21 Jun 2012 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>Without a doubt the best OpenSource monitoring software out there is Nagios. Now I&#39;m sure this is a debatable topic and some favor other options out there such as Zabbix. But if your a true Linux head, and loath any type of point and click graphical user interface then Zabbix would automatically be out of the question.</p>
<p>However, with the power of Nagios also comes the learning curve. Even though the documentation for Nagios is extreamly thorough it can be a bit daunting for a newbie to get it up and running. Even just to monitor a few hosts. But with this article I aim to change that perception.</p>
<p>Meet <strong>OMD</strong></p>
<p>Literally the Best way to install Nagios - Using OMD The Open Monitoring Distribution</p>
<p><a href="http://omdistro.org/">http://omdistro.org/</a></p>
<p>OMD as the name implies is an Open Monitoring Distribution. Its not actually an entire Linux distribution but just a pre-packaged and easy to install monitoring platform that utilizes Nagios Core and many of the bells and whistles that people would like to install to get the most out of Nagios. Best of all you can run multiple instances (or sites as OMD refers them) on a single server. This would allow you to run a dev and a prod instance on the same machine for testing configuration changes on one before pushing them to production. OMD also allows for easy backups, as well as copying and mv&#39;ing sites. Upgrades are also a breeze, simply installing the latest version of OMD and running an omd upgrade on the desired site will do all the hard work for you. And boom your done. You could even try upgrading dev first to make sure it doesn&#39;t break anything on your production site first.</p>
<p>From their site:</p>
<blockquote>
<p>OMD avoids the tedious work of manually compiling and integrating Nagios addons while at the same time avoiding the problems of pre-packaged installations coming with your Linux distribution, which are most times outdated and provide no regular updates.</p>
<p>OMD bundles Nagios together with many important addons and can easily be installed on every major Linux distribution. We provide prebuilt packages for all enterprise Linux distributions and also for some other, such as Ubuntu 11.04.</p>
</blockquote>
<p>Here are a few of the things that comes installed with OMD.</p>
<ul>
<li>Nagios<ul>
<li>nagios-plugins</li>
<li>NSCA</li>
<li>check_nrpe</li>
</ul>
</li>
<li>Icinga</li>
<li>Shinken</li>
<li>nagvis</li>
<li><a href="http://omdistro.org/wiki/omd/Pnp4nagios">pnp4nagios</a></li>
<li>rrdtool/rrdcached</li>
<li>Check_MK</li>
<li>MK Livestatus</li>
<li>Multisite</li>
<li><a href="http://omdistro.org/wiki/omd/Dokuwiki">dokuwiki</a></li>
<li>Thruk</li>
<li>Mod-Gearman</li>
<li>check_logfiles</li>
<li>check_oracle_health</li>
<li>check_mysql_health</li>
<li>jmx4perl</li>
<li>check_webinject</li>
<li>check_multi</li>
</ul>
<p>Now the real highlight here, and I will get to it later is Check_MK. Check_MK really answers the age old problem of monitoring remote Linux systems in an easy and efficient way. No longer are the days of lots of Nagios plugins and hours of configuration file setup just to monitor CPU/Mem/Disk/Network etc. Those are the types of things that should be simple to do. Giving us more time to do the more in-depth monitoring that we really want to get to (if thats your goal).</p>
<p>Since I am a RHEL/CentOS type of guy, this guide will follow installing OMD on a RHEL based system. But OMD also packages .deb files as well for easy install. Where I work we use Satellite/Spacewalk, so I simply upload the OMD RPM to the related channel and use yum to install. But for simplicity sake, we will just do it the old fashion way.</p>
<ul>
<li>Download the .rpm package from OMD&#39;s website <strong>(MAKE SURE YOU GET THE LATEST VERSION!!)</strong> At the time I wrote this article there was 0.54</li>
</ul>
<pre><code class="lang-bash"># wget http://omdistro.org/attachments/download/174/omd-0.54-rh60-28.x86_64.rpm
</code></pre>
<ul>
<li>Install the RPM</li>
</ul>
<pre><code class="lang-bash"># yum localinstall omd-0.54-rh60-28.x86_64.rpm
</code></pre>
<ul>
<li>Now all we need to do is create a monitoring instance, or what OMD calls a site.</li>
</ul>
<pre><code class="lang-bash"># omd create prod
</code></pre>
<ul>
<li>After creating the monitoring instance you can either start it right away and get right to using it. Or you can do a little customization first.</li>
<li>One thing to note, is since OMD creates contained sites for each instance, it runs that site as the user the site is called in our example above &quot;prod&quot; all of the prod instance services are running as the user prod. To admin the site you should always su to that user for configuration.</li>
</ul>
<pre><code class="lang-bash">OMD[prod]:~$ omd config
</code></pre>
<p><img src="/assets/articles/omd_screen_1.png" alt="OMD Screenshot 1"></p>
<ul>
<li>First thing you notice is how professional and clean the interfaces for OMD and check_mk are. Here is an example of the configuration settings for the site prod. In later articles I will go more in depth.</li></li>
<li>After configuration startup your instance. OMD will start up each component for you.</li></li>
</ul>
<p><img src="/assets/articles/omd_screen_2.png" alt="OMD Screenshot 2"></p>
<ul>
<li>I will include a few more screenshots below so you can get an appreciation for some of the options and features available.</li></li>
</ul>
<p><img src="/assets/articles/omd_screen_3.png" alt="OMD Screenshot 3"></p>
<p><img src="/assets/articles/omd_screen_4.png" alt="OMD Screenshot 4"></p>
<ul>
<li>Now you can simple go to the URL or hostname of your server and choose from the various interfaces that OMD comes packaged with.</li>
<li><p>Just take your pick, you can configure the default one using omd config sitename.</p>
</li>
<li><p>Nagios Classic</p>
</li>
<li>MultiSite</li>
<li>Thurk</li>
<li>Icinga</li>
<li>Shinken</li>
<li>Nagvis (although this is not really a dashboard like the others)</li>
</ul>
<p>More articles to come on further configuring and actually getting hosts in and monitored. As well as the benefits and features of check_mk, and scaling OMD/Nagios to monitor large number of hosts and large infrastructures.</p>
<p>Kyle</p>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[Copy your SSH key to multiple servers from a list]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/copy-your-ssh-key-to-multiple-servers-from-a-list/</link>
      <guid isPermaLink="true">http://localhost:3333/copy-your-ssh-key-to-multiple-servers-from-a-list/</guid>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Mon, 18 Jun 2012 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>We&#39;ve all been there, you&#39;re sitting at work and your boss gives you a task and you immediately get frustrated because not only is it one of the most annoying jobs to do but it also will consume a lot of your time. This happened to me at an old company that I worked for, because whenever a new person would get hired we would have to manually copy their SSH keys to all of the servers that we had (hundreds of servers!). If you love to code like I do, it immediately turns into a project for yourself to automate it so you don&#39;t have to deal with it in the future. That is why I created this script. This is just to use as reference (it still works) but I would actually recommend using something called <a href="https://github.com/Ruyk/pssh-copy-id">pssh-copy-id</a> instead of this script now but back when I created this script I didn&#39;t even know it existed.</p>
<p>Also please note, this is definitely not some of my best work.. Don&#39;t hold it against me! :)</p>
<pre><code class="lang-perl">
#!/usr/bin/perl
# *************************************************
# * SSH RSA Copier v1.0 * Created on 1/31/2011 *
# *************************************************
# * Geoff Hatch - geoff.hatch@linux.com           *
# *************************************************
# * This script will use the ssh-copy-id command *
# * to copy your current RSA key to all servers *
# * listed in a file. *
# *************************************************

# Let&#39;s declare the global attributes.
use warnings;
use Expect;
use Term::ReadKey;
use Net::SSH::Expect;

# Run the main function.
&amp;main;

sub main()
{
 $scp = &quot;/usr/bin/ssh-copy-id&quot;;

print &quot;********************************************************************************************************************************\r\n&quot;;
 print &quot;* SSH RSA Copier v1.0 * Created on 1/31/2011 * *\r\n&quot;;
 print &quot;********************************************************************************************************************************\r\n&quot;;
 print &quot;* Directions: *\r\n&quot;;
 print &quot;* *\r\n&quot;;
 print &quot;* Step 1) When prompted enter your username, or press RETURN to use the current username. *\r\n&quot;;
 print &quot;* Step 2) When prompted enter your password, this field is required and the script will not continue if nothing is entered. *\r\n&quot;;
 print &quot;* Step 3) Let the script do it&#39;s thing. *\r\n&quot;;
 print &quot;* *\r\n&quot;;
 print &quot;* Note: If you do not have a id_rsa and id_rsa.pub key in your .ssh directory, the script will automatically generate them *\r\n&quot;;
 print &quot;* for you, and then add your identity via ssh-add. *\r\n&quot;;
 print &quot;********************************************************************************************************************************\r\n&quot;;
 print &quot;* The script will start in 10 seconds, if you did not mean to run this or wish to stop it, press Ctrl + C now. *\r\n&quot;;
 print &quot;********************************************************************************************************************************\r\n\r\n&quot;;
 sleep 10;

print &quot;Downloading recent servers list... &quot;;
 if (-e &quot;./rsa_copier.log&quot;) { system(&quot;rm -rf rsa_copier.log&quot;); }
 # Change this to point to where your server list is
 system(&quot;wget -q -O serverlist.txt &#39;http://mywebsite.com/serverlist.txt&#39;&quot;);
 print &quot; completenr&quot;;
 &amp;get_user_info;

&amp;check_priv_rsa;
 &amp;check_pub_rsa;
 system(&quot;ssh-add&quot;);

open SERVERS, &quot;./serverlist.txt&quot; or die $!;

while (&lt;SERVERS&gt;)
 {
 chomp;
 $server = $_;
 &amp;copyrsa($scp, $user, $pass, $server);
 }
 close SERVERS;
 print &quot;\r\n\r\n&quot;;
 print &quot;********************************************************************************************************************************\r\n&quot;;
 print &quot;* Your RSA key has been pushed to all servers. If you did not have access to any of them you can view the rsa_copier.log *\r\n&quot;;
 print &quot;* file which is located in the same directory as the script. *\r\n&quot;;
 print &quot;* *\r\n&quot;;
 print &quot;* Thank you for using SSH RSA Copier v1.0 *\r\n&quot;;
 print &quot;********************************************************************************************************************************\r\n&quot;;
}

# Check if the user has a private key.
sub check_priv_rsa()
{
 $privrsadir = $ENV{HOME}.&quot;/.ssh/id_rsa&quot;;
 if (-e $privrsadir)
 { &amp;check_pub_rsa; }
 else
 {
 print &quot;No private RSA found. Let&#39;s make one.\r\n&quot;;
 &amp;generate_rsa;
 }
}

# Check if the user has a public key.
sub check_pub_rsa()
{
 $pubrsadir = $ENV{HOME}.&quot;/.ssh/id_rsa.pub&quot;;

if (-e $pubrsadir)
 {
 #&amp;main;
 }
 else
 {
 print &quot;No public RSA found. Let&#39;s make one.&quot;;
 &amp;generate_rsa;
 }
}

# No key found, Let&#39;s generate the key for them
sub generate_rsa()
{
 $scp_exp = new Expect;

$scp_exp-&gt;spawn(&quot;ssh-keygen -t rsa&quot;) or die &quot;Unable to run the command: $!n&quot;;
 $scp_exp-&gt;expect(5, [qr&#39;Enter file in which&#39; , sub {$expp = shift; print $expp &quot;r&quot; ;exp_continue; }],
 [qr&#39;Enter passphrase&#39; , sub {$expp = shift; print $expp &quot;r&quot; ;exp_continue; }],
 [qr&#39;Enter same passphrase&#39; , sub {$expp = shift; print $expp &quot;r&quot; ;exp_continue; }],&#39;-re&#39;, &#39;$&#39;);

$scp_exp-&gt;hard_close();
}

# User Input
sub get_user_info()
{
 # Let&#39;s get the username to use, if line is empty we&#39;ll use the current username.
 print &quot;* Username (blank = use current): &quot;;
 chomp($user = &lt;STDIN&gt;);
 print &quot;\r\n&quot;;

if (!$user) # No username? Let&#39;s use current.
 { $user = $ENV{&#39;LOGNAME&#39;}; }

# Now let&#39;s get the password, but hide the input text.
 print &quot;* Password: &quot;;
 &amp;pass;

sub pass # No password, No continue!
 {
 ReadMode(&#39;noecho&#39;); # Hiding the input text
 chomp($pass = ReadLine(0));
 if (!$pass)
 {
 print &quot;n* Password cannot be blank.n* Password: &quot;;
 &amp;pass;
 }
 }
 ReadMode (&#39;normal&#39;); # Un-hiding the rest of the input text.
 print &quot;\r\n&quot;;
}

# Main function that will copy the RSA key to the server.
sub copyrsa()
{
 $scp_exp = new Expect;

# Run the copy command.
 $scp_exp-&gt;spawn(&quot;$scp $server&quot;) or die &quot;Cannot spawn ssh-copy-id: $!n&quot;;

$scp_exp-&gt;expect(5, [qr&#39;(yes/no)s*&#39; , sub {$exph = shift; print $exph &quot;yesr&quot; ;exp_continue; }],
 [qr&#39;Permission denieds*&#39; , sub {$exph = shift; $error = &quot;Permission denied &quot;; &amp;logerror($error, $server); exp_break; }],
 [qr&#39;word:s*&#39; , sub {$exph = shift; print $exph &quot;$passr&quot;;exp_continue; }],
 [EOF =&gt; sub {$exph = shift; $error = &quot;Error: Could not login, EOF!&quot;; &amp;logerror($error, $server); exp_break; }],
 [timeout =&gt; sub {$exph = shift; $error = &quot;Error: Could not login, timeout!&quot;; &amp;logerror($error, $server); exp_break; }],&#39;-re&#39;, &#39;$&#39;);

$scp_exp-&gt;hard_close();
}

# Let&#39;s log the issues to a file.
sub logerror
{
 open LOGFILE , &quot;&gt;&gt; ./rsa_copier.log&quot; or die $!;
 print LOGFILE &quot;$error: $serve\r\n&quot;;
 close LOGFILE;
}

# End
</code></pre>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[Common Misconceptions About Linux]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/common-misconceptions-about-linux/</link>
      <guid isPermaLink="true">http://localhost:3333/common-misconceptions-about-linux/</guid>
      <category><![CDATA[centos]]></category>
      <category><![CDATA[grub]]></category>
      <category><![CDATA[Install]]></category>
      <category><![CDATA[Linux]]></category>
      <category><![CDATA[redhat]]></category>
      <category><![CDATA[satellite]]></category>
      <category><![CDATA[spacewalk]]></category>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Fri, 15 Jun 2012 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>Before I started learning Linux, I was very hesitant to start because of how intimidated I was from what I had heard. Now that I have much more experience, I have the ability to refute most of these misconceptions.</p>
<p>None of these are in any specific order at all.</p>
<p>1) Most software isn&#39;t compatible with Linux.</p>
<p>This is kind of a silly excuse not to use Linux. Expecting every Windows Application to work on a Linux Server is like expecting the transmission from your VW Beatle to work in your Camaro SS... It won&#39;t work. However, there are many ways to get Windows applications to work on Linux, for example, you can use the application called <a href="http://www.linuxdigest.org/wp-content/uploads/about">Wine</a>. Wine is a very popular application which allows you to run Windows applications under a Linux OS. It basically tricks the application into thinking its running on Windows by creating the registry and the C:/ drive with the System32 folder and other essentials.</p>
<p>Most of the popular or mainstream applications will usually have different install packages for Windows, OSX and Linux, but sometimes you might find an application that won&#39;t work in Wine and doesn&#39;t have an installable for Linux. The Open-Source community is big on coming with free alternatives for applications that are otherwise expensive and/or incompatible with platforms other than what they were originally designed to work with.</p>
<p>Lets take the Microsoft Office Suite for example. Originally, you were unable to get Microsoft Word, Microsoft Outlook and Microsoft Excel installed on Ubuntu, even using Wine. But you could easily find alternatives for each of these. Instead of Microsoft Word, you could use <a href="http://www.linuxdigest.org/wp-content/uploads/www.openoffice.org">Open Office</a>, instead of Excel, you could use <a href="http://www.linuxdigest.org/wp-content/uploads/calc.html">Calc</a>, instead of Outlook, you could use <a href="http://www.linuxdigest.org/wp-content/uploads/thunderbird">Mozilla Thunderbird</a>. The Linux community works hard making free applications that you can use as alternative to proprietary software, unfortunately, not everything will work.</p>
<p>2) Theres no support for Linux. 
<img src="/assets/articles/linux_support.png" alt="Linux Support">
100% not true! You can get just as much support for Linux as you can for Windows, if not more. Keep in mind, the mentality of the Open-Source type of administrator using a <em>free</em> operating system, probably believes in <em>free</em> support as well, wouldn&#39;t you think? Every distribution has their own support forum where members can jump on and help support you for free, or other forums that aren&#39;t associated with your specific distribution such as www.linuxforums.org, www.linuxquestions.com, www.linuxforum.com, and many more if you just google &quot;Linux Support&quot;.</p>
<p>If you choose to take the paid support route, thats perfectly fine as well, sites such as <a href="http://www.linuxdigest.org/wp-content/uploads/www.redhat.com">RedHat</a> or <a href="http://www.linuxdigest.org/wp-content/uploads/www.canonical.com">Canonical</a> provide paid support for Linux distributions.</p>
<p>3) No one uses it.
This one actually kind of upsets me, anyone saying this or using it as an excuse not to learn Linux obviously has done little to no research. I guess to dig into this a little more, I should split it up into Linux being used by desktop users and Linux being used for servers.</p>
<p><strong>Linux as a Desktop at Home</strong>
I suppose I can  see why some people would believe this. For a while, you didn&#39;t really have an option to get a computer with Linux installed, you had to either get it with Windows or nothing at all, and install whatever flavor of Linux you desired, and depending on the laptop and manufacturer, you may have just voided the warranty. Because of this, not many people installed Linux for home use, just the tech savvy nerds (or <em>Linux Ninjas</em> ;-)), who knew what they were getting into.</p>
<p>However, much has changed in the recent few years. Installing Linux won&#39;t void your warranty, and some manufacturers even give you the option to get the laptop/desktop with a popular desktop version of Linux already installed, <a href="https://www.ubuntu.com/">Ubuntu</a>. This means you are getting a laptop or desktop with the guarantee that Ubuntu will work with the hardware provided, using the latest drivers.</p>
<p><strong>Linux as a Desktop at Work</strong>
Back when I started doing technical support, in my young teens, someone using Linux as a desktop at work was completely unheard of. The Windows SysAdmins used Windows Desktops, the Linux SysAdmins used Windows Desktops with <a href="http://www.linuxdigest.org/wp-content/uploads/putty">PuTTY</a> to manage Linux/Unix servers, and the developers or designers all used Mac OSX, but using any flavor of Linux to support a Windows or Linux environment was out of the question, regardless of how experienced you were or if you were able to support yourself.</p>
<p>This too has changed. In every tech company I have been contracted to or worked at, I was able to use whatever OS I felt would best give me the ability to support the environment. Meaning if I was supporting Microsoft servers, I would use a Windows desktop, if I was supporting a Linux environment, then I would use Ubuntu or whatever other flavor I preferred.</p>
<p>The support of Linux Workstations in the corporate environment is pretty limited, which seems to be fine. The type of person who would rather use Linux than Windows every day at work and/or home, is typically the type of person who has the mentality that they can support themselves without the aid of a support department.</p>
<p><strong>Linux as a Web Server</strong>
<img src="/assets/articles/wsweb1.gif" alt="Linux as a Web Server">
Linux&#39;s most popular application, bu far, is the Apache web server, which is the #1 most used web service in the world. Compared to Microsofts IIS, Apache is cheaper, easier, more secure and much more stable.</p>
<p>I&#39;m not a big fan of throwing up graphs from other websites, especially since I can&#39;t guarantee the reliability of the research behind it, but heres a few that shows reports of which Web Servers are used the most, and what years they were taken: <a href="http://www.search-this.com/2007/06/27/microsoft-iis-vs-apache-who-serves-more/">2007 - search-this.com</a>, <a href="http://royal.pingdom.com/2008/03/18/apache-dominates-the-top-100-websites-iis-still-far-behind/">2008 - Pingdom</a>, <a href="http://www.linuxdigest.org/wp-content/uploads/30">2009 - Netcraft</a>, <a href="http://www.linuxdigest.org/wp-content/uploads/apache-web-server-hit-a-home-run-in-2010">2010 - pingdom.com</a>, <a href="http://www.linuxdigest.org/wp-content/uploads/microsoft-iis-web-server-market-share-loss">2011 - pingdom.com</a>.</p>
<p>Now if you take a look at the first one I listed, by search-this.com taken in  2007, you will see that Microsoft/IIS is the most popular choice for Fortune 500 companies. At first, this seemed a little off to me, but when you scroll down and actually read the article, it will make much more sense. None of the Fortune 500 companies are Internet based companies, like Facebook, Google, Yahoo, etc etc, (At least not when that article was published).</p>
<p>Companies that are not Internet based web companies don&#39;t really need to have all of the features that Apache has and IIS doesn&#39;t. Lets take <strong>Exxon Mobil</strong> as an example. It&#39;s a Fortune 500 company, but what does the website <em>www.exxonmobil.com</em> itself do? Nothing really, it&#39;s not a search engine or a social networking website, Exxons products aren&#39;t sold online. The website probably doesn&#39;t get enough traffic to actually need the extensive features of Linux/Apache, they can get by using Microsoft/IIS, and not having to hire any Linux Administrators.</p>
<p>Previously, I worked at a Windows based hosting company, which used IIS as it&#39;s primary HTTP provider, and currently I work at a company that has just two Microsoft/IIS servers used for the billing service, (We got suckered into using Metranet, which apparently requires Microsoft Server 2003/IIS). Both companies  require the Windows servers to be rebooted every so often to release some of the allocated resources or some other reasons. This blew me away... How is this not seen as a problem?! I just never understood the mentality behind the SysAdmins who managed these servers. To them, it was &quot;typical&quot; to have to reboot the servers every week or two weeks, to me, this is like having to pull over and turn your car off then back on every 10 miles or so. On the Linux systems at my current company, if you login and look at the uptime, and it&#39;s <em>under</em> a certain time, we try to figure out who rebooted it and why, but if its a Windows server and we look at the uptime, and its <em>over</em> a certain amount of time, we start to wonder why someone hasn&#39;t rebooted it lately, and a reboot gets scheduled. If you&#39;re running an internet based company, uptime is your #1 priority, would you want a web server that needed to be rebooted like that? Nope!</p>
<p>4) It isn&#39;t optimal for the work environment.
This really depends on who is using it and why. If you&#39;re a secretary of the CEO of PetsMart, you&#39;re probably better off using Microsoft Windows, perfect since hes probably using it as well, and its just easier for when you&#39;re scheduling his meetings and making PowerPoint presentations.</p>
<p>But if you&#39;re talking about the technical aspect of the company, you should pick your primary OS based off of what you are managing. If you are doing technical support for a call center, you probably still want to use Windows, if you&#39;re managing a Microsoft/IIS based web server, you should again be on a Windows computer. Now if you&#39;re managing Linux Servers, or if you&#39;re an Oracle or MySQL DBA, then wouldn&#39;t you think the most appropriate OS to be working on would be the most relevant? I manage 100% Linux servers at work, I was using Ubuntu for a while, until the only way to get better hardware was to get a Mac, which actually seems to be working out great, since OSX is based off of FreeBSD, this works just as well, if not better, than when I was using Ubuntu</p>
<p>5) Linux is difficult, or near impossible, for a non-techy or Windows Admin to use.
Again, this is probably easier explained split between desktop/server distros.</p>
<p><strong>Linux Servers</strong>
One of the main reasons that Linux servers are more stable and able to operate with less overhead than Windows, is the fact that it doesn&#39;t require a Graphical User Interface to be managed. You are able to install the X Window System, and then install Gnome or KDE or some other type of Desktop Environment, then install a service that enables you to remotely manage the server by connecting to it through a Graphical User Interface, much like RDC on Windows. The two most popular options are VNC and NX, (I prefer NX, since it operates over SSH and uses posix users, which is much more secure).</p>
<p>Microsoft has focused on making the management of Windows Servers very &quot;Point and Click&quot;, meaning you have a good chance of being able to find out how to work an application simply by looking around in the interface and the menus of the application, and taking a couple guesses. So when a Windows administrator attempts to perform a task on a Linux server that doesn&#39;t have a GUI installed (which is about 99% of the time), they are going to have no idea how to perform a single task. When managing a Linux server, you actually have to know what you&#39;re doing, you can&#39;t blindly log into the server and click around hoping to get it right, theres no &quot;Next, Next, Next, Next&quot; buttons, just the command line and the &quot;man&quot; command to look at the manual for said command(s). This is typically why people think its &quot;impossible&quot; to manage Linux Servers.</p>
<p><strong>Linux Desktops</strong>
Now the problem here, is the impression of difficulty of managing a Linux Server seems to have carried over to the Linux Desktop, possibly because both are &quot;Linux&quot;? I guess since a Windows Server is just as easy to manage as a Windows Desktop, people assume a Linux Desktop is just as difficult to manage as a Linux Server, totally wrong!</p>
<p><img src="/assets/articles/gnome_control_center2.png" alt="gnome_control_center2"></p>
<p>Ubuntu, the most popular version of a Linux Desktop, has come a very long way. I use Ubuntu at home, and besides when I have to use the terminal to restart services for development purposes, I never use the terminal, and I see no reason to do so. Nearly everything works fine, out of the box, and is very simple to understand.</p>
<p>Ubuntu has accomplished a very Windows-like &quot;Point and Click&quot; interface for the common home use computer. You even have the ability to select from multiple themes, most of which resemble the Windows Desktop GUI.  You get the task bar, the start menu, the window frame with a max/min/exit buttons, all programs menu, etc.</p>
<p>Personally, I prefer using a different distribution of Linux Desktop, Linux Mint. Very similar to Ubuntu, Mint just seems to work a little better, it has some extra drivers pre-installed and some extra features. <a href="http://www.youtube.com/watch?v=0z_IIq2su2w">Heres a small youtube review</a></p>
<p>6) You have to know how to code to use Linux, for either a Desktop or Server distribution.
Unless you are doing some type of custom development, or writing some of your own automation tasks, theres absolutely no reason to know how to code if you are just using a desktop version of Linux, such as Ubuntu or Mint in #5 above. Most of the system files are written in Bash, Perl, Python or C, since these distributions are Open Source, you have the ability to edit these as you please to customize your desktop, but again.... You don&#39;t NEED to know how to code.</p>
<p>But do you NEED to know how to code to manage a Linux <em>Server</em>? Technically, no. You can install the X Window System, and then install Gnome or KDE or some other type of Desktop Environment, or install a web-based control panel such as cPanel/WHM or Webmin, and manage the server from there just fine. But dont expect to get a job managing a Linux environment without the ability to automate some tasks by scripting them out.</p>
<p>Lets face it, any SysAdmins should be able to code. Linux Administrators should learn some languages such as Perl, Python or Bash, and Windows Administrators should learn Powershell or VB. It has nothing to do with the OS itself, good System Administrators should know how to code! Why? If you&#39;re managing a Linux server, and someone tells you to perform a task such as killing the process &#39;/usr/bin/perl /opt/scripts/someScript.pl&quot;, how do you do it? Typically you just run <em>ps aux | grep someScript.pl</em>, get the process ID, then run <em>kill -9 123</em>.</p>
<p>But what if you&#39;re given a file called servers.list, which contains 400 servers, and you&#39;re told to run it on every one of those servers, right now. What do you do then? You might be expected to write a script to complete this task. The same goes for Windows administrators, the most useful Windows SysAdmins can accomplish the same tasks that can be done in the GUI via WMI or Powershell scripts.</p>
<p><img src="/assets/articles/RHN.png" alt="RHN"></p>
<p>7) Linux is difficult to maintain and keep updated.
I&#39;ve heard this one a few times before, but I dont get how it would be any harder to keep up to date than any other OS out there.</p>
<p>If you&#39;re just using a single desktop, you get alerts about updates that need to be installed, you can easily select to ignore the updates, postpone them, or select which updates to install and which ones to ignore.</p>
<p>If you&#39;re managing a full network of servers, there are plenty of utilities you can use to manage the updates on all the servers at once. For the CentOS distro, you can use a free utility called Spacewalk, if you&#39;re using RedHat servers, you can use RedHat Satellite, which is the exact same thing as Spacewalk, only with commercial support from RedHat, so it comes with a nice hefty price tag.</p>
<p>Both RedHat Satellite and Spacewalk give you the ability to manage updates, errata, configuration files and much more on an entire network of servers, just as easily as it is to manage a single server.</p>
<p>8) You can&#39;t run games on Linux.
It really depends on what game you are trying to run, and the restrictions of the game itself. Theres usually a way to get any game working on Linux, even if it&#39;s not supposed to work on Linux or isn&#39;t supported, a lot of games can be ran via Wine, which is covered in #1.</p>
<p>I can&#39;t exactly testify to every game out there and say whether or not it will work on Linux or not, or if it needs Wine or not. I know that you can install <strong>Cedega</strong>, which is a proprietary fork of Wine that provides the ability to install multiple games with little to no effort, however I haven&#39;t personally used it. The only games I have used was anything powered by Steam, and Battlefield 2, all of the Steam games and Battlefield 2 worked perfectly fine. Every now and then when you closed out of the game, the resolution would be a little messed up, but all you need to do is restart X.</p>
<p>One of the biggest reasons, specific to gaming, would be the infamous World Of Warcraft. There was a rumor going around that it wasn&#39;t possible to run WoW on Linux via Wine.. Well guess what, GOOGLE IT! It is totally possible, <a href="http://www.youtube.com/watch?v=DIzgUq4Lpa4">heres a Youtube vide on how to do it</a>. However, there is a slight problem with it. WoW bans any account it perceives as a &quot;Bot&quot;, now one way it determines if it is a bot or not, is by the Operating System. The most popular bots run on Linux, so when WoW sees that your account is running on an OS other than Windows, theres a good chance they will shut down and purge your account, and you will lose all of our precious gold! What will you do then?! You might have to go outside into the sunlight.</p>
<p>9) Too many hardware constraints and limitations.
Theres no more hardware constraints on a Linux Server than a Windows Server, the key is to do research before you build your box.</p>
<p>Every distribution will have a list of hardware requirements on the website, heres the requirements for Ubuntu, and RedHat. All you really need to do is google the name of your distribution + &quot;Hardware Requirements&quot;. Along with the hardware requirements, the website should contain a blacklist of hardware, which contains hardware that is supported, hardware that will not work and is unsupported, and hardware that may work but is unsupported.</p>
<p>10) Linux is difficult to install, and you have to remove Windows/Mac to use it.
Actually, the latest installation process of Ubuntu seems to be much more user-friendly than any install of Windows. Windows seems to be greedy and want to take up the entire harddrive for itself.</p>
<p><img src="/assets/articles/ubuntu-install-partitions.png" alt="ubuntu-install-partitions"></p>
<p>Ubuntu on the other hand is much more polite, it&#39;s as easy as the Windows install, where you can continuously click &quot;Next&quot; and get everything installed by default. If you have another Operating System installed on your harddrive, and you want to keep it, you can select to keep it during the Ubuntu install. Ubuntu will then setup Grub for you s you can select what OS ou wish to boot into at boot time.</p>
<p>If you&#39;re installing a Server version of Linux, the installation might be a little more difficult, but not much more at all. Theres still the option to install via the Graphical User Interface, and simply select what applications you wish to have installed for the purpose of the server. Heres a <a href="http://www.youtube.com/watch?v=fjEfO_cMkXQ">YouTube video</a> I found showing you how to do a basic RedHat OS install.</p>
<p>11) You have to know how to use the Command Line Interface to use it.
This isn&#39;t true in the least bit, especially for desktop versions, there wouldn&#39;t be much point of a desktop of there wasn&#39;t a GUI for it. Desktop distributions of Linux usually come with the option to install something like <a href="http://www.kde.org/">KDE</a> or <a href="http://www.gnome.org/">GNOME</a> or any other interface you choose.
Regarding Servers, you can setup X and manage it via a desktop just as if it was a desktop distribution, however, it&#39;s highly advised against, due to security and resource issues.
There are other routes you can take to manage a Linux server without installing X, and having limited knowledge of the command line.</p>
<p>One of them called <a href="http://www.parallels.com/products/plesk/">Plesk, by Parallels</a>. I can&#39;t really give a solid opinion on this, since I haven&#39;t ever actually used it, but I know it&#39;s one of the more popular &quot;solutions&quot;.</p>
<p>Another route would be <a href="http://www.directadmin.com/install.html">DirectAdmin</a>. This is an open source application used to manage Linux servers. A lot of the cheaper hosting providers will install this for you for free if you request. It&#39;s not nearly as powerful or secure as any of the paid solutions, but it gets the job done, sometimes... I have very limited experience with DirectAdmin, and thats because it ended up breaking the configurations it was managing more than anything. I would definitely <strong>not</strong> recommend this product, not that I would recommend using any web based System Management application.</p>
<p><img src="/assets/articles/cpanel-whm.png" alt="cpanel-whm"></p>
<p>The most popular that comes to mind, would be <a href="http://cpanel.net/">WHM/cPanel</a>. If you get a Dedicated server or a VPS from a hosting provider, theres a good chance that they will offer this as part of the package. Though I am against any of these management applications, I will say this is the lesser of the evils. There are security holes (Just like any other application does), but they are patched relatively quickly, and the support is actually very efficient. I used cPanel to manage my first Linux server, and I feel safe recommending this for anyone looking for a web based solution.</p>
<p>12) The Hardware Compatibility Is Very Limited.
I don&#39;t believe this is true at all. I&#39;ve been using Linux (Desktop and Server Distros) for most of my career, and I have yet to run into a compatibility issue that held me up for longer than a few minutes. If there&#39;s something brand new, then there may not be a driver out for it yet, but rest assured, someones working n it! Just to be safe though, Google for the compatibility list for whatever hardware you are looking to install.</p>
<p>13) Linux Isn&#39;t Susceptible to Malware or Viruses.
This is more of a myth I would say, Linux is just as susceptible as anything else when it comes to viruses and malware. However, viruses that target Linux are much less common than ones that target Windows. I believe that the reason behind this is because Linux is more commonly used as a server, and less commonly used as a desktop, and most viruses target the desktop users via email attachments or downloads on exploited websites.  It may also be because the average Linux user simply knows how to spot a phishy website or email more efficiently than the average Windows user.</p>
<p>Just as there are viruses, malware, and other vulnerabilities to exploit on both Linux server and desktop distributions, theres firewall software as well, some of the more popular ones are <a href="http://www.clamav.net/">ClamAV</a>, <a href="http://free.avg.com/us-en/download.prd-alf">AVG</a> and <a href="http://howtoubuntu.org/how-to-install-and-update-avast-antivirus-in-ubuntu">Avast</a>. Also, one of my personal favorites would be <a href="http://configserver.com/cp/csf.html">CSF</a>, CSF is actually just a wrapper around iptables, but it also checks for common vulnerabilities in existing applications you have installed, and gives you suggestions on how to remediate said vulnerabilities, it also gives you a &quot;security rating&quot; scale, the higher, the better!</p>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[Why the GOTO Statement Is Evil]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/why-the-goto-statement-is-evil/</link>
      <guid isPermaLink="true">http://localhost:3333/why-the-goto-statement-is-evil/</guid>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Thu, 14 Jun 2012 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p><img src="cpp_goto_statement.jpg" alt="Goto statement"></p>
<p><strong>What is the goto statement?</strong>
 Basically, it&#39;s a way for you to easily &quot;jump&quot; around in your code. You can insert &quot;goto labels&quot; within your code, then simply jump to that segment of your code by just inserting &quot;goto LABELNAME&quot;. In some languages, you don&#39;t need to specify the labels at all, you can simply specify the line number you wish to jump to within the current page. Regardless of the language you are using, you will not find it in any Structured Programming Paradigm, due to the fact goto is considered a &quot;short cut&quot; or a &quot;temporary fix&quot; for a task that can just as easily be accomplished using a subroutine, codeblock, function, or a loop such as for or while.</p>
<p>Back when I started learning Perl, I will admit I had pretty much no idea what I was doing, I just knew that if I wanted to be a good SysAdmin, I would need to learn some type of server side code, other than PHP, I chose Perl.</p>
<p>I was creating the first version of Cronus, without going into too much detail on the project itself, its a command line utility, written in perl, and made to run on Windows. It basically allows you to remotely manage Windows servers using the Win32::OLE module, so you could quickly restart services, see whats killing the server, whos logged in, reboot it, etc etc.</p>
<p>Here would be a basic example of how I constructed Cronus using the goto statement:
<strong>NOTE:</strong> Obviously this isn&#39;t the exact script, this is just an example of how I would have done it.</p>
<pre><code class="lang-perl">#!C:Perlperl.exe

use strict;
use warnings;
use Switch;

goto MAIN;

MAIN:
# Get user input
print &quot;Command&gt; &quot;;
chomp(my $command = &lt;&gt;);

# Switch
switch($command) {
    case &quot;who&quot;      { goto WHO; }
    case &quot;reboot&quot;   { goto REBOOT; }
    case &quot;exit&quot;     { goto EXIT; }
    else            { goto HELP; }
}

# WHO: This returns who is on the server
WHO:
# --- Code to query WMI and see whos taking up all the spots on the server ---
goto MAIN;

# REBOOT: Guess what this does?
REBOOT:
# --- Code to force a shutdown/reboot on the server ---
goto MAIN;

# HELP: For those who just dont know
HELP:
# --- Code to display a help menu with commands ---
goto MAIN;

# Exit!
EXIT:
print &quot;Goodbyen&quot;;
exit;
</code></pre>
<p>Now to any beginner, this looks perfectly fine, however when I showed it to someone who was an experienced SysAdmin and Perl programmer, I received a nice long lecture on why using goto is highly frowned upon in the programming world, and how it should be done correctly, at least in perl, for the sake of this tutorial.</p>
<p>The goto statement doesn&#39;t cause any programming defects within the code itself, meaning it doesn&#39;t cause any problems within the code or how it runs, or prohibit functionality, or make it run slower, or throw errors when it&#39;s compiling. Using goto just makes the code much harder to deal with, it turns your code into &quot;Spaghetti Code&quot;.</p>
<blockquote>
<p>Spaghetti code is a pejorative term for source code that has a complex and tangled control structure, especially one using many GOTOs,
exceptions, threads, or other &quot;unstructured&quot; branching constructs. It is named such because program flow tends to look like a bowl of spaghetti, i.e. twisted and tangled. Spaghetti code can be caused by several factors, including inexperienced programmers and a complex program which has been continuously modified over a long life cycle. Structured programming greatly decreased the incidence of spaghetti code.</p>
</blockquote>
<p>I realize that Wikipedia isn&#39;t a reliable source for quotes, but it&#39;s kinda funny they specifically use GOTO as an example huh? I actually wasn&#39;t able to find any other examples of spaghetti code, every example referred to the goto statement!</p>
<p>So there you have it, goto is just a PITA to work with, especially if you are picking up from someone else&#39;s work, if you think about it, you&#39;re basically giving them the virtual run-around within your script.  A good code structure consists of modules being separated from all other code, and being called upon from the main script.</p>
<p>So, if I were to re-write the Perl example above, and remove all &#39;goto&#39; statements, it would look like this.</p>
<pre><code class="lang-perl">#!C:Perlperl.exe

use strict;
use warnings;
use Switch;

while(1) {
   # Get user input
    print &quot;Command&gt; &quot;;
    chomp(my $command = &lt;&gt;);

    # Switch
    switch($command) {
        case &quot;who&quot;      { WHO(); }
        case &quot;reboot&quot;   { REBOOT(); }
        case &quot;exit&quot;     { EXIT(); }
        else            { HELP(); }
    }
}

#########################################
# SUBROUTINES

sub WHO
{
    # --- Code to query WMI and see whos taking up all the spots on the server ---
}

sub REBOOT
{
    # --- Code to force a shutdown/reboot on the server ---
}

sub HELP
{
    # -- Code to display a help menu with commands ---
}

sub EXIT
{
    print &quot;Goodbyen&quot;;
    exit;
}
</code></pre>
<p>See what I mean? This separates the subroutines from the main code which calls the subroutines, much easier to read, anyone digging into this code or adding to it would know exactly where to look and where to add other subroutines.</p>
<p>Here is a visual representation of why it&#39;s hard to read, if you were to draw arrows as to how you follow the code:
<img src="/assets/articles/09fig09.gif" alt="09fig09.gif"></p>
<p>Get what I mean by a &quot;virtual run-around&quot; now?</p>
<p>As of now, I can&#39;t think of any &quot;valid&quot; reasons to use the GoTo statement, it&#39;s been said thats it&#39;s &quot;acceptable&quot;  in lower level programming languages, but again, I don&#39;t see what you can accomplish with goto that you can&#39;t with some other form of loop or function.</p>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[Automate server backups to Amazon S3]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/automate-server-backups-to-amazon-s3/</link>
      <guid isPermaLink="true">http://localhost:3333/automate-server-backups-to-amazon-s3/</guid>
      <category><![CDATA[aws]]></category>
      <category><![CDATA[s3]]></category>
      <category><![CDATA[backup]]></category>
      <category><![CDATA[automation]]></category>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Mon, 11 Jun 2012 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>I&#39;ve always tried to figure out what the best way would be to backup a server, without me having to do anything at all. I looked online and found a lot of great websites, software that does it but everything is extremely expensive especially for storing your backups. If there is a free method out there, I am one to try and go that route and give it a try to get everything set up properly before I start investing my money into paid products and what is really funny about that is sometimes the free products work WAY better than the paid products.</p>
<p>Anyway, here is how I automated my server backups to Amazon S3 using their free usage tier which provides 5 GB of Amazon S3 storage, 20,000 Get Requests, 2,000 Put Requests, and 15GB of data transfer out each month for one year. Yes, after the year is up you have to pay but even then if you look at the storage pricing it is <a href="https://aws.amazon.com/s3/pricing/">extremely cheap</a></p>
<p>First off you need to install a tool called s3cmd on your server. You can find the s3cmd page by navigating <a href="http://s3tools.org/s3cmd">here</a></p>
<p>Once you get s3cmd installed then you need to configure it using the --configure flag</p>
<pre><code class="lang-bash"># s3cmd --configure
</code></pre>
<p>Then input all of the information that it reqeusts such as access key and secret key (click <a href="https://www.amazon.com/gp/redirect.html?ie=UTF8&amp;location=https%3A%2F%2Faws-portal.amazon.com%2Fgp%2Faws%2Fdeveloper%2Faccount%2Findex.html%2F?action=access-key&amp;tag=bucket-20">HERE</a> to be taken directly to your access keys), your encryption password which is used to password protect your files, https yes/no, http proxy settings, etc. Here is an example output:</p>
<pre><code class="lang-bash"># s3cmd --configure

Enter new values or accept defaults in brackets with Enter.
Refer to user manual for detailed description of all options.

Access key and Secret key are your identifiers for Amazon S3
Access Key []:
Secret Key []:

Encryption password is used to protect your files from reading
by unauthorized persons while in transfer to S3
Encryption password []:
Path to GPG program [/usr/bin/gpg]:

When using secure HTTPS protocol all communication with Amazon S3
servers is protected from 3rd party eavesdropping. This method is
slower than plain HTTP and can\&#39;t be used if you\&#39;re behind a proxy
Use HTTPS protocol [No]:

On some networks all internet access must go through a HTTP proxy.
Try setting it here if you can\&#39;t conect to S3 directly
HTTP Proxy server name:

New settings:
 Access Key: &lt;YOUR ACCESS KEY&gt;;
 Secret Key: &lt;YOUR SECRET KEY&gt;;
 Encryption password: &lt;YOUR PASSWORD&gt;;
 Path to GPG program: /usr/bin/gpg
 Use HTTPS protocol: False
 HTTP Proxy server name:
 HTTP Proxy server port: 0

Test access with supplied credentials? [Y/n] y
Please wait...
Success. Your access key and secret key worked fine

Now verifying that encryption works...
Success. Encryption and decryption worked fine

Save settings? [y/N] y
Configuration saved to &#39;/root/.s3cfg&#39;
</code></pre>
<p>Once you have all of that setup and configured, then you need to create a backup directory, this is the directory that your backups will be placed in. I personally made it /backup and inside that directory create a Archive, Data, and MySQL directories. At the end of it your directory should look like this:</p>
<pre><code class="lang-bash">/backups
/backups/Archive
/backups/Data
/backups/MySQL
</code></pre>
<p>Now make sure you have File::NFSLock and Date::Format perl modules installed on your system if you don&#39;t install them via cpan (cpan -i File::NFSLock/Date::Format) and put the following script on your server:</p>
<pre><code class="lang-perl">#!/usr/bin/perl

use Fcntl qw(LOCK_EX LOCK_NB);
use File::NFSLock;
use Date::Format;

# Include the directories you wish to backup separated by space, like so:
# my $datadirs = &quot;/home /root /var/tools /www&quot;;
my $datadirs = &quot;### INPUT YOUR DIRECTORIES ###&quot;;

# Try to get an exclusive lock on myself.
my $lock = File::NFSLock-&gt;new($0, LOCK_EX|LOCK_NB);
die &quot;$0 is already running!n&quot; unless $lock;

my ( $sec, $min, $hour, $mday, $mon, $year, $wday, $yday, $isdst ) = localtime(time);
$year += 1900;
$mon += 1;
my $datestring = time2str( &quot;%m-%d-%Y&quot;, time );
$logfile=&quot;/var/log/s3backup-$datestring.log&quot;;

open( LOGFILE , &quot;&gt;&gt; $logfile&quot; )
 or die &quot;Can&#39;t open file &#39;$logfile&#39;. $!n&quot;;
select((select(LOGFILE), $| = 1)[0]); # autoflush LOGFILE

sub logh() {
 my $msg = shift(@_);
 my ($sec,$min,$hour,$mday,$mon,$year,$wday,$yday,$isdst) = localtime(time);
 $mon += 1; $wday += 1; $year += 1900;

my @months = qw { Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec };
 my $dts;
 $dts = sprintf(&quot;%04d%3s%02d %02d:%02d:%02d&quot;,
 $year,$months[$mon],$mday,$hour,$min,$sec);
 my $pid = $$;
 printf LOGFILE (&quot;$dts:$pid: $msgn&quot; , @_);
}

sub backupDir {
 &amp;logh(&quot;Creating tarball of directories&quot;);
 $dirfilename=&quot;data-$datestring.tar.gz&quot;;
 if (-e &quot;/backups/$dirfilename&quot;) {
 &amp;logh(&quot;Error: Backup file already exists: /backups/$dirfilename&quot;);
 }
 else {
 my $dirbak=`tar czPvf &quot;/backups/Data/$dirfilename&quot; $datadirs`;
 &amp;logh(&quot;Directories tarball has been created&quot;);
 }

}

sub mysqlBackup {
 my $user =&quot;root&quot;;
 my $password = &quot;### INPUT ROOT MYSQL PASSWORD ###&quot;;
 my $outputdir = &quot;/backups/MySQL&quot;;
 my $mysqldump = &quot;/usr/bin/mysqldump&quot;;
 my $mysql = &quot;/usr/bin/mysql&quot;;

&amp;logh(&quot;Creating MySQL backup dump&quot;);
 $msqlfilename=&quot;mysql-$datestring.tar.gz&quot;;
 if (-e &quot;/backups/$msqlfilename&quot;) {
 &amp;logh(&quot;Error: MySQL backup file already exists: /backups/$dirfilename&quot;);
 }
 else {
 system(&quot;rm -rf $outputdir/*.gz&quot;);
 &amp;logh(&quot;Deleted old backups..&quot;);
 my @dblist = `$mysql -u$user -p$password -e &#39;SHOW DATABASES;&#39; | grep -Ev &#39;(Database|information_schema)&#39;`;
 for $db (@dblist) {
 chomp($db);
 my $execute = `$mysqldump -u $user -p$password $db | gzip &gt; $outputdir/$db.sql.gz`;
 }
 my $mysqlbak=`tar czvf &quot;/backups/Data/$msqlfilename&quot; $outputdir/*.gz`;
 system(&quot;rm -rf $outputdir/*.gz&quot;);
 &amp;logh(&quot;MySQL Backup dump has been created.&quot;);

}
}

sub createOne {
 &amp;logh(&quot;Merging backups into one file&quot;);
 my $filename=&quot;ServerBackup-$datestring.tar.gz&quot;;
 if (-e &quot;/backups/$filename&quot;) {
 &amp;logh(&quot;Error: Backup file already exists: /backups/$filename&quot;);
 }
 else {
 my $arbak=`tar czvf /backups/Archive/$filename /backups/Data/*.gz`;
 system(&quot;rm -rf /backups/MySQL/* /backups/Data/*&quot;);
 &amp;logh(&quot;Merge complete.&quot;);
 }
}

sub syncS3 {
 &amp;logh(&quot;Syncing to S3.. &quot;);
 my $sync=`s3cmd sync --delete-removed /backups/Archive/ s3://### INPUT YOUR BUCKET NAME ### &gt;&gt; $logfile`;
 if ($? == 0) { &amp;logh(&quot;Sync to s3 complete.&quot;); }
}

sub cleanArchive {
 &amp;logh(&quot;Removing backups older than 7 days&quot;);
 system(&quot;find /backups/Archive -type f -mtime +7 -print | xargs rm&quot;);
 &amp;logh(&quot;Delete complete.&quot;);
}

&amp;cleanArchive;

my $filename=&quot;ServerBackup-$datestring.tar.gz&quot;;
if (-e &quot;/backups/Archive/$filename&quot;) {
 &amp;logh(&quot;Error: Backup file already exists: /backups/$filename&quot;);
 exit 1;
}

&amp;backupDir;
&amp;mysqlBackup;
&amp;createOne;
&amp;syncS3;
</code></pre>
<p>In the above script you only need to change 3 values if you wish for your server to keep 7 days of backups, just search and replace the following values:</p>
<pre><code>### INPUT YOUR DIRECTORIES ###
### INPUT YOUR BUCKET NAME ###
### INPUT ROOT MYSQL PASSWORD ###
</code></pre><p>You may also want to look at the sub mysqlBackup function if your MySQL installation is special or if you don&#39;t want to use the root user to login and backup your databases. Once you have done all that, simply test out the script by running it and if all is well then create a cron job to automatically run it every so often. I&#39;ve also been meaning to make a mail function in this script to email me when a backup is completed/created I just haven&#39;t had the time to get around to it.. I&#39;ll update this post when I do so.</p>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[Bash script to create MySQL database and user]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/bash-script-to-create-mysql-database-and-user/</link>
      <guid isPermaLink="true">http://localhost:3333/bash-script-to-create-mysql-database-and-user/</guid>
      <category><![CDATA[bash]]></category>
      <category><![CDATA[mysql]]></category>
      <category><![CDATA[script]]></category>
      <category><![CDATA[database]]></category>
      <category><![CDATA[user]]></category>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Sun, 10 Jun 2012 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>I don&#39;t know about you, but often times I find myself needing to create a MySQL database and I simply forget all of the commands. This is really something that I should remember, but for some strange reason I just never am able to remember it. So like any other admin would do, you script it out to make it easier on yourself. This bash script will allow you to create new databases on the fly quickly and easily. This has been nothing but a time-saver.</p>
<pre><code class="lang-bash">#!/bin/bash

echo -n &quot;Enter the MySQL root password: &quot;
read -s rootpw
echo -n &quot;Enter database name: &quot;
read dbname
echo -n &quot;Enter database username: &quot;
read dbuser
echo -n &quot;Enter database user password: &quot;
read dbpw

db=&quot;create database $dbname;GRANT ALL PRIVILEGES ON $dbname.* TO $dbuser@localhost IDENTIFIED BY &#39;$dbpw&#39;;FLUSH PRIVILEGES;&quot;
mysql -u root -p$rootpw -e &quot;$db&quot;

if [ $? != &quot;0&quot; ]; then
 echo &quot;[Error]: Database creation failed&quot;
 exit 1
else
 echo &quot;------------------------------------------&quot;
 echo &quot; Database has been created successfully &quot;
 echo &quot;------------------------------------------&quot;
 echo &quot; DB Info: &quot;
 echo &quot;&quot;
 echo &quot; DB Name: $dbname&quot;
 echo &quot; DB User: $dbuser&quot;
 echo &quot; DB Pass: $dbpw&quot;
 echo &quot;&quot;
 echo &quot;------------------------------------------&quot;
fi
</code></pre>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[A better nslookup]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/a-better-nslookup/</link>
      <guid isPermaLink="true">http://localhost:3333/a-better-nslookup/</guid>
      <category><![CDATA[nslookup]]></category>
      <category><![CDATA[dig]]></category>
      <category><![CDATA[DNS]]></category>
      <category><![CDATA[nameserver]]></category>
      <category><![CDATA[resolv.conf]]></category>
      <category><![CDATA[networking]]></category>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Fri, 08 Jun 2012 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>If you&#39;re a Systems Administrator, and you use a Mac or any flavor of Linux/Unix for your workstation, chances are you have used the <strong><a href="https://en.wikipedia.org/wiki/Nslookup">nslookup</a></strong> command, typically located at /usr/bin/nslookup. It&#39;s a fairly simple command, used to resolve a hostname to an IP, or lookup the PTR record(s) for a given IP address, very useful in every day situations.</p>
<p>In using this command, I found 2 issues with it that I believe hinder the command in some scenarios. If you&#39;re just doing a simple lookup on one record, that&#39;s no problem. But what if someone gives you a list of hostnames or IP addresses, and tells you to do a lookup on them, and provide the results, or report which ones failed? It gets a little more complicated.</p>
<p>The &quot;issues&quot; I see associated with the command are below.</p>
<pre><code>1. No &quot;simple&quot; or &quot;less verbose&quot; output. Whenever you run the command, it will provide data about the server it is using to do the DNS lookup, as well as the hostname and IP address of the record.
2. No exit codes. Even if it fails to find a record, or it can&#39;t contact the DNS server, it will give an exit code of 0, which is a &quot;Success&quot; exit code.
</code></pre><p>Of course, with a little bit of creativity, you can resolve both of these issues, it&#39;s just a bit of a pain. You can use <strong><a href="https://en.wikipedia.org/wiki/AWK">awk</a></strong> and <strong><a href="https://en.wikipedia.org/wiki/Grep">grep</a></strong> to return just the results of the lookup, or you can use a conditional statement and return your own exit code. But after doing this a few times, you will realize it&#39;s a pain.</p>
<p>I scripted a relatively simple bash script that will fix both of these issues.</p>
<pre><code class="lang-bash">#!/bin/bash

# Make sure a paramater was passed
if [ -n &quot;$1&quot; ]
then
    lookup=$1
else
    exit 1
fi

# Do some regex to see if it&#39;s an IP or Hostname
if [ $(echo $lookup | egrep -o &#39;^[0-9]+.[0-9]+.[0-9]+.[0-9]+&#39;) ]
then
    # Its an IP, lookup the PTR record
    records=$(nslookup $lookup | grep &#39;name = &#39; | awk -F&#39; = &#39; &#39;{print $2}&#39; | sed &#39;s/.$//g&#39; | sort)
else
    # Its a hostname, lookup the A record
    records=$(nslookup $lookup | grep -A1 &#39;Name:&#39; | grep Address | awk -F&#39;: &#39; &#39;{print $2}&#39;)
fi

# Were there any records returned?
if [ -z $records ]
then
    exit 1
else
  echo &quot;$records&quot;
fi
</code></pre>
<p>Basically, here are the steps it follows:</p>
<pre><code>1. Is there a paramater provided? If not, exit 1
2. Does the paramater match the regex for an IP?
3. If it does, then execute lookup and look for the Hostname results, if not, look for  PTR results
4. If records were found, display records and exit 0, if no results, exit 1
</code></pre><p>So like I said, pretty simple, but it works perfect for every day situations. Heres a great example of a typical usage.</p>
<pre><code class="lang-bash">$ for i in justinhyland.com break.com fake-site.omg; do echo -n &quot;$i - &quot;; /bin/bash quick_nslookup.sh $i || echo &quot;No Records Found&quot;; done
justinhyland.com - 108.174.52.211
break.com - 216.69.227.70
fake-site.omg - No Records Found
</code></pre>
<p>As you can see, the results are straight to the point and easy to read, that&#39;s what the managers want right?</p>
<h1 id="p-s-">P.S.</h1>
<p>Depending on your ISP, the results may vary. For example, if you are on COX, if you run nslookup against a domain like &quot;some-fake-site.idk&quot;, obviously it doesn&#39;t exist, therefore it doesn&#39;t resolve, but COX will return the IP 72.215.225.9, which is the Cox &quot;Page Not Found&quot; error you get when a hostname doesn&#39;t resolve. Not much you can do about that except throw in a conditional statement that will match for the IP and return an exit code of 1</p>
]]>
      </content:encoded>
    </item>
    <item>
      <title><![CDATA[Use Perl modules on remote servers.]]></title>
      <description><![CDATA[]]></description>
      <link>http://localhost:3333/use-perl-modules-on-remote-servers/</link>
      <guid isPermaLink="true">http://localhost:3333/use-perl-modules-on-remote-servers/</guid>
      <category><![CDATA[cpan]]></category>
      <category><![CDATA[perl]]></category>
      <category><![CDATA[perl-module]]></category>
      <category><![CDATA[ppm]]></category>
      <category><![CDATA[remote-module]]></category>
      <dc:creator><![CDATA[[object Object]]]></dc:creator>
      <pubDate>Fri, 08 Jun 2012 00:00:00 GMT</pubDate>
      <content:encoded>
        <![CDATA[<p>As a Systems Administrator, mainly for Unix/Linux, you realize how big of a deal automation is. At my work, we have nearly a thousand CentOS/RHEL servers. To execute scripts or commands on a list of servers, you can use a variety of methods.... Pssh, Multiexec (Which Geoff, Kyle and myself made, thank ya very much ;]), Cluster SSH, or even just use Terminator, (I know theres a way to run the same command in all terminator windows).</p>
<p>The problem I had was Perl modules. I write a lot of my own Perl modules, and with that, I update them a lot as well. Now to use the Perl module, it needs to be on the local server. either in the $ENV{&#39;PATH&#39;}, or in the directory the script is being executed itself. So I would write a .pm file, and then just put it on all the servers via either GIT, SVN, or wget it using one of the above tools I listed.</p>
<p>This kinda got to be a pain. I just wanted to update the module, then have it done. with 1000 servers, you run into problems updating them on every server. FW rules, svn conflicts, git conflicts, rsa keys failing, authentication not working, etc etc etc.</p>
<p>Well, I finally found a solution, and it works VERY well. I found a way to keep the Perl module on a remote server, and use it on another, as long as it&#39;s accessible via HTTP(s), then you can use it. You can even use authentication, this way not everyone can open a browser and grab the module content.</p>
<p>So lets get started.</p>
<h3 id="step-1-create-the-module-repository">Step 1) Create The Module Repository</h3>
<p>Now all you have to do is to create a space on a web server for your modules. I recommend just having a publicly accessible SVN repository, so you can just edit your code locally, commit, and they are automatically updated on all servers. Also, its probably a good idea to create a hierarchy for the modules. Maybe later you will want to use different authentication for different modules.</p>
<h3 id="step-2-make-the-module">Step 2) Make The Module</h3>
<p>This is a tutorial on how to access modules from a remote location, NOT a tutorial on how to make modules.</p>
<p>This is a very simple module called FOOBAR.pl. It has one function called check_less, it takes two numbers, and if one number is greater than the other, it returns a fail code:</p>
<pre><code class="lang-perl">package FOOBAR;

use strict;
use warnings;

sub check_less {
 # Initiate the values
 my ($self, $num, $limit) = (@_);

 # Basic sanity checking
 return 0 unless(($self) &amp;&amp; ($limit));

 # If $num is greater than $limit, fail
 return ($num &gt; $limit) ? 0 : 1;
};

1;
</code></pre>
<p>Pretty basic right? If you know Perl at all, then you don&#39;t need much more explication for the above.</p>
<h3 id="step-3-initializing-the-module">Step 3) Initializing The Module</h3>
<p>I guess before you do this, you should test to make sure this can hit the module, just jump into the command line and run a wget or telnet or whatever, just make sure you can hit it.</p>
<p>So lets start. First off, paste this into your perl script...</p>
<pre><code class="lang-perl">BEGIN {
    my $LWP = eval { require LWP::UserAgent; };
    if(!$LWP) {
        print &quot;Perl module LWP::UserAgent is not installed on this server.n&quot;;
        exit 1;
    }
    else {
        use LWP::UserAgent;
    }

    push @INC, sub {
        my $URL = &#39;http://someserver.com/perl_modules/FOOBAR.pm&#39;;
        my $ua = LWP::UserAgent-&gt;new;
        $ua-&gt;timeout(10);
        my $module = $ua-&gt;get($URL);
        if($module-&gt;is_success) {
            my $package = $module-&gt;content;
            open my $fh, &#39;status_line.&quot;n&quot;;
            exit 1;
        }
    }
}
</code></pre>
<p>So, explaining what that does...</p>
<p>The BEGIN code block makes sure that no matter where you put this, it gets ran in the beginning.</p>
<p>Next, is the section that actually initializes the LWP::UserAgent module. If you have this installed on your server, you shouldn&#39;t have a problem. You can test by running this line:</p>
<pre><code class="lang-bash">$ perl -MLWP::UserAgent -e &quot;print &quot;Module installed.n&quot;;&quot;
</code></pre>
<p>If it works, then you can continue.</p>
<pre><code class="lang-perl">my $LWP = eval { require LWP::UserAgent; };
    if(!$LWP) {
        print &quot;Perl module LWP::UserAgent is not installed on this server.n&quot;;
        exit 1;
    }
    else {
        use LWP::UserAgent;
    }
</code></pre>
<p>This tries to load the LWP::UserAgent module. Typically if you try to use a module that doesn&#39;t exist, you get an ugly error,  this will catch the error and print out a nice pretty one, then exit. You can edit this to log the error, or send an alert to Nagios or Zabbix or something of that sort.</p>
<p>The second part of the BEGIN code block, is the part that actually puts the module content into use.</p>
<pre><code class="lang-perl">   push @INC, sub {
        my $URL = &#39;http://someserver.com/perl_modules/FOOBAR.pm&#39;;
        my $ua = LWP::UserAgent-&gt;new;
        $ua-&gt;timeout(10);
        my $module = $ua-&gt;get($URL);
        if($module-&gt;is_success) {
            my $package = $module-&gt;content;
            open my $fh, &#39;status_line.&quot;n&quot;;
            exit 1;
        }
    }
</code></pre>
<p>The @INC is basically the path of all of the modules or the module directories. If you were to print Dumper(@INC), you would see all of the modules. So this will simply try to use LWP::UserAgent to call the content of the module, if it fails, instead of spitting out an ugly error, it just throws a fail statement. Again, you can send this to Nagios or Zabbix or anything you want.</p>
<p>Thats pretty much it. Put it all together and you have your script. Heres an example of the entire script:</p>
<pre><code class="lang-perl">#!/usr/bin/perl
use strict;
use warnings;

BEGIN {
    my $LWP = eval { require LWP::UserAgent; };
    if(!$LWP) {
        print &quot;Perl module LWP::UserAgent is not installed on this server, it must be installed to use the remote SAM modules.n&quot;;
        exit 1;
    }
    else {
        use LWP::UserAgent;
    }

    push @INC, sub {
        my $URL = &#39;http://someserver.com/perl_modules/FOOBAR.pm&#39;;
        my $ua = LWP::UserAgent-&gt;new;
        $ua-&gt;timeout(10);
        my $module = $ua-&gt;get($URL);
        if($module-&gt;is_success) {
            my $package = $module-&gt;content;
            open my $fh, &#39;status_line.&quot;n&quot;;
            exit 1;
        }
    }
}

use FOOBAR;

my($number, $limit) = ($ARGV[0], $ARGV[1]);

my $result = FOOBAR-&gt;check_less($number, $limit);

print $number, ($result) ? &quot; is less than &quot; : &quot; is greater than &quot;, $limit .&quot;n&quot;;
</code></pre>
<p>Heres some benchmarks of the script running with the module in the local directory, and the module running remotely on a remote web server:</p>
<h3 id="examples-">EXAMPLES:</h3>
<p>Example using the FOOBAR.pm module remotely:</p>
<pre><code class="lang-bash">$ perl jhyland$ time perl test.pl 10 20
10 is less than 20

real    0m0.687s
user    0m0.125s
sys    0m0.059s
</code></pre>
<p>Example using the FOOBAR.pm module in the local directory:</p>
<pre><code class="lang-bash">$ perl jhyland$ time perl test.pl 10 20
10 is less than 20

real    0m0.012s
user    0m0.007s
sys    0m0.004s
</code></pre>
<h3 id="now-for-the-con">Now For The Con</h3>
<p>Pretty much the only issue with using modules on remote servers, is you&#39;re script is subject to the response time of the HTTP requests. As we all know, HTTP isn&#39;t always reliable. You deal with timeouts, server reset, etc etc</p>
]]>
      </content:encoded>
    </item>
  </channel>
</rss>
